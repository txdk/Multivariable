% Chapter 2
\chapter{Differentiation}
\section{Definition of the Derivative}
We will now begin developing the theory of calculus for vector-valued functions with vector-valued inputs, starting with \emph{differential calculus}. When talking about the differentiability of mappings between Euclidean spaces, we will consider the domain of the map to be an open set, so that we can move some small amount in any direction about any point in the domain. This, loosely speaking, will give us some notion of being able to approach any particular point from any given direction, and thus allow us to consider the rate of change of the function along this path. Indeed, the rate of change of a function as we vary the input variable(s) is one intuitive notion of the derivative of the function.

\vspace{3mm}

In the discussion that will follow, let \( E \) be Euclidean \( n \)-space, let \( F \) be Euclidean \( m \)-space and let \( U\subseteq E \) be an open set. We want to define the notion of differentiability for mappings \( f:U\to F \). Another way to think whether a map \( f \) is differentiable is whether it looks locally like a first-order polynomial, i.e.\ in some neighbourhood of a point \( x_0 \) in the domain of \( f \), the following expression is a `good' approximation for \( f \)
\[ f(x)\approx f(x_0)+T(x-x_0) \]
for some linear map \( T\in L(E,F) \). Of course, the behaviour of \( f \) can be quite complicated, and so such a linear approximation for \( f \) may be quite bad far away from \( x_0 \). The expression is of course exact \emph{at} \( x_0 \). As for points `close to' \( x_0 \), we will designate the functions \( f \) for which the approximation is `good' to be the differentiable functions. The linear map \( T \) appearing in the linear approximation (the `slope', if you may, to make an analogy with real-valued functions of a single variable) that minimises the error of the linear approximation is what we will call the derivative of \( f \), so that the derivative is the linear map used to construct the `best' linear approximation of \( f \) at \( x_0 \).

\begin{definition}
  \label{def:derivative}
  A mapping \( f:U\to F \) is said to be \textbf{differentiable} at \( x_0\in U \) if there exists a linear map \( \D f(x_0)\in L(E,F) \) and a map \( R:U\to F \) such that for every \( x\in U \), we have
  \[ f(x)=f(x_0)+[\D f(x_0)]\qty(x-x_0)+R(x) \]
  and
  \[ \lim_{\norm{x-x_0}\to 0}\frac{\norm{R(x)}}{\norm{x-x_0}}=0\fstop \]

  If \( f \) is differentiable at \( x_0 \), the linear map \( \D f(x_0):E\to F \) is called the \textbf{derivative} of \( f \) at \( x_0 \).
\end{definition}

So, we can express any function \( f \) as the sum of a linear approximation for \( f \) and some remainder/rubbish term \( R(x) \), the latter of which captures the `error' of the linear approximation compared to the true value of \( f \). The value of the error can be quite large for certain values of \( x \), but if \( f \) is differentiable, the error tends to zero as one approaches \( x_0 \). This makes rigourous the notion that the linear approximation can be made arbitrarily accurate for values of \( x \) that are sufficiently close to \( x_0 \).

\vspace{3mm}

Since the derivative is a linear map (i.e.\ an element of \( L(E,F) \)), it can be represented by an \( m\times n \) matrix. This matrix is called the \emph{Jacobian matrix}.

\vspace{3mm}

If \( f \) is differentiable at each \( x_0\in U \), we say that \( f \) is differentiable on \( U \) (or simply: differentiable), and can define the differentiation map \( \D f:U\to L(E,F) \), which takes each \( x_0\in U \) to \( \D f(x_0) \); the derivative of \( f \) at \( x_0 \).

\vspace{3mm}

Let's look at a couple of examples. First, consider the first-order polynomial \( p:\R^n\to\R^m \)
\[ p(\vb{x})=A\vb{x}+\vb{b} \]
where \( A\in L(\R^n,\R^m) \) is a linear map and \( \vb{b}\in\R^m \) is a constant vector. We claim that \( p \) is a differentiable function for all \( \vb{x_0}\in\R^n \) with derivative \( A \). To see this, define the function \( R:\R^n\to \R^m \) by
\[ R(\vb{x})=p(\vb{x})-p(\vb{x_0})-A(\vb{x}-\vb{x_0})\fstop \]

Then substituting the definition of \( p \) gives
\[ R(\vb{x})= A\vb{x}+\vb{b}-(A\vb{x_0}+\vb{b})-A\vb{x}+A\vb{x_0}=\vb{0}\]
which holds for all \( \vb{x}\in\R^n \). Thus, we clearly have that
\[ \lim_{\norm{\vb{x}-\vb{x_0}}\to 0}\frac{\norm{R(\vb{x})}}{\norm{\vb{x}-\vb{x_0}}}=0\fstop \]

This shows that \( p \) is differentiable on \( \R^n \), and moreover that \( \D p(\vb{x_0})=A \) for all \( \vb{x_0}\in\R^n \), i.e.\ the differentiation map \( \D p:\R^n\to L(\R^n,\R^m) \) is constant with value \( A \). Notice that the remainder function \( R \) is identically zero on \( \R^n \). This is because \( p \) is a first-order polynomial in \( \vb{x} \), so it is \emph{equal} to its linear approximation. The Jacobian matrix for \( p \) at any \( \vb{x_0}\in\R^n \) would simply be the matrix representation of \( A \).

\vspace{3mm}

Now for a slightly more nontrivial example. Consider the function \( f:\R^2\to\R^2 \) defined by
\[ f(x,y)=\mqty(x^2+y^2 \\ xy) \]
where we have expressed all elements of \( \R^2 \) in terms of the standard basis. We will show that \( f \) is differentiable on \( \R^2 \), with the differentiation map \( \D f:\R^2\to L(\R^2,\R^2) \) given by
\[ \D f(x,y)=\mqty(2x & 2y\\ y & x)\fstop \]

Let \( \vb{x_0}=(x_0,y_0)^T\in\R^2 \). Using the above linear map as our candidate for the derivative, we will define the remainder function \( R:\R^2\to\R^2 \) by
\[ R(x,y)=f(x,y)-f(x_0,y_0)-[\D f(x_0)](\vb{x}-\vb{x_0})\fstop \]

Writing the above expression out as a vector equation gives
\begin{align*}
  R(x,y)&=\mqty(x^2+y^2\\xy)-\mqty(x_0^2+y_0^2\\x_0y_0)-\mqty(2x & 2y\\ y & x)\mqty(x-x_0\\y-y_0)\\
  &= \mqty(x^2-x_0^2+y^2-y_0^2\\xy-x_0y_0)-\mqty(2x(x-x_0)+2y(y-y_0)\\y(x-x_0)+x(y-y_0))\\
  &= \mqty(-(x-x_0)^2-(y-y_0)^2\\-(x-x_0)(y-y_0))\fstop
\end{align*}

Consider taking the maximum norm of both sides
\[ \norm{R(x,y)}_\infty=\max\qty{\abs{(x-x_0)^2+(y-y_0)^2},\abs{(x-x_0)(y-y_0)}}\fstop \]

But since \( \norm{\vb{x}-\vb{x_0}}_\infty=\max\qty{\abs{x-x_0},\abs{y-y_0}} \), we have that
\[ \norm{R(x,y)}_\infty\leq\max\qty{2\norm{\vb{x}-\vb{x_0}}_\infty^2,\norm{\vb{x}-\vb{x_0}}^2_\infty}=2\norm{\vb{x}-\vb{x_0}}_\infty^2\fstop \]

Dividing through by \( \norm{\vb{x}-\vb{x_0}}_\infty\) yields
\[ 0\leq\frac{\norm{R(x,y)}_\infty}{\norm{\vb{x}-\vb{x_0}}_\infty}\leq 2\norm{\vb{x}-\vb{x_0}}_\infty\fstop \]

Taking the limit as \( \norm{\vb{x}-\vb{x_0}}_\infty\to 0 \) and applying the Squeeze Theorem yields
\[ \lim_{\norm{\vb{x}-\vb{x_0}}_\infty\to 0}\frac{\norm{R(x,y)}_\infty}{\norm{\vb{x}-\vb{x_0}}_\infty}=0\fstop \]

Since the maximum norm is strongly equivalent to the Euclidean norm, the above limit also converges to 0 in the Euclidean norm. This shows that \( f \) is differentiable at \( \vb{x_0} \). Since the choice of \( \vb{x_0}\in\R^2 \) was arbitrary, it follows that \( f \) is differentiable on \( \R^2 \), with derivative \( \D f \) as  shown.

\vspace{3mm}

Our use of the maximum norm over the Euclidean norm to prove the differentiability of \( f \) resulted in a greatly simplified calculation. Our argument for why this was justified may be generalised to the following result.

\begin{proposition}
  \label{thm:diff-norm-inv}
  The differentiability of a mapping \( f:U\to F \) does not depend on the choice of norm on \( E \) and \( F \).
\end{proposition}
\begin{proof}
  Suppose \( f:U\to F \) is differentiable at \( x_0\in U \) with respect to the norms \( \norm{-}_E \) and \( \norm{-}_F \) on \( E \) and \( F \) respectively. That is, there exist maps \( \D f(x_0)\in L(E,F) \) and \( R:U\to F \) such that for all \( x\in U \),
  \[ f(x)=f(x_0)+[\D f(x_0)](x-x_0)+R(x) \]
  and
  \[ \lim_{\norm{x-x_0}_E\to 0}\frac{\norm{R(x)}_F}{\norm{x-x_0}_E}=0\fstop \]

  Let \( \norm{-}_{E'} \) be any other norm on \( E \) and \( \norm{-}_{F'} \) be an arbitrary norm on \( F \). Then by \Cref{thm:norm-equiv}, \( \norm{-}_E\) and \( \norm{-}_{E'} \) are strongly equivalent, and \( \norm{-}_F \) and \( \norm{-}_{F'} \) are strongly equivalent. This means that there exists \( K_1,K_2>0 \) such that
  \begin{align*}
    \norm{x-x_0}_E&\leq K_1\norm{x-x_0}_{E'}\\
    \norm{R(x)}_{F'}&\leq K_2\norm{R(x)}_F
  \end{align*}
  which holds for all \( x\in U \). Thus, we have that
  \[ 0\leq \frac{\norm{R(x)}_{F'}}{\norm{x-x_0}_{E'}}\leq K_1K_2\frac{\norm{R(x)}_F}{\norm{x-x_0}_E}\fstop \]

  Taking limits gives
  \[  0\leq \lim_{\norm{x-x_0}_{E'}\to 0}\frac{\norm{R(x)}_{F'}}{\norm{x-x_0}_{E'}}\leq K_1K_2\lim_{\norm{x-x_0}_{E'}\to 0}\frac{\norm{R(x)}_F}{\norm{x-x_0}_E}\fstop \]

  But by \Cref{thm:norm-equiv-conv}, we have that \( \norm{x-x_0}_E\to 0 \) if and only if \( \norm{x-x_0}_{E'}\to 0 \). Thus, the limit on the right-hand side of the above inequality vanishes, and thus we conclude by the Squeeze Theorem that
  \[ \lim_{\norm{x-x_0}_{E'}\to 0}\frac{\norm{R(x)}_{F'}}{\norm{x-x_0}_{E'}}=0 \]
  which proves that \( f \) is differentiable at \( x_0 \) with respect to the norms \( \norm{-}_{E'} \) and \( \norm{-}_{F'} \), with the same derivative \( \D f(x_0) \) and remainder function \( R \).
\end{proof}

You may have noticed a few things about the calculations in the previous examples. Firstly, they were somewhat tedious to do! Indeed, in the future we surely do not want to be showing that a given function is differentiable directly from the definition. Secondly (and perhaps most importantly), to show that a function is differentiable, we somehow have to pre-ordain what the derivative ought to be (which we did before by coincidentally materialising the correct linear map seemingly out of nowhere). These factors combined means that the definition of the derivative alone does not naturally lend itself to \emph{practical} computations for the derivative of a multivariable function, and so we'll need to develop the theory further to obtain some results and formulae to simplify this process.

\vspace{3mm}

The definition of the derivative can actually be stated in terms of a single limit expression by combining the two expressions in \Cref{def:derivative} by eliminating the remainder function \( R \). Explicitly, a function \( f:U\to F \) is differentiable at \( x_0\in U \) if and only if there exists a linear map \( \D f(x_0)\in L(E,F) \) such that
\[ \lim_{\norm{x-x_0}\to 0}\frac{\norm{f(x)-f(x_0)-[\D f(x_0)](x-x_0)}}{\norm{x-x_0}}=0\fstop \]

The derivative of a function \( f \) at \( x_0 \) retains its interpretation as the infinitesimal rate of change of the function \( f \) as you move away from \( x_0 \) even in the multivariable realm. Given any vector \( y\in E \), the derivative at \( x_0 \) evaluated at \( y \), \( [\D f(x_0)](y) \), is the variation of \( f \) as one moves an infinitesimal distance in a straight line in the direction of \( y \) from the point \( x_0 \).

\begin{proposition}
  \label{thm:direction-deriv}
  Suppose \( f:U\to F \) is differentiable at \( x_0\in U \). Then for every \( y\in E \), we have that
  \[ [\D f(x_0)](y)=\lim_{t\to 0}\frac{f(x_0+ty)-f(x_0)}{t}\fstop \]
\end{proposition}
\begin{proof}
  Let \( y\in E \). If \( y=0 \), then
  \[ \lim_{t\to 0}\frac{f(x_0+ty)-f(x_0)}{t}=\lim_{t\to 0}\frac{f(x_0)-f(x_0)}{t}=0\fstop \]

  Moreover, we have that \( [\D f(x_0)](0)=0 \) since \( \D f(x_0) \) is linear, which verifies the statement for this particular case.

  \vspace{3mm}

  Now suppose that \( y\neq 0 \). Since \( U \) is open, there exists a \( \delta>0 \) such that \( B(x_0,\delta)\subseteq U \). Then for all \( t\in\qty(-\flatfrac{\delta}{\norm{y}},\flatfrac{\delta}{\norm{y}}) \), we have that \( x=x_0+ty\in U \), and hence
  \[ f(x_0+ty)-f(x_0)=[\D f(x_0)](ty)+R(x) \]
  since \( f \) is differentiable at \( x_0 \). Dividing through the above expression by \( t \) and rearranging gives
  \[ \frac{f(x_0+ty)-f(x_0)}{t}- \frac{t[\D f(x_0)](y)}{t}=\frac{R(x)}{t} \]
  noting that we have used the linearity of \( \D f(x_0) \) to pull out the factor of \( t \). Now, by applying the norm to both sides and taking the limit \( t\to 0 \) gives
  \[ \lim_{t\to 0}\norm{\frac{f(x_0+ty)-f(x_0)}{t}- [\D f(x_0)](y)}=\lim_{t\to 0}\frac{\norm{R(x)}}{\abs{t}}\fstop \]

  Since \( \norm{x-x_0}=\norm{ty}=\abs{t}\norm{y} \), we can write
  \begin{align*}
    \norm{\lim_{t\to 0}\frac{f(x_0+ty)-f(x_0)}{t}- [\D f(x_0)](y)}&=\norm{y}\lim_{t\to 0}\frac{\norm{R(x)}}{\abs{t}\norm{y}}\\
    &= \norm{y}\lim_{\norm{x-x_0}\to 0}\frac{\norm{R(x)}}{\norm{x-x_0}}\\
    &= 0
  \end{align*}
  which follows from the differentiability of \( f \) at \( x_0 \) and by exploiting the continuity of the norm in two places: to assert that \( \norm{x-x_0}\to 0 \) as \( t\to 0 \) and to pull limits inside of the norm. By positivity of the norm, we obtain the desired result
  \[ \lim_{t\to 0}\frac{f(x_0+ty)-f(x_0)}{t}=[\D f(x_0)](y)\fstop \]
\end{proof}

This leads us to define the concept of the \emph{directional derivative}, a weaker notion of the derivative of a vector-valued function.

\begin{definition}
  The \textbf{directional derivative} of a map \( f:U\to F \) at \( x_0\in U \) in the direction of \( y\in E\backslash\qty{0} \) is
  \[ \lim_{t\to 0}\frac{f(x_0+t\vu{y})-f(x_0)}{t}\cma \]
  provided that the above limit exists. The vector \( \vu{y}=\flatfrac{y}{\norm{y}} \) is a unit vector in the direction of \( y \).
\end{definition}

An immediate consequence of \Cref{thm:direction-deriv} is that it provides a convenient relation between the derivative of a differentiable function and its corresponding directional derivatives.

\begin{corollary}
  \label{thm:direction-cor}
  Suppose \( f:U\to F \) is differentiable at \( x_0\in U \). Then for every \( y\in E\backslash\qty{0} \), the directional derivative of \( f \) at \( x_0 \) in the direction of \( y \) exists, and is equal to \( [\D f(x_0)](\vu{y}) \).
\end{corollary}

The converse of the above statement does not hold. That is, if the directional derivative of \( f \) at \( x_0 \) in the direction of \( y \) exists for every \( y\in E \), \( f \) is not necessarily differentiable at \( x_0 \). A nice counterexample is the following: define the function \( f:\R^2\to\R \) by
\[ f(x,y)=
  \begin{cases}
    \frac{x^3}{x^2+y^2} &\text{if }(x,y)\neq (0,0)\\
    0 &\text{if }(x,y)=(0,0)\fstop
  \end{cases}
\]

Let \( \vb{v}\in\R^2 \) be any nonzero vector. We can express \( \vu{v} \) in terms of the standard basis \( \qty{\vu{e}_x,\vu{e}_y} \) in \( \R^2 \) as follows
\[ \vu{v}=\cos\theta\vu{e}_x+\sin\theta\vu{e}_y \]
for some \( \theta\in\R \). Then for any \( t\neq 0 \), we have
\[ \lim_{t\to 0}\frac{f(\vb{0}+t\vu{v})-f(\vb{0})}{t}=\lim_{t\to 0}\frac{1}{t}\qty(\frac{t^3\cos^3\theta}{t^2\cos^2\theta+t^2\sin^2\theta})=\lim_{t\to 0}\frac{t^3\cos^3\theta}{t^3}=\lim_{t\to 0}\cos^3\theta=\cos^3\theta \]
noting that we have used the fact that \( \cos^2\theta+\sin^2\theta=1 \). This shows that the directional derivative of \( f \) at \( \vb{0} \) in the direction of \( \vb{v} \) exists, and is equal to \( \cos^3\theta \). Since the choice of \( \vb{v}\in\R^2\backslash\qty{\vb{0}} \) was arbitrary, it follows that all of the directional derivatives of \( f \) at \( \vb{0} \) exist. However, \( f \) is \emph{not} differentiable at \( \vb{0} \). Assume for a contradiction that \( f \) were differentiable at \( \vb{0} \) with derivative \( \D f(\vb{0}) \). By \Cref{thm:direction-cor}, \( \qty[\D f(\vb{0})](\vu{e}_x) \) is equal to the directional derivative of \( f \) at \( \vb{0} \) in the direction of \( \vu{e}_x \), so that
\[ [\D f(\vb{0})](\vu{e}_x)=\eval{\cos^3\theta}_{\theta=0}=1\fstop \]

Similarly, \( \qty[\D f(\vb{0})](\vu{e}_y) \) is equal to the directional derivative of \( f \) at \( \vb{0} \) in the direction of \( \vu{e}_y \), which implies that
\[ [\D f(\vb{0})](\vu{e}_y)=\eval{\cos^3\theta}_{\theta=\frac{\pi}{2}}=0\fstop \]

Since \( \D f(\vb{0}) \) is a linear map, we have that
\[ [\D f(\vb{0})](\vu{e}_x+\vu{e}_y)=[\D f(\vb{0})](\vu{e}_x)+[\D f(\vb{0})](\vu{e}_y)=1\fstop \]

However, by \Cref{thm:direction-cor}, we can obtain an expression for \( [\D f(\vb{0})(\vu{e}_x+\vu{e}_y) \) by evaluating the directional derivative in the direction of \( (1,1) \).
\[ [\D f(\vb{0})](\vu{e}_x+\vu{e}_y)=\sqrt{2}[\D f(\vb{0})]\qty(\frac{\vu{e}_x+\vu{e}_y}{\sqrt{2}})=\sqrt{2}\eval{\cos^3\theta}_{\theta=\frac{\pi}{4}}=\sqrt{2}\qty(\frac{1}{\sqrt{2}})^3=\frac{1}{2}\fstop \]

Putting this all together leads to the statement that \( \frac{1}{2}=1 \), which gives us our desired contradiction.

\vspace{3mm}

An important consideration that should be on the forefront of our minds is whether the way we've defined the notion of derivatives is consistent with the one-variable definition of the derivative. That is, since \( \R \) is (one-dimensional) Euclidean space, we can talk about the notion of a map \( f:\R\to\R \) being differentiable with respect to \Cref{def:derivative}. However, at the surface level, this appears to be a different notion to that of \( f \) being differentiable in the ordinary, one-variable sense. We must check that these two notions actually coincide, so that our definition of the derivative is indeed a generalisation of the notion of differentiability to the multivariable case.

\begin{proposition}
  Let \( A\subseteq\R \) be open and let \( x_0\in A \). Then \( f:A\to\R \) is differentiable at \( x_0 \) if and only if \( f \) is differentiable in the one-variable sense, i.e.\ the following limit exists
  \[ f'(x_0)\coloneqq\lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}\fstop \]

  Moreover, \( [\D f(x_0)](1)=f'(x_0) \).
\end{proposition}
\begin{proof}
  (\( \implies \)) Suppose \( f \) is differentiable at \( x_0 \). By \Cref{thm:direction-cor}, the directional derivative of \( f \) at \( x_0 \) in the direction of \( 1\in\R \) exists, and is given by
  \[ [\D f(x_0)](1)=\lim_{t\to 0}\frac{f(x_0+t)-f(x_0)}{t}\fstop \]

  Let \( x=x_0+t \), which will be an element of the domain \( A \) provided that \( t \) is chosen sufficiently small in magnitude. Then \( x\to x_0 \) as \( t\to 0 \), and so we have that
  \[ [\D f(x_0)](1)=\lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}=f'(x_0)\fstop \]

  (\( \impliedby \)) Suppose \( f \) is differentiable at \( x_0 \) in the one-variable sense. Let \( x\in A \). Define the function \( R:A\to\R \) by
  \[ R(x)=f(x)-f(x_0)-f'(x_0)(x-x_0)\fstop \]

  Then
  \[ \frac{\abs{R(x)}}{\abs{x-x_0}}=\abs{\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)}\fstop \]

  Taking the limit as \( x\to x_0 \) gives
  \[ \lim_{x\to x_0}\frac{\abs{R(x)}}{\abs{x-x_0}}=\abs{\lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)}=\abs{f'(x_0)-f'(x_0)}=0\fstop \]

  From the definition of \( R \), we obtain the desired expression
  \[ f(x)=f(x_0)+[\D f(x_0)](x-x_0)+R(x) \]
  where we have defined the linear map \( \D f(x_0):\R\to \R \) by
  \[ [\D f(x_0)](a)=f'(x_0)a\fstop \]

  This shows that \( f \) is differentiable at \( x_0 \) with derivative \( \D f(x_0) \). Evaluating the derivative at \( a=1 \) gives \( [\D f(x_0)](1)=f'(x_0) \), as required.
\end{proof}

Given the linear nature of the derivative map, one can recover all of the information about the derivative by evaluating it on a basis of \( E \). This means that there is a distinguished set of directional derivatives that serve to describe the total derivative of the function. These are none other than the function's \emph{partial derivatives}, which are simply the directional derivatives of the function in the direction of the basis vectors.

\begin{definition}
  Let \( \qty{e_1,\dots, e_n} \) be a basis for \( E \). The \textbf{partial derivative} of \( f:U\to F \) with respect to the \( i^{\text{th}} \) coordinate at \( x_0\in U \) is the directional derivative of \( f \) at \( x_0 \) in the direction of the basis vector \( e_i \), i.e.\
  \[ {\pdv{f}{x_i}} (x_0)=\lim_{t\to 0}\frac{f(x_0+te_i)-f(x_0)}{t}\fstop \]
\end{definition}

The definition of a partial derivative closely resembles that of a one-variable derivative. Indeed, we are simply restricting our attention to a one-dimensional subset of the domain (a straight line segment in the direction of the basis vector \( e_i \) passing through the point of interest \( x_0 \)), and so the corresponding limit is a one-dimensional limit. This means that partial derivatives can be readily computed using techniques for computing derivatives of single-variable, real-valued functions. To compute the partial derivative with respect to some coordinate \( x_i \), you essentially treat all other coordinates/variables as constants and consider the function as a one-variable function in \( x_i \), and thus compute the one-variable derivative of the function with respect to \( x_i \).

\vspace{3mm}

For example, consider the scalar function \( f:\R^2\to\R \) given by
\[ f(x,y)=3x^2y+x\cos(y)\fstop \]

Again, we are working in the standard basis, so our basis vectors are
\[ \vu{e}_1=\mqty(1\\0), \quad \vu{e}_2=\mqty(0\\1)\fstop \]

Let's first compute the partial derivative with respect to the \( x \)-coordinate directly from the definition. As is conventional, the coordinate \( x \) corresponds to the basis vector \( \vu{e}_1 \), so the corresponding partial derivative at \( \vb{x_0}=(x_0,y_0)^T\in\R^2 \) is
\begin{align*}
  {\pdv{f}{x}}(\vb{x_0})&= \lim_{t\to 0}\frac{f(x_0+t,y_0)-f(x_0,y_0)}{t}\\
  &= \lim_{t\to 0}\frac{3(x_0+t)^2y+(x_0+t)\cos(y_0)-3x_0^2y_0-x_0\cos(y_0)}{t}\\
  &= \lim_{t\to 0}\frac{3x_0^2y+6x_0y_0t+3t^2y_0+x_0\cos(y_0)+t\cos(y_0)-3x_0^2y_0-x_0\cos(y_0)}{t}\\
  &= \lim_{t\to 0}\frac{6x_0y_0t+3t^2y_0+t\cos(y_0)}{t}\\
  &= \lim_{t\to 0}\qty(6x_0y_0+3ty_0+\cos(y_0))\\
  &= 6x_0y_0+\cos(y_0)\fstop
\end{align*}

Observe that we have basically computed a derivative by first principles. The faster approach that makes use of our knowledge of one-variable derivatives goes as follows: fix \( y_0\in\R \) and consider the restricted (now single-variable) function \( g:\R\to\R \) given by
\[ g(x)=f(x,y_0)=3x^2y_0+x\cos(y_0)\fstop \]

We can readily compute the one-variable derivative
\[ g'(x)=6xy_0+\cos(y_0)\fstop \]

But notice that
\[ {\pdv{f}{x}}(\vb{x_0})=g'(x_0)\fstop \]

We will use the latter method to compute the partial derivative of \( f \) with respect to the coordinate \( y \), which corresponds to the basis vector \( \vu{e}_2 \). Fix \( x_0\in\R \), and consider the restricted map \( h:\R\to\R \) given by
\[ h(y)=f(x_0,y)=3x_0^2y+x\cos(y)\fstop \]

Then the partial derivative with respect to \( y \) is given by
\[ {\pdv{f}{y}}(\vb{x_0})=h'(y_0)=3x_0^2-x_0\sin(y_0)\fstop \]

Wasn't that so much easier to do? This is the method we will adopt to compute partial derivatives. After all, why would we want to compute derivatives via first principles given what we know about single-variable calculus? (Answer: we don't!)

\vspace{3mm}

Now, as we eluded to previously, the reason why we're making such a big deal about the partial derivatives is because we can express the (total) derivative of a function entirely in terms of its partial derivatives. Since the partial derivatives are fairly easy to compute, this gives us a practical means to compute the total derivative. The foundation of this method will be built upon the following result.

\begin{corollary}
  Suppose \( f:U\to F \) is differentiable at \( x_0\in U \). Then all of the partial derivatives of \( f \) at \( x_0 \) exist, and moreover we have
  \[ [\D f(x_0)](e_i)={\pdv{f}{x_i}}(x_0) \]
  for each \( i=1,\dots, n \).
\end{corollary}
\begin{proof}
  Since, by definition, the partial derivatives of \( f \) at \( x_0 \) are simply directional derivatives of \( f \) at \( x_0 \) in the direction of the basis vectors, all of the partial derivatives at \( x_0 \) exist by \Cref{thm:direction-cor}. Moreover for each \( i\in\qty{1,\dots,n} \), we have that
  \[ [\D f(x_0)](e_i)=\lim_{t\to 0}\frac{f(x_0+te_i)-f(x_0)}{t}={\pdv{f}{x_i}} (x_0)\fstop \]
\end{proof}

We are now ready to discuss our method for computing the derivative of a multivariable function \( f \) from its partial derivatives. We'll start by considering the case where \( f \) is a scalar function then work our way up to a vector-valued function. Suppose we somehow knew that the scalar function \( f:U\to\R \) is differentiable at \( x_0\in U \) and want to compute its derivative \( \D f(x_0) \). Since \( \D f(x_0)\in L(E,\R)=E^* \), i.e.\ it is a linear functional, we can expand \( \D f(x_0) \) in terms of the \emph{dual basis} introduced in \Cref{sec:dual}
\[ \D f(x_0)=\sum_{i=1}^na_iv_i^* \]
for some coefficients \( a_i\in\R \). To determine the values of \( a_i \), we evaluate the operators on both sides of the above expression at the \( k^{\text{th}} \) basis vector \( e_k \)
\begin{align*}
  [\D f(x_0)](e_k)&=\sum_{i=1}^na_iv_i^*(e_k)\\
  &= \sum_{i=1}^na_i\delta_{ik}\\
  &= a_k
\end{align*}
but the left-hand side is simply the \( k^{\text{th}} \) partial derivative of \( f \) at \( x_0 \), so that
\[ a_k={\pdv{f}{x_k}}(x_0)\fstop \]

Hence, the derivative of a scalar function has the following representation in terms of its partial derivatives
\[ \D f(x_0)=\sum_{i=1}^n{\pdv{f}{x_i}}(x_0)\,v_i^*\fstop \]

Thus, the task of computing the derivative of a differentiable function reduces to computing each of its partial derivatives. The Jacobian matrix of \( f \) at \( x_0 \), i.e.\ the matrix representation of \( \D f(x_0) \) is simply a \( 1\times n \) matrix of the form
\[ \mqty({\pdv{f}{x_1}}(x_0)&{\pdv{f}{x_2}}(x_0)&{\pdv{f}{x_3}}(x_0)& \dots&{\pdv{f}{x_n}}(x_0))\fstop \]

Now let's consider the case for vector-valued functions. Let \( \qty{\overline{e}_1,\dots,\overline{e}_m} \) be a basis for \( F \), so that for all \( x\in U \), we can write
\[ f(x)=\sum_{i=1}^m f_i(x)\overline{e}_i \]
where each \( f_i:U\to\R \) are the component functions of \( f \). Each of the component functions are scalar functions. The differentiability of \( f \) implies the differentiability of each of the \( f_i \). In fact, we also have the converse.

\begin{proposition}
  \label{thm:comp-diff}
  A function \( f:U\to F \) is differentiable at \( x_0\in U \) if and only if all of its component functions \( f_i:U\to\R \) are differentiable at \( x_0 \).
\end{proposition}
\begin{proof}
  (\( \implies \)) Suppose \( f \) is differentiable at \( x_0\in U \). Then
  \[ f(x)=f(x_0)+[\D f(x_0)](x-x_0)+R(x) \]
  for some \( R:U\to F \) satisfying
  \[ \lim_{\norm{x-x_0}}\frac{\norm{R(x)}_\infty}{\norm{x-x_0}}=0 \]
  for all \( x\in U \). Notice that we have equipped \( F \) with the maximum norm \( \norm{-}_\infty \), but the final result will be independent of the choice of norm by way of \Cref{thm:diff-norm-inv}.

  \vspace{3mm}

  We expand the first expression in terms of the basis in \( F \) (i.e.\ in terms of component functions)
  \[ \sum_{i=1}^m f_i(x)\overline{e}_i= \sum_{i=1}^m\Big(f_i(x_0)+[\D f_i(x_0)](x-x_0)+R_i(x)\Big)\overline{e}_i \]
  where \( \D f_i(x_0) \) and \( R_i \) are the \( i^{\text{th}} \) component functions of \( \D f(x_0) \) and \( R \), respectively. Then by linear independence of the \( \overline{e}_i \), we can equate the above expression component-wise to obtain
  \[ f_i(x)=f_i(x_0)+[\D f_i(x_0)](x-x_0)+R_i(x)\fstop \]

  Recall from \Cref{sec:linear-maps} that an arbitrary linear map in \( L(E,F) \) can be expressed in terms of a basis consisting of linear maps \( \phi_{ij}:E\to F \) satisfying
  \[ \phi_{ij}(e_k)=\delta_{jk}\overline{e}_i\fstop \]

  The corresponding representation of \( \D f(x_0) \) in terms of this basis is
  \[ \D f(x_0)=\sum_{i=1}^m\sum_{j=1}^n J_{ij}(x_0)\phi_{ij} \]
  where \( J_{ij}(x_0)\in\R \) is the \( i,j \)-element of the Jacobian matrix of \( f \) at \( x_0 \). Let \( x=a_1e_1+\dots+a_ne_n\in E \). Then
  \begin{align*}
    [\D f(x_0)](x)&=\sum_{i=1}^m\sum_{j=1}^n J_{ij}(x_0)\sum_{k=1}^na_k\phi_{ij}(e_k)\\
    &= \sum_{i=1}^m\sum_{j=1}^n J_{ij}(x_0)\sum_{k=1}^na_k\delta_{jk}\overline{e}_i\\
    &= \sum_{i=1}^m\Big(\sum_{j=1}^nJ_{ij}(x_0)a_j\Big)\overline{e}_i\\
    &= \sum_{i=1}^m\Big(\sum_{j=1}^nJ_{ij}(x_0)v_j^*(x)\Big)\overline{e}_i
  \end{align*}

  Based on the above representation of \( [\D f(x_0)](x) \) in terms of the basis in \( F \), we see that
  \[ [\D f_i(x_0)](x)=\sum_{j=1}^nJ_{ij}(x_0)v_j^*(x) \]
  which holds for all \( x\in E \). The \( i^{\text{th}} \) component of the derivative \( \D f_i(x_0):E\to\R \) is hence a linear map, since it is a linear combination of linear maps.

  \vspace{3mm}

  Furthermore, for each \( i\in\qty{1,\dots, m} \), we have that
  \[ 0\leq\frac{\abs{R_i(x)}}{\norm{x-x_0}_\infty}\leq\max_{k\in\qty{1,\dots,m}}\frac{\abs{R_k(x)}}{{\norm{x-x_0}_\infty}}=\frac{\norm{R(x)}_\infty}{{\norm{x-x_0}_\infty}}\fstop \]

  Applying the limit as \( \norm{x-x_0}_\infty\to 0 \) and applying the Squeeze Theorem gives
  \[ \lim_{\norm{x-x_0}_\infty\to 0}\frac{\abs{R_i(x)}}{\norm{x-x_0}_\infty}=0\fstop \]

  This shows that \( f_i \) is differentiable at \( x_0 \) with derivative \( [\D f(x_0)]_i \) and remainder function \( R_i \).

  \vspace{3mm}

  (\( \impliedby \)) Suppose each of the component functions \( f_i \) are differentiable at \( x_0 \). Then
  \[ f_i(x)=f_i(x_0)+[\D f_i(x_0)](x-x_0)+R_i(x) \]
  where \( R_i:U\to\R \) satisfies
  \[ \lim_{\norm{x-x_0}_\infty\to 0}\frac{\abs{R_i(x)}}{\norm{x-x_0}_\infty}=0 \]
  for each \( i\in\qty{1,\dots,m} \). Define the functions \( \D f(x_0):E\to F \) and \( R:U\to F \) by
  \begin{align*}
    [\D f(x_0)](x)&= \sum_{i=1}^m[\D f_i(x_0)](x)\overline{e}_i\\
    R(x)&= \sum_{i=1}^m R_i(x)\overline{e}_i\fstop
  \end{align*}

  Thus, we clearly have that
  \[ f(x)=f(x_0)+[\D f(x_0)](x-x_0)+R(x)\fstop \]

  Now, \( \D f(x_0) \) is linear because it is a sum of linear maps. We also have that
  \[ 0\leq\frac{\norm{R(x)}_\infty}{\norm{x-x_0}_\infty}\leq\sum_{i=1}^m\frac{\abs{R_i(x)}}{\norm{x-x_0}_\infty} \]
  for all \( x\in U \). Applying the limit as \( \norm{x-x_0}_\infty\to 0 \) and using the Squeeze Theorem gives
  \[ \lim_{\norm{x-x_0}}\frac{\norm{R(x)}_\infty}{\norm{x-x_0}}=0 \]
  which proves that \( f \) is differentiable at \( x_0 \) with derivative \( \D f(x_0) \) and remainder function \( R \).
\end{proof}

In light of the above result, given a vector-valued function \( f:U\to F \) that is differentiable at \( x_0\in U \), each of the component functions \( f_i \) are scalar functions that are differentiable at \( x_0 \), and thus they can each be expressed in terms of their partial derivatives
\[ \D f_i(x_0)=\sum_{j=1}^n{\pdv{f_i}{x_j}}(x_0)v_j^*\fstop \]

In the proof for \Cref{thm:comp-diff}, we saw that the derivative of \( f \) can be constructed from the derivatives of the components of \( f \) as follows
\[ \D f(x_0)=\sum_{i=1}^m\overline{e}_i\D f_i(x_0)=\sum_{i=1}^m\sum_{j=1}^n{\pdv{f_i}{x_j}}(x_0)\overline{e}_iv_j^*=\sum_{i=1}^m\sum_{j=1}^n{\pdv{f_i}{x_j}}(x_0)\phi_{ij}\fstop \]

This implies that the derivative \( \D f \) is constructed from all of the partial derivatives of the component functions. Written in this way, we see that the \( i,j \)-component of the corresponding Jacobian matrix \( J_{ij}(x_0) \) is simply
\[ J_{ij}(x_0)={\pdv{f_i}{x_j}}(x_0)\fstop \]

Thus, the derivative of a differentiable multivariable function can be completely expressed in terms of its partial derivatives. To put this into practice, recall our early example of the function \( f:\R^2\to\R^2 \) given by
\[ f(x,y)=\mqty(x^2+y^2\\ xy)\fstop \]

Let's compute its derivative using our described method. First, we'll explicitly write out the component functions of \( f \)
\begin{align*}
  f_1(x,y)&= x^2+y^2\\
  f_2(x,y)&= xy\fstop
\end{align*}

Next, we compute all of the partial derivatives at some arbitrary \( \vb{x_0}=(x_0,y_0)^T\in\R^2 \)
\begin{align*}
  {\pdv{f_1}{x}}(\vb{x_0})&= 2x_0 & {\pdv{f_1}{y}}(\vb{x_0})=2y_0\\
  {\pdv{f_2}{x}}(\vb{x_0})&= y_0 & {\pdv{f_2}{y}}(\vb{x_0})=x_0\fstop
\end{align*}

Finally, we represent the derivative \( \D f(\vb{x_0}):\R^2\to\R^2 \) by its Jacobian matrix
\[ \D f(\vb{x_0})=\mqty({\pdv{f_1}{x}}(\vb{x_0})&{\pdv{f_1}{y}}(\vb{x_0})\\{\pdv{f_2}{x}}(\vb{x_0}) & {\pdv{f_2}{y}}(\vb{x_0}))=\mqty(2x_0 & 2y_0 \\ y_0 & x_0)\fstop \]

You will notice that this coincides with the derivative for \( f \) that we plucked out of thin air when we first considered this example.

\vspace{3mm}

We remark that while this is an excellent method for actually computing the derivative, it relies on the assumption that \( f \) is actually differentiable. Just like with directional derivatives, the partial derivatives of a multivariable function can exist without the function itself actually being differentiable. The existence of the partial derivatives is a necessary condition for differentiability, but not sufficient. At the very least, with what we know now, we can check the differentiability of a function by computing a candidate for the derivative using the partial derivative method, and then directly checking against the definition for differentiability. This is still somewhat awkward, and indeed we will later resolve this inconvenience by using a nice result to establish a criterion for a function to be differentiable based on the behaviour of its partial derivatives. This will be carried out in \Cref{sec:C1}.

\section{Properties of the Derivative}
In this section, we'll begin to tackle some easy facts about the derivative. All of these are actually generalised versions of their counterparts in one-variable calculus. Namely, the uniqueness of the derivative, differentiability implying continuity, and how differentiation behaves on sums of vector-valued functions and on products of scalar functions. We'll delay the discussion of differentiation of compositions of functions and the Chain Rule for \Cref{sec:Chain-rule}, as it deserves its very own section.

\vspace{3mm}

First, we prove the easy fact that if the derivative of a function exists, it is unique. The difference from the one-variable case is that now the derivative (at a point) is a linear map instead of a single number. To check that it is indeed unique, we suppose the existence of another linear map defined on the same domain that satisfy the properties of the derivative of the given function, and show that the two linear maps agree on their entire (shared) domain.
\begin{proposition}
  If \( f:U\to F \) is differentiable at \( x_0\in U \), the derivative \( \D f(x_0) \) is unique.
\end{proposition}
\begin{proof}
  Suppose the linear maps \( \D f(x_0)\in L(E,F) \) and \( T\in L(E,F) \) are both derivatives of \( f \) at \( x_0 \). Let \( y\in E \). Then by \Cref{thm:direction-deriv}, we have that
  \[ [\D f(x_0)](y)=\lim_{t\to 0}\frac{f(x_0+ty)-f(x_0)}{t}=Ty\fstop \]

  Since the above equality holds for any \( y\in E \), it follows that \( \D f(x_0)=T \).
\end{proof}

Next, we prove the famous fact that the differentiability of a mapping implies that the mapping is also continuous. So, differentiability is still a stronger property than continuity, even in the multivariable case.

\begin{theorem}
  (Differentiability implies continuity) Suppose \( f:U\to F \) is differentiable at \( x_0\in U \). Then \( f \) is continuous at \( x_0 \).
\end{theorem}
\begin{proof}
  Since \( f \) is differentiable at \( x_0 \), then for all \( x\in U \),
  \[ f(x)-f(x_0)=[\D f(x_0)](x-x_0)+R(x)\fstop \]

  We can thus establish the following inequality
  \[ 0\leq\norm{f(x)-f(x_0)}\leq\norm{[\D f(x_0)](x-x_0)}+\norm{R(x)}\fstop \]

  Consider taking the limit as \( x\to x_0 \) of the above
  \begin{equation*}
    \lim_{x\to x_0}\norm{f(x)-f(x_0)}\leq \lim_{x\to x_0}\norm{[\D f(x_0)](x-x_0)}+\lim_{x\to x_0}\norm{R(x)}\fstop
  \end{equation*}

  Any vector norm \( \norm{-} \) is a continuous function. Moreover, since \( \D f(x_0) \) is a linear map, it is continuous by \Cref{thm:lin-cont}. We can hence `pull the limit inside the function' on the first term on the right-hand-side of the above as follows
  \[ \lim_{x\to x_0}\norm{[\D f(x_0)](x-x_0)}=\norm{[\D f(x_0)]\qty(\lim_{x\to x_0}\qty(x-x_0))}=\norm{[\D f(x_0)](x_0-x_0)}=\norm{[\D f(x_0)](0)}=0\fstop \]

  This yields the following simplification
  \begin{align*}
    0\leq \lim_{x\to x_0}\norm{f(x)-f(x_0)}&\leq \lim_{x\to x_0}\norm{R(x)}\\
    &= \lim_{x\to x_0}\qty(\norm{x-x_0}\frac{\norm{R(x)}}{\norm{x-x_0}})\\
    &= \qty(\lim_{x\to x_0}\norm{x-x_0})\qty(\lim_{x\to x_0}\frac{\norm{R(x)}}{\norm{x-x_0}})\\
    &= 0\fstop
  \end{align*}

  It follows that \( \norm{f(x)-f(x_0)}\to 0 \) as \( x\to x_0 \). This implies that \( f \) is continuous at \( x_0\).
\end{proof}

Given a differentiable function \( f:U\to F \), is the derivative map \( \D f:U\to L(E,F) \) continuous? The answer is not necessarily. In fact, we don't even need to invoke a multivariable function to construct a suitable counterexample. Consider the function \( f:\R\to\R \) given by
\[ f(x)=
  \begin{cases}
    x^2\sin(\frac{1}{x}) &x\neq 0\\
    0 &x=0\fstop
  \end{cases}
\]

Since \( f \) is differentiable on \( \R \) in the one-variable sense, it is differentiable in the multivariable sense. Its corresponding differentiation map \( \D f:\R\to L(\R,\R) \) is
\[ \D f(x)=f'(x)\,I_1=
  \begin{cases}
    \qty(2x\sin(\frac{1}{x})-\cos(\frac{1}{x}))I_1 &x\neq 0\\
    0\, I_1 &x=0\fstop
  \end{cases}
\]

We claim that \( \D f \) is \emph{not} continuous at \( x=0 \). We will show this by showing that \( \D f \) is not sequentially continuous at \( x=0 \). That is, we will identify a real-valued sequence \( (x_n)_{n=1}^\infty \) that converges to 0, but orchestrate for \( \norm{\D f(x_n)-\D f(0)} \) to not converge to 0 (in operator norm). To this end, consider the sequence defined by
\[ x_n=\frac{1}{2\pi n}\fstop \]

Clearly, we have that \( x_n\to 0 \) as \( n\to\infty \). Moreover, we have for all \( n\in\N \)
\[ \norm{\D f(x_n)-\D f(0)}=\norm{(-\cos(2\pi n))I_1}=1\fstop \]

Thus, the derivative map \( \D f \) of a differentiable function is not necessarily continuous. Those that do indeed have a continuous derivative fall into a class of functions called \( \mathcal{C}^1 \) functions. Such functions will be the topic of \Cref{sec:C1}.

\vspace{3mm}

Now, we will consider ways in which we may obtain new differentiable functions from old. One of the simplests ways to do so is to consider a vector sum of two differentiable functions. The theorem below says that the resulting sum function is itself differentiable, and the formula for the derivative is simply the sum of the respective derivatives.

\begin{theorem}
  (Sum Rule) Suppose the maps \( f:U\to F \) and \( g:U\to F \) are differentiable at \( x_0\in U \). Then the sum function \( f+g:U\to F \) defined by
  \[ (f+g)(x)=f(x)+g(x) \]
  is differentiable at \( x_0 \), and its derivative is given by
  \[ \D (f+g)(x_0)=\D f(x_0)+\D g(x_0)\fstop \]
\end{theorem}
\begin{proof}
  From the differentiability of \( f \) and \( g \) at \( x_0 \), we can establish the existence of the mappings \( \D f(x_0),\D g(x_0)\in L(E,F) \) and \( R_f,R_g:U\to F \) satisfying
  \begin{align*}
    f(x)&= f(x_0)+[\D f(x_0)](x-x_0)+R_f(x)\\
    g(x)&= g(x_0)+[\D g(x_0)](x-x_0)+R_g(x)
  \end{align*}
  and
  \begin{align*}
    \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_f(x)}}{\norm{x-x_0}}&= 0\\
    \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_g(x)}}{\norm{x-x_0}}&= 0\fstop
  \end{align*}

  Summing the expressions for \( f(x) \) and \( g(x) \) yields
  \[ (f+g)(x)=(f+g)(x_0)+(\D f(x_0)+\D g(x_0))(x-x_0)+(R_f(x)+R_g(x))\fstop \]

  Note that \( (\D f(x_0)+\D g(x_0))\in L(E,F) \), since the sum of linear maps is linear. Moreover, if we define \( R_{f+g}:U\to F \) by \( R_{f+g}=R_f+R_g \), then
  \[ 0\leq\lim_{\norm{x-x_0}\to 0}\frac{\norm{R_{f+g}(x)}}{\norm{x-x_0}}\leq\lim_{\norm{x-x_0}\to 0}\qty(\frac{\norm{R_f(x)}}{\norm{x-x_0}}+\frac{\norm{R_g(x)}}{\norm{x-x_0}})=0\fstop \]

  This proves that \( f+g \) is differentiable at \( x_0 \). Moreover, we see that \( \D(f+g)(x_0)=\D f(x_0)+\D g(x_0) \).
\end{proof}

Next, we consider a multivariable product rule for differentiation. Given two differentiable scalar-valued functions (i.e.\ the outputs are real numbers), we can sensibly form the product function and think about trying to differentiate it. The Product Rule below says that the differentiation procedure on the product is A-OK and furthermore there is a formula for the derivative that is a mere generalisation of the familiar one-variable product rule.

\begin{theorem}
  (Product Rule) Suppose the scalar functions \( f:U\to\R \) and \( g:U\to\R \) are differentiable at \( x_0\in U \). Then the product function \( fg:U\to\R \) defined by
  \[ (fg)(x)=f(x)g(x) \]
  is differentiable at \( x_0 \), and its derivative is given by
  \[ \D(fg)(x_0)=f(x_0)\D g(x_0)+g(x_0)\D f(x_0)\fstop \]
\end{theorem}
\begin{proof}
    From the differentiability of \( f \) and \( g \) at \( x_0 \), we can establish the existence of the mappings \( \D f(x_0),\D g(x_0)\in L(E,F) \) and \( R_f,R_g:U\to F \) satisfying
  \begin{align*}
    f(x)&= f(x_0)+[\D f(x_0)](x-x_0)+R_f(x)\\
    g(x)&= g(x_0)+[\D g(x_0)](x-x_0)+R_g(x)
  \end{align*}
  and
  \begin{align*}
    \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_f(x)}}{\norm{x-x_0}}&= 0\\
    \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_g(x)}}{\norm{x-x_0}}&= 0\fstop
  \end{align*}

  Multiplying the expressions for \( f(x) \) and \( g(x) \) yields
  \begin{align*}
    (fg)(x)&= (fg)(x_0)+f(x_0)[\D g(x_0)](x-x_0)+f(x_0)R_g(x)+g(x_0)[\D f(x_0)](x-x_0)+\dots\\
    &\hspace{6mm}+([\D f(x_0)](x-x_0))([\D g(x_0)](x-x_0))+R_g(x)[\D f(x_0)](x-x_0)+\dots\\
    &\hspace{6mm}+g(x_0)R_f(x)+R_f(x)[\D g(x_0)](x-x_0)+R_f(x)R_g(x)\fstop
  \end{align*}

  We will define the linear map \( \D(fg)(x_0)\in L(E,\R) \) by the linear combination of linear maps
  \[ \D(fg)(x_0)=f(x_0)\D g(x_0)+g(x_0)\D f(x_0) \]
  and define the remainder function \( R_{fg}:U\to\R \) by `all of the other stuff in the expression'
  \begin{align*}
    R_{fg}(x)&=f(x_0)R_g(x)+([\D f(x_0)](x-x_0))([\D g(x_0)](x-x_0))+R_g(x)[\D f(x_0)](x-x_0)+\dots\\
    &\hspace{6mm}+g(x_0)R_f(x)+R_f(x)[\D g(x_0)](x-x_0)+R_f(x)R_g(x)\fstop
  \end{align*}

  Thus, \( (fg)(x) \) can be expressed as follows
  \[ (fg)(x)=(fg)(x_0)+\D(fg)(x_0)(x-x_0)+R_{fg}(x)\fstop \]

  It remains to be checked that
  \[ \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_{fg}(x)}}{\norm{x-x_0}}=0\fstop \]

  By inspection of the expression for \( R_{fg} \), by taking the norm and dividing through by \( \norm{x-x_0} \), we can repeatedly apply the triangle inequality to separate the normed expression into the norms of each of its individual summands. Any terms containing \( R_f \) or \( R_g \) can thus be made to vanish by taking the limit \( \norm{x-x_0}\to 0 \) by hypothesis. The only other term to worry about is
  \begin{align*}
    \frac{\norm{([\D f(x_0)](x-x_0))([\D g(x_0)](x-x_0))}}{\norm{x-x_0}}&\leq\frac{\norm{\D f(x_0)}\norm{x-x_0}\norm{\D g(x_0)}\norm{x-x_0}}{\norm{x-x_0}}\\
    &= \norm{\D f(x_0)}\norm{\D g(x_0)}\norm{x-x_0}
  \end{align*}
  where \( \norm{\D f(x_0)} \) and \( \norm{\D g(x_0)} \) are the operator norms of the linear maps \( \D f(x_0) \) and \( \D g(x_0) \), respectively. Taking the limit as \( \norm{x-x_0}\to 0 \) makes the right-hand side of the above expression vanish, which implies that the left-hand side also vanishes by the Squeeze Theorem. This proves the desired result, and thus demonstrates the differentiability of \( fg \) at \( x_0 \) with the anticipated derivative.
\end{proof}

We saw earlier that first-order monomials (e.g.\ functions of the form \( f(x,y,z)=x \), \( g(x,y,z)=y \), \( h(x,y,z)=z \)) are differentiable. Thus, by the Sum Rule and Product Rule together, we have that every multivariable scalar function that is a polynomial in terms of its coordinate variables is differentiable.

\vspace{3mm}

With the previous results under our belt, you may be eager to extend the familiar Quotient Rule to multivariable calculus. We will actually delay the proof of it till the following section, where it will be shown to be a straightforward consequence of the Product Rule and Chain Rule.

\section{Compositions of Differentiable Mappings}
\label{sec:Chain-rule}
\begin{theorem}
  (Chain Rule) Let \( E, F, G \) be Euclidean spaces, let \( U\subseteq E \) and \( V\subseteq F \) be open and consider the mappings
  \begin{align*}
    f&:U\to F\\
    g&:V\to G\fstop
  \end{align*}
  such that \( f(U)\subseteq V \). If \( f \) is differentiable at \( x_0\in U \) and \( g \) is differentiable at \( f(x_0)\in V \), then the composite mapping
  \[ g\circ f:U\to G \]
  is differentiable at \( x_0 \) with derivative \( \D g(f(x_0))\circ\D f(x_0) \).
\end{theorem}
\begin{proof}
  From the differentiability of \( f \) at \( x_0 \), we can establish the existence of the mappings \( \D f(x_0)\in L(E,F) \) and \( R_f:U\to F \) satisfying
  \[ f(x)=f(x_0)+[\D f(x_0)](x-x_0)+R_f(x) \]
  for all \( x\in E \), and
  \[ \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_f(x)}}{\norm{x-x_0}}=0\fstop \]

  Similarly, from the differentiability of \( g \) at \( f(x_0) \), we can establish the existence of the mappings \( \D g(f(x_0))\in L(F,G) \) and \( R_g:V\to G \) satisfying
  \[ g(y)=g(f(x_0))+[\D g(f(x_0))](y-f(x_0))+R_g(y) \]
  for all \( y\in F \), and
  \[ \lim_{\norm{y-f(x_0)}\to 0}\frac{\norm{R_g(y)}}{\norm{y-f(x_0)}}=0\fstop \]

  Let \( x\in E \). Then \( f(x)\in V \) by hypothesis so that \( f(x)\in\mathcal{D}(g) \). Thus we can meaningfully substitute \( y=f(x) \) into the expression for \( g(y) \) to obtain
  \[ g(f(x))=g(f(x_0))+[\D g(f(x_0))](f(x)-f(x_0))+R_g(f(x))\fstop \]

  Now substitute the expression for \( f(x) \) into the above
  \begin{align*}
    g\circ f(x)&= g\circ f(x_0)+[\D g(f(x_0))]\qty([\D f(x_0)](x-x_0)+R_f(x))+R_g(f(x))\\
    &= g\circ f(x_0)+[\D g(f(x_0))]\circ[\D f(x_0)](x-x_0)+R_{g\circ f}(x)
  \end{align*}
  where \( R_{g\circ f}:U\to G \) is defined as
  \[ R_{g\circ f}(x)=[\D g(f(x_0))](R_f(x))+R_g(f(x))\fstop \]

  From the above, we can establish the following inequality
  \[ \frac{\norm{R_{g\circ f}(x)}}{\norm{x-x_0}}\leq [\D g(f(x_0))]\qty(\frac{\norm{R_f(x)}}{\norm{x-x_0}})+\frac{\norm{R_g(f(x))}}{\norm{x-x_0}}\fstop \]

  Taking the limit as \( \norm{x-x_0}\to 0 \) yields
  \begin{align*}
    \lim_{\norm{x-x_0}\to 0}\frac{\norm{R_{g\circ f}(x)}}{\norm{x-x_0}}&\leq \lim_{\norm{x-x_0}\to 0}[\D g(f(x_0))]\qty(\frac{\norm{R_f(x)}}{\norm{x-x_0}})+\lim_{\norm{x-x_0}\to 0}\qty(\frac{\norm{R_g(f(x))}}{\norm{f(x)-f(x_0)}}\times\frac{\norm{f(x)-f(x_0)}}{\norm{x-x_0}})\\
    &= [\D g(f(x_0))]\qty(\lim_{\norm{x-x_0}\to 0}\frac{\norm{R_f(x)}}{\norm{x-x_0}})+\lim_{\norm{x-x_0}\to 0}\frac{\norm{R_g(f(x))}}{\norm{f(x)-f(x_0)}}\frac{\norm{[\D f(x_0)](x-x_0)+R_f(x)}}{\norm{x-x_0}}\\
    &\leq [\D g(f(x_0))](0)+0\times\lim_{\norm{x-x_0}\to 0}\qty(\norm{\D f(x_0)}+\frac{\norm{R_f(x)}}{\norm{x-x_0}})\\
    &= 0\fstop
  \end{align*}

  We remark that the above calculations are valid provided that \( f(x)\neq f(x_0) \), so that the expression containing \( \flatfrac{1}{\norm{f(x)-f(x_0)}} \) is non-singular. However, notice that for any \( x\in U \) satisfying \( f(x)=f(x_0) \), we have that\footnote{To see this, consider substituting \( y=f(x_0) \) into \[ g(y)=g(f(x_0))+[\D g(f(x_0))](y-f(x_0))+R_g(y) \] and solve for \( R_g(f(x_0)) \).} \( R_g(f(x))=R_g(f(x_0))=0 \), so the problematic term can be replaced with 0 any time we have \( f(x)=f(x_0) \) whilst taking the limit \( x\to x_0 \).

  \vspace{3mm}

  The above calculation, together with the fact the composition of linear maps is linear, shows that \( g\circ f \) is differentiable at \( x_0 \) with the expected derivative.
\end{proof}

Multiplication of Jacobian matrices.

\begin{theorem}
  (Quotient Rule) Suppose the scalar functions \( f:U\to\R \) and \( g:U\to\R \) are differentiable at \( x_0\in U \), and moreover we have that \( g(x_0)\neq 0 \). Then the quotient function \( \flatfrac{f}{g}:U\to\R \) defined by
  \[ (\flatfrac{f}{g})(x)=\frac{f(x)}{g(x)} \]
  is differentiable at \( x_0 \), and its derivative is given by
  \[ \D\qty(\flatfrac{f}{g})(x_0)=\frac{g(x_0)\D f(x_0)-f(x_0)\D g(x_0)}{(g(x_0))^2}\fstop \]
\end{theorem}
\begin{proof}
  Define the function \( h:\R\backslash\qty{0}\to\R \) by
  \[ h(x)=\frac{1}{x}\fstop \]

  Then \( \flatfrac{1}{g}=h\circ g \). Since \( h \) is differentiable on \( \R\backslash\qty{0} \), and \( g \) is differentiable at \( x_0 \), it follows that \( h\circ g \) is differentiable at \( x_0 \) by the Chain Rule, which yields the formula
  \[ \D(h\circ g)(x_0)=\D h(g(x_0))\circ\D g(x_0)\fstop \]

  We recall from one-variable calculus the derivative of \( \flatfrac{1}{x} \)
  \[ \D h(g(x_0))=h'(g(x_0))I_1=-\frac{1}{(g(x_0))^2}I_1 \]
  where \( I_1:\R\to\R \) is the 1D identity map \( I_1(x)=x \), which has been inserted to make clear that the above expression is an operator expression. Substituting this into our expression for the derivative of \( h\circ g \) yields
  \[ \D(h\circ g)(x_0)=-\frac{1}{(g(x_0))^2}\D g(x_0)\fstop\]

  Finally, we apply the Product Rule to \( f \) and \( \flatfrac{1}{g}=h\circ g \) and conclude that \( \flatfrac{f}{g} \) is differentiable at \( x_0 \) with derivative
  \begin{align*}
    \D(\flatfrac{f}{g})(x_0)&= f(x_0)\D(h\circ g)(x_0)+(h\circ g)(x_0)\D f(x_0)\\
    &= \frac{-f(x_0)\D g(x_0)}{(g(x_0))^2}+\frac{\D f(x_0)}{g(x_0)}\\
    &= \frac{g(x_0)\D f(x_0)-f(x_0)\D g(x_0)}{(g(x_0))^2}\fstop
  \end{align*}
\end{proof}

\vspace{3mm}

An important application of the Chain Rule is the computation of derivatives of functions with respect to an alternative coordinate system. By default, when we work in \( \R^n \), we express every vector in terms of an orthonormal basis \( \qty{\vu{e}_1,\dots, \vu{e}_n} \) in \( \R^n \) (typically the standard basis). So given an arbitrary vector \( \vb{x}\in\R^n \) and the representation
\[ \vb{x}=\sum_{i=1}^nx_i\vu{e}_i\cma \]
the \( n \)-tuple \( (x_1,\dots,x_n) \) of real numbers are the coordinates of \( \vb{x} \) in a \emph{rectangular} or \emph{Cartesian} coordinate system. The name `rectangular' comes from the fact that it is very easy to specify rectangular regions (i.e.\ \( n \)-dimensional boxes) using this coordinate system; they are simply the set of vectors whose coordinates lie in a Cartesian product of intervals
\[ \qty{\vb{x}\in\R^n\mid (x_1,\dots,x_n)\in [a_1,b_1]\times\dots\times[a_n,b_n]} \]
where \( a_i,b_i\in\R \).

\vspace{3mm}

Sometimes, it is convenient to express vectors in different coordinate systems. One reason is to exploit a particular symmetry that a function may exhibit that is not too clear to realise in Cartesian coordinates. Another reason is that the domain of the function may not be a rectangular region (or the entire vector space), and so it becomes easier to specify the vectors that lie in the domain using different coordinates.

\vspace{3mm}

To change from Cartesian coordinates to a different coordinate system, one applies a suitable \emph{coordinate transformation} \( T \). To express a mapping \( f:\R^n\to\R^m \) with respect to our new coordinate system, we simply take the composition \( f\circ T \). Thus, to compute the derivative of \( f \) with respect to the new coordinates, we differentiate \( f\circ T \). This is where the Chain Rule comes in.

\vspace{3mm}

Let's consider a concrete example. A popular choice of coordinates in \( \R^2 \) is the \emph{plane polar coordinate system}, in which every \( \vb{x}\in\R^2 \) is specified by its distance \( r \) from the origin, and the angle \( \theta \) it makes with the positive \( x \)-axis (i.e.\ with the basis vector \( \vu{e}_1 \)), measured in a counterclockwise direction. That is, any vector can be represented in terms of the coordinates \( (r,\theta) \), where the corresponding representation is given by the coordinate transformation \( T:[0,\infty)\times[0,2\pi)\to\R^2 \) given by
\[ T(r,\theta)=\mqty(r\cos\theta\\ r\sin\theta)\fstop \]

This coordinate transformation is surjective, i.e.\ every vector in \( \R^2 \) can be represented in plane polar coordinates.

\vspace{3mm}

Now, let's consider the scalar function \( f:\R^2\to\R \) given by
\[ f(x,y)=x^2+y^2\fstop \]

We can compute the derivative of \( f \) with respect to plane polar coordinates using the Chain Rule.
\begin{align*}
  \D(f\circ T)(r,\theta)&= \D f(T(r,\theta))\circ\D T(r,\theta)\\
  &= \eval{\mqty(\pdv{f}{x} & \pdv{f}{y})}_{(x,y)=(r\cos\theta,r\sin\theta)} \mqty(\pdv{x}{r} &\pdv{x}{\theta}\\\pdv{y}{r} &\pdv{y}{\theta})\\
  &= \eval{\mqty(2x & 2y)}_{(x,y)=(r\cos\theta,r\sin\theta)}\mqty(\cos\theta & -r\sin\theta\\\sin\theta & r\cos\theta)\\
  &= \mqty(2r\cos\theta &2r\sin\theta)\mqty(\cos\theta & -r\sin\theta\\\sin\theta & r\cos\theta)\\
  &= \mqty(2r\cos^2\theta+2r\sin^2\theta & -2r^2\sin\theta\cos\theta+2r^2\sin\theta\cos\theta)\\
  &= \mqty(2r & 0)
\end{align*}

Since \( \D(f\circ T)(r,\theta)=\mqty(\pdv{(f\circ T)}{r} &\pdv{(f\circ T)}{\theta}) \), by comparison with the above expression we see that
\begin{align*}
  \pdv{(f\circ T)}{r}&= 2r\\
  \pdv{(f\circ T)}{\theta}&=0\fstop
\end{align*}

Of course, we still have the interpretation that the partial derivatives are the infinitesimal change of the function with respect to a single coordinate, holding all others fixed. So for instance, \( \pdv{(f\circ T)}{r} \) is the infinitesimal rate of change of \( f \) in the radial direction \( \vu{r} \). Which way does the radial direction point? If we perturb \( r \) slightly whilst holding the other coordinate, \( \theta \), fixed, the \( x \)-coordinate changes by an amount \( \pdv{x}{r} \), and the \( y \)-coordinate changes by an amount \( \pdv{y}{r} \), so a vector \( \va{r} \) pointing in the radial direction is given by
\[ \va{r}=\pdv{x}{r}\vu{e}_1+\pdv{y}{r}\vu{e}_2=\cos\theta\vu{e}_1+\sin\theta\vu{e}_2\fstop \]

To obtain a unit vector \( \vu{r} \), we then normalise the length of \( \va{r} \)
\[ \vu{r}=\frac{\va{r}}{\norm{\va{r}}}=\cos\theta\vu{e}_1+\sin\theta\vu{e}_2\fstop \]

Here, it so happened that \( \va{r} \) already had unit length. Notice that the direction of \( \vu{r} \) changes depending on where you are in \( \R^2 \), as opposed to the direction of the basis vectors \( \vu{e}_1 \) and \( \vu{e}_2 \), whose directions are the same everywhere. The unit vector \( \vu{r} \) always points radially away from the origin, as expected.

\vspace{3mm}

We can play a similar game to compute the unit vector \( \vu*{\theta} \)
\[ \vu*{\theta}=\qty(\pdv{x}{\theta}\vu{e}_1+\pdv{y}{\theta}\vu{e}_2)\qty(\qty(\pdv{x}{\theta})^2+\qty(\pdv{y}{\theta})^2)^{-\flatfrac{1}{2}}=\frac{1}{r}\Big(-r\sin\theta\vu{e}_1+r\cos\theta\vu{e}_2\Big)=-\sin\theta\vu{e}_1+\cos\theta\vu{e}_2\fstop \]

For some \( \vb{x}=(r\cos\theta,r\sin\theta)^T\in\R^2 \), \( \vu*{\theta} \) is tangential to a circle of radius \( r \) centred at the origin (pointing in a counterclockwise direction). This aligns with our intuition that \( \theta \) is an angular coordinate.

\vspace{3mm}

Now, notice that the partial derivative of \( f\circ T \) with respect to \( \theta \) vanishes, which reveals that \( f \) possesses \emph{angular symmetry}, i.e.\ the value of \( f \) is constant on any circle centred on the origin. Thus, the value of \( f \) at a point \( \vb{x} \) is only dependent on the distance \( r \) of \( \vb{x} \) from the origin. This is an instance of a property of a function that becomes much easier to notice when expressed in a more appropriate coordinate system.

\vspace{3mm}

We can also observe this by expressing \( f \) directly in plane polar coordinates
\[ f\circ T(r,\theta)=f(r\cos\theta,r\sin\theta)=r^2\cos^2\theta+r^2\sin^2\theta=r^2\fstop \]

Notice that \( f\circ T \) is independent of the angular coordinate \( \theta \), which is consistent with our previous observations.

\vspace{3mm}

When expressing functions in different coordinate systems, it is common to adopt the abuse of notation by dropping the coordinate transformation \( T \). That is, one would write \( f(r,\theta) \) to mean \( f\circ T(r,\theta) \). To express \( f \) in Cartesian coordinates, one would simply write \( f(x,y) \). So, the coordinates appearing as arguments of the function indicate indicate which particular coordinate system we are in. Here, the coordinate pair \( (r,\theta) \) is implicitly understood to be plane polar coordinates and \( (x,y) \) is understood to designate Cartesian coordinates. For the sake of clarity, we will \emph{not} adopt such conventions, but warn the reader that such (mal)practice is standard.
\section{Mappings of Class \( \mathcal{C}^1\)}
\label{sec:C1}
\begin{definition}
  A mapping \( f:U\to F \) is said to be of \textbf{class \(\vb*{\mathcal{C}^1}\)} if it is differentiable on \( U \) and its derivative map \( \D f:U\to L(E,F) \) is continuous on \( U \).
\end{definition}

\begin{theorem}
  \label{thm:scalar-C1}
  A scalar function \( f:U\to \R \) is \( \mathcal{C}^1 \) if and only if all of the partial derivatives of \( f \) with respect to the coordinates of an orthonormal basis in \( E \) exist and are continuous on \( U \).
\end{theorem}
\begin{proof}
  (\( \implies \)) Suppose \( f \) is \( \mathcal{C}^1 \). So \( f \) is differentiable and its derivative map \( \D f:U\to E^* \) is continuous. Recall that \( \D f \) has the following representation in terms of its partial derivative mappings
  \[ \D f(x)=\sum_{i=1}^n{\pdv{f}{x_i}}(x)\,v_i^* \]
  where \( v_i^*:E\to\R \) is the dual basis vector corresponding to the \( i^{\text{th}} \) coordinate.

  \vspace{3mm}

  Let \( k\in\qty{1,\dots,n} \) and let \( x_0\in U \). Then for all \( x\in U \), we have that
  \begin{align*}
   0&\leq\abs{{\pdv{f}{x_k}}(x)-{\pdv{f}{x_k}}(x_0)}\\
    &\leq \norm{{\pdv{f}{x_k}}(x)-{\pdv{f}{x_k}}(x_0)}_\infty\\
    &=\norm{\D f(x)-\D f(x_0)}\fstop
  \end{align*}

  Thus, by taking the limit as \( x\to x_0 \) in the above inequality, we see that \( \norm{\D f(x)-\D f(x_0)}\to 0 \) since \( \D f \) is continuous at \( x_0 \), and thus by the Squeeze Theorem we have that
  \[ \lim_{x\to x_0}\abs{{\pdv{f}{x_k}}(x)-{\pdv{f}{x_k}}(x_0)}=0 \]
  which shows that \( \pdv{f}{x_k} \) is continuous at \( x_0 \).

  \vspace{3mm}

  (\( \impliedby \)) Let \( \qty{e_1,\dots,e_n} \) be an orthonormal basis on \( E \) and suppose that all of the partial derivative mappings of \( f \) with respect to the coordinates of \( \qty{e_1,\dots,e_n} \) exist and are continuous on \( U \).

  Let \( x_0=\sum_{i=1}^na_ie_i\in U \).

  Let \( \epsilon>0 \).

  By continuity of the partial derivatives, for each \( i\in\qty{1,\dots,n} \), there exists a \( \delta_i>0 \) such that for all\footnote{By \( B_\infty(x_0,\delta_i) \), we mean the open ball with centre \( x_0 \) and radius \( \delta_i \) constructed from the metric induced by the maximum norm.} \( x\in B_\infty(x_0,\delta_i) \), we have
  \[ \abs{\pdv{f(x)}{x_i}-\pdv{f(x_0)}{x_i}}<\frac{\epsilon}{n}\fstop \]

  Choose \( \delta=\min_{i\in\qty{1,\dots, n}}\delta_i \), and let \( x=\sum_{i=1}^nb_ie_i\in B_\infty(x_0,\delta) \). Now, for each \( k\in\qty{1,\dots,n-1} \), we define the point \( x_k\in E \) by
  \[ x_k=\sum_{i=k+1}^na_ie_i+\sum_{i=1}^kb_ie_i \]
  i.e.\ the finite sequence \( x_0,x_1,x_2,\dots,x_{n-1},x_n=x \) gradually replaces the coordinates of \( x_0 \) one component at a time with that of \( x \). We have that \( x_k\in B_\infty(x_0,\delta) \) for each \( k \), since
  \[ \norm{x_k-x_0}_\infty=\max_{i\in\qty{1,\dots,k}}\abs{b_i-a_i}\leq\max_{i\in\qty{1,\dots,n}}\abs{b_i-a_i}=\norm{x-x_0}_\infty<\delta\fstop \]

  Observe that we can express \( f(x)-f(x_0) \) as the following telescoping sum
  \[ f(x)-f(x_0)=\sum_{i=1}^nf(x_i)-f(x_{i-1})\fstop \]

  For each \( k\in\qty{1,\dots,n} \), we consider the restriction of \( f \) to the line segment connecting \( x_{k-1} \) and \( x_k \), which defines a one-variable function \( g_k:[0,b_k-a_k]\to\R \) by\footnote{Without loss of generality, we assume that \( a_k<b_k \). If \( b_k<a_k \), we construct \( g_k \) on the interval \( [b_k-a_k,0] \). When we apply the Mean Value Theorem, we pick up a minus sign from both the numerator and denominator, which cancel, giving us the same expression as before.}
  \[ g_k(t)=f(x_0+te_k)\fstop \]

  We see that \( g_k \) is a differentiable one-variable function on \( [0,b_k-a_k] \), since the derivative is simply given by the partial derivative of \( f \) in the \( k^{\text{th}} \) coordinate
  \[ g_k'(t)=\eval{\pdv{f(x)}{x_k}}_{x=x_0+te_k}\fstop \]

  Hence, by the Mean Value Theorem from one-variable calculus, there exists a \( c_k\in [0,b_k-a_k] \) such that
  \[ g_k'(c_k)=\frac{g_k(b_k-a_k)-g_k(0)}{b_k-a_k} \]
  i.e.\ we have that
  \[ {\pdv{f(x_0+c_k e_k)}{x_k}}(b_k-a_k)=f(x_k)-f(x_{k-1})\fstop \]

  Putting this all together, we have that
  \begin{align*}
    f(x)-f(x_0)&=\sum_{i=1}^n{\pdv{f(x_0+c_i e_i)}{x_i}}(b_i-a_i)\\
    &=\sum_{i=1}^n{\pdv{f(x_0+c_i e_i)}{x_i}}v_i^*(x-x_0)\\
    &= \sum_{i=1}^n\qty[{\pdv{f(x_0)}{x_i}}-\qty({\pdv{f(x_0)}{x_i}}-{\pdv{f(x_0+c_i e_i)}{x_i}})]v_i^*(x-x_0)\\
    &= [\D f(x_0)](x-x_0)+R(x)
  \end{align*}
  where
  \[ R(x)=\sum_{i=1}^n\qty({\pdv{f(x_0+c_i e_i)}{x_i}}-{\pdv{f(x_0)}{x_i}})(b_i-a_i)\fstop \]

  Taking absolute values and applying the triangle inequality, we have that
  \begin{align*}
    \abs{R(x)}&\leq\sum_{i=1}^n\abs{{\pdv{f(x_0+c_i e_i)}{x_i}}-{\pdv{f(x_0)}{x_i}}}\abs{b_i-a_i}\\
    &\leq\sum_{i=1}^n\abs{{\pdv{f(x_0+c_i e_i)}{x_i}}-{\pdv{f(x_0)}{x_i}}}\norm{x-x_0}_\infty\\
    &<\epsilon\norm{x-x_0}_\infty
  \end{align*}
  i.e.\ we have that
  \[ \frac{\abs{R(x)}}{\norm{x-x_0}_\infty}<\epsilon\fstop \]

  This shows that \( f \) is differentiable at \( x_0 \), with derivative
  \[ \D f(x_0)=\sum_{i=1}^n{\pdv{f(x_0)}{x_i}}v_i^*\fstop \]

  Since the choice of \( x_0\in U \) was arbitrary, it follows that \( f \) is differentiable on \( U \). Furthermore, since all of the partial derivative maps are continuous functions on \( U \), it follows that \( \D f \) is continuous because it a sum of continuous maps. Hence, \( f \) is \( \mathcal{C}^1 \).
\end{proof}

\begin{theorem}
  A mapping \( f:U\to F \) is \( \mathcal{C}^1 \) if and only if all of the partial derivatives of \( f \) with respect to the coordinates of an orthonormal basis in \( E \) exist and are continuous on \( U \).
\end{theorem}
\begin{proof}
  (\( \implies \)) Suppose \( f \) is \( \mathcal{C}^1 \). Thus, \( f \) is differentiable, and its derivative \( \D f:U\to L(E,F) \) is continuous. This implies that all of the component functions (with respect to some orthonormal basis) \( f_i:U\to\R \) are differentiable scalar functions by \Cref{thm:comp-diff}, and the components of the derivative \( \D f_i:U\to E^* \) are continuous mappings by \Cref{thm:comp-cont}. Thus, each of the component functions \( f_i \) are \( \mathcal{C}^1 \). We then apply \Cref{thm:scalar-C1} to conclude that for each \( i\in\qty{1,\dots,m} \), the partial derivative mappings
  \[ \pdv{f_i}{x_j}:U\to\R \]
  exist and are continuous for each \( j=1,\dots, n \).

  \vspace{3mm}

  (\( \impliedby \)) Suppose that all of the partial derivative mappings of \( f \) with respect to the coordinates of an orthonormal basis in \( E \) exist and are continuous on \( U \). We can apply \Cref{thm:scalar-C1} to conclude that the component function \( f_i:U\to\R \) is \( \mathcal{C}^1 \) for each \( i\in\qty{1,\dots,m} \). So each \( f_i \) is differentiable, and each component of the derivative \( \D f_i:U\to E^* \) is continuous. Thus, by \Cref{thm:comp-diff}, we see that \( f \) is differentiable, and from \Cref{thm:comp-cont}, we see that \( \D f:U\to L(E,F) \) is continuous. This shows that \( f \) is \( \mathcal{C}^1 \).
\end{proof}
\section{Higher Order Derivatives}
\begin{definition}
  A mapping \( f:U\to F \) is said to be of \textbf{class \(\vb*{\mathcal{C}^k}\)} (for some \( k\in\N \)) if it is \( k \)-times differentiable on \( U \) and its derivative mappings \( \D^i f:U\to L(E,F) \) are continuous on \( U \) for all orders \( i \) up to \( k \) (i.e.\ for all \( 1\leq i\leq k \)).
\end{definition}

\begin{theorem}
  (Clairaut-Schwarz Theorem) Suppose \( f:U\to\R \) is \( \mathcal{C}^2 \). Then
  \[ \pdv[2]{f}{x_i}{x_j}=\pdv[2]{f}{x_j}{x_i} \]
  for all \( i,j\in\qty{1,\dots, n} \).
\end{theorem}

\begin{definition}
  A mapping \( f:U\to F \) is said to be of \textbf{class \(\vb*{\mathcal{C}^\infty}\)} or \textbf{smooth} if it is \( \mathcal{C}^k \) for all \( k\in\N \).
\end{definition}
\section{Taylor's Theorem}
\section{Optimisation}
\begin{definition}
  Let \( f:U\to F \) be differentiable. A point \( x_0\in U \) is called a \textbf{critical point} of \( f \) if \( \D f(x_0)=0 \).
\end{definition}

\begin{definition}
  Let \( f:U\to\R \) be a scalar function.
  \begin{enumerate}[label=(\alph*)]
  \item A point \( x_0\in U \) is called a \textbf{local minimum} of \( f \) if there exists a neighbourhood \( A \) of \( x_0 \) such that \( f(x_0)\leq f(x) \) for all \( x\in A \).
  \item A point \( x_0\in U \) is called a \textbf{local maximum} of \( f \) if there exists a neighbourhood \( A \) of \( x_0 \) such that \( f(x)\leq f(x_0) \) for all \( x\in A \).
  \item A point \( x_0\in U\) is called a \textbf{local extremum} if it is either a local minimum or local maximum.
  \end{enumerate}
\end{definition}

\begin{theorem}
  Let \( f:U\to\R \) be differentiable, and suppose \( x_0\in U \) is a local extremum. Then \( x_0 \) is a critical point of \( f \).
\end{theorem}

\begin{definition}
  Let \( V \) be a (real) vector space. A mapping \( g:V\cross V\to\R \) is called a \textbf{bilinear form} if for all \( x,y,z\in V \) and \( a\in\R \), we have
  \begin{enumerate}[label=(\alph*)]
  \item \( g(x+y,z)=g(x,z)+g(y,z) \).
  \item \( g(ax,y)=ag(x,y) \).
  \item \( g(x,y+z)=g(x,y)+g(x,z) \).
  \item \( g(x,ay)=ag(x,y) \).
  \end{enumerate}
\end{definition}

\begin{definition}
  Let \( V \) be a vector space. A bilinear form \( g:V\cross V\to\R \) is called \textbf{symmetric} if \( g(x,y)=g(y,x) \) for all \( x,y\in V \).
\end{definition}

\begin{definition}
  Let \( V \) be a vector space and let \( g:V\cross V\to\R \) be a symmetric bilinear form. The \textbf{quadratic form} associated with \( g \) is the mapping \( Q:V\to\R \) defined by
  \[ Q(x)=g(x,x)\fstop \]
\end{definition}

\begin{definition}
  Let \( V \) be a vector space, and let \( Q:V\to\R \) be a quadratic form.
  \begin{enumerate}[label=(\alph*)]
  \item We say that \( Q \) is \textbf{positive definite} if \( Q(x)>0 \) for all \( x\in V\backslash\qty{0} \).
  \item We say that \( Q \) is \textbf{negative definite} if \( Q(x)<0 \) for all \( x\in V\backslash\qty{0} \).
  \end{enumerate}
\end{definition}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "multivar"
%%% End: