%Preliminaries chapter

\chapter{Preliminaries}
\section{Vector Spaces}
Our end goal is to do \emph{multivariable} calculus. That is, we want to differentiate and integrate (real) vector-valued functions of several variables. To this end, we'll need to rigourously understand what a vector is so that we can comfortably manipulate them (beyond the highschool `definition' of `a vector is a quantity with both a magnitude and a direction'). Thus, we'll begin with a lightning review of basic concepts from linear algebra, beginning with the \emph{actual} definition of a vector. A vector is simply an element of a structure called a \emph{vector space}, which we define below:
\begin{definition}
  A \textbf{vector space} over a field \( \mathbb{K} \) is a set \( V \), together with the operations of `vector addition' \( f:V\cross V\to V \) and `scalar multiplication' \(g:\mathbb{K}\cross V\to V  \), typically denoted by \(f(x,y)=x+y  \) and \( g(\alpha,x)=\alpha x \), which satisfy the following axioms:
  \begin{enumerate}[label=(\alph*)]
  \item \( V \) is an abelian group with respect to the binary operation of vector addition.
  \item Associativity of scalar multiplication: For every \( v\in V \) and \( a,b\in\mathbb{K} \), \( (ab)v=a(bv) \).
  \item Distributivity of scalar multiplication: For every \( v\in V \) and \( a,b\in\mathbb{K} \), \( (a+b)v=av+bv \).
  \item Distributivity of scalar multiplication over vector addition: For every \( v,w\in V \) and \( a\in\mathbb{K} \), \( a(v+w)=av+aw \).
  \item Multiplicative identity: For every \( v\in V \), \( 1v=v \).
  \end{enumerate}
\end{definition}

We will only consider vector spaces over the field of real numbers; from now on we will let the arbitrary field \( \mathbb{K} \) be \( \R \), and herein when we say `vector space' we are implicitly referring to a \emph{real} vector space. The prototypical example of a vector space is the very first vector space that we all worked with before we even heard the term `vector space': namely, \( n \)-tuples of real numbers, \( \R^n \). Indeed, this will basically be the only vector space we will care about, as we will later see.

\vspace{3mm}

We'll now fly through some standard linear algebra concepts. Let \( V \) be a vector space. A \textbf{subspace} is a subset \( S\subset V \) such that \( S \) is itself a vector space under the same vector addition and scalar multiplication operations, restricted to \( S \). Fortunately, to check that a subset of a vector space is a subspace, one does not actually have to check every single one of the vector space axioms. A necessary and sufficient condition for this is simply for the subset to be closed under vector addition and scalar multiplication, i.e.\ for all \( x,y\in S \) and \( a\in\R \), \( x+y\in S \) and \( ax\in S \).

\vspace{3mm}

A mapping \( f:V\to W \) between vector spaces that preserves the vector space structure is called a \textbf{linear map} or a \textbf{vector space homomorphism}, i.e.\ for all \( x,y\in V \) we have \( f(x+y)=f(x)+f(y) \) and for all \( a\in\R \) we have \( f(ax)=af(x) \). A bijective homomorphism is called an \textbf{isomorphism}. Two vector spaces are \emph{isomorphic} is there exists an isomorphism between them. Isomorphic vector spaces are essentially `the same' for all intents and purposes; vector spaces that are isomorphic share all the same algebraic properties.

\vspace{3mm}

Let \( f:V\to W \) be a vector space homomorphism. The set \( \ker f=\qty{x\in V\mid f(x)=0} \) is called the \textbf{kernel} of \( f \), and the set \( \mathcal{R}(f)=\qty{f(x)\mid x\in V} \) is called the \textbf{range} of \( f \). It can be easily shown that \( \ker f \) is a subspace of \( V \) and \( \mathcal{R}(f) \) is a subspace of \( W \). Another useful fact worth noting is that a linear map \( f \) is injective if and only if its kernel is trivial (i.e.\ \( \ker f=\qty{0} \)). Hence, \( f \) is an isomorphism if and only if \( \ker f=\qty{0} \) and \( \mathcal{R}(f)=W \).

\vspace{3mm}

A particularly important way that we use to classify vector spaces is the notion of dimension, which intuitively speaking is the `number of degrees of freedom' it possesses. We will proceed to formalise this below. To find out how many degrees of freedom a vector space has, we essentially need to find the minimum number of fixed vectors required to write any arbitrary vector from the space as some weighted sum of these fixed vectors. We call such weighted sums of vectors \textbf{linear combinations}; a linear combination of the vectors in the subset \( \qty{v_1,\dots,v_n} \) is a sum \( \sum_{i=1}^nc_iv_i \) for some constants \( c_i\in\R \). The \textbf{span} of a subset \( \qty{v_1,\dots,v_n} \) is the set of all possible linear combinations of vectors in the set. Hence, in order to describe the entire vector space in terms of sums of vectors from one of its subsets \( \qty{v_1,\dots,v_n} \), we require that \( V=\text{span}\qty{v_1,\dots,v_n} \).

\vspace{3mm}

However, even if we are able to identify a finite subset \( \qty{v_1,\dots,v_n} \) of \( V \) such that \( V=\text{span}\qty{v_1,\dots,v_n} \), it is possible that we are able to find a smaller subset that does the trick, which suggests that the original subset may contain redundant information. The way we describe this redundancy is through the concept of linear dependence:

\begin{definition}
  Let \( V \) be a vector space. A subset \( A\subset V \) is \textbf{linearly independent} if for every finite subset \( \qty{v_1,\dots, v_n} \) of \( A \), we have that \(a_1v_1+\dots +a_nv_n=0  \) for scalars \( a_1,\dots, a_n\in\mathbb{K} \) implies that \( a_1=\dots =a_n=0 \).

  \vspace{3mm}

  If \( A \) is not linearly independent, then it is \textbf{linearly dependent}.
\end{definition}

At last, we arrive at our desired criteria for a subset of a vector space to summarise all of the information of \( V \) in the most minimalistic way possible:

\begin{definition}
  Let \( V \) be a vector space. A \textbf{basis} for \( V \) is a linearly independent subset \( A\subset V \) such that \( V=\text{span }A \).
\end{definition}

\emph{`Fun' remark:} The definition we provide above is that of a \emph{Hamel} basis. There are other types of bases. However, this distinction is not relevant for us in the finite dimensional setting, which we will be exclusively working in.

\vspace{3mm}

Given a basis \( \qty{v_1,\dots,v_n} \) for a vector space \( V \), the representation of a vector \( v\in V \) in that basis is unique. That is, if \( v=a_1v_1+\dots+a_nv_n \) and \( v=b_1v_1+\dots+b_nv_n \) for constants \( a_i,b_i\in\R \) for \( i=1,\dots, n \), we have that \( a_i=b_i \) for each \( i \). This fact follows from the linear independence of the basis vectors.

\vspace{3mm}

So, we would like to say that the number of elements in a basis quantifies the number of degrees of freedom that the vector space possesses. However, one concern that arises is whether in our definition above that there is a possibility that there are bases with different numbers of elements. Fortunately, the answer is no, as we will now show:
\begin{theorem}
  \label{thm:dim}
  Let \( V \) be a vector space. Then every basis of \( V \) has the same number of elements, or are all infinite.
\end{theorem}
\begin{proof}
  Suppose \( \qty{v_1,\dots ,v_n} \) is a basis for \( V \) and suppose for a contradiction that \( \qty{x_1,\dots, x_{n+1}} \) is a linearly independent subset of \( V \). We can write \( x_1 \) as a linear combination of the basis elements
  \[ x_1=a_{11}v_1+\dots a_{1n}v_n \]
  where \( a_{1i}\in\R \) for all \( i=1,\dots, n \). Since \(\qty{x_1,\dots, x_{n+1}}  \) is a linearly independent set, then \( x_1\neq 0 \), so that not all of the \( a_{1i} \)'s are zero. Without loss of generality, suppose that \( a_{11}\neq 0 \) (else swap its label with one of the \( a_{1i} \)'s that is nonzero, and also exchange the indices of the corresponding basis vectors accordingly). Then we can rearrange the above expression to solve for \( v_1 \)
  \[ v_1=\frac{1}{a_{11}}\qty(x_1-a_{12}v_2-\dots-a_{1n}v_n)\fstop \]
  
  It follows that the set \( \qty{x_1,v_2,\dots v_n} \) is a basis for \( V \). To see this, let \( y\in V \). Then \( y=c_1v_1+\dots +c_nv_n \) for constants \( c_1,\dots,c_n\in\R \) (since the \( v_i \)'s form a basis for \( V \)). Thus, we have that
  \[ y=\frac{c_1}{a_{11}}x_1+\qty(c_2-\frac{a_{12}}{a_{11}})v_2+\dots+\qty(c_n-\frac{a_{1n}}{a_{11}})v_n\fstop \]
  
  So \( y \) can be written as a linear combination of the vectors \( x_1,v_2,\dots,v_n \), i.e.\ \( y\in\text{span}\qty{x_1,v_2,\dots,v_n} \). Since the choice of \( y\in V \) was arbitrary, we have that \( V=\text{span}\qty{x_1,v_2,\dots,v_n} \). Linear independence of this set follows from the linear independence of the original basis. Indeed, consider the following vector equation
  \[ k_1x_1+k_2v_2+\dots+k_nv_n=0 \]
  for constants \( k_1,\dots,k_n\in\R \). Expanding \( x_1 \) in terms of the original basis gives
  \begin{align*}
    0&= k_1\qty(a_{11}v_1+\dots a_{1n}v_n)+k_2v_2+\dots+k_nv_n\\
    &= k_1a_{11}v_1+\qty(k_1a_{12}+k_2)v_2+\dots+\qty(k_1a_{1n}+k_n)v_n\fstop
  \end{align*}
  
  Now, by linear independence of \( \qty{v_1,\dots,v_n} \), all of the coefficients in the above equation must vanish. In particular, we see that \( k_1a_{11}=0 \), which implies that \( k_1=0 \) since \( a_{11}\neq 0 \) by assumption. Then, since \( k_1a_{1i}+k_i=0 \) for all \( i=2,\dots, n \), we must have that \( k_i=0 \) as well. Hence, \( \qty{x_1,v_2,\dots, v_n} \) is linearly independent, and our claim is proven.

  \vspace{3mm}

  What have we achieved? We have just replaced one of the basis vectors (namely \( v_1 \)) with one of the vectors from the linearly independent set (namely \( x_1 \)), and after the dust cleared we still have a basis for \( V \). We will continue this process, gradually replacing all of the \( v_i \)'s with an \( x_i \) until we have a basis consisting of only \( x_i \)'s. We will prove that this process will work via induction.

  \vspace{3mm}

  Suppose we have replaced \( j \) of the basis vectors, and we have that \( \qty{x_1,\dots, x_j, v_{j+1},\dots v_n} \) is a basis for \( V \) (after possibly some relabelling of the vectors). We will show that we will obtain a basis by replacing one of the remaining \( v_i \)'s with \( x_{j+1} \). The argument will follow quite similarly to our first replacement process. Write \( x_{j+1} \) in terms of the basis \( \qty{x_1,\dots, x_j, v_{j+1},\dots v_n} \)
  \[ x_{j+1}=a_{j+1,1}x_1+\dots a_{j+1,j}x_j+a_{j+1,j+1}v_{j+1}+\dots+a_{j+1,n}v_n\fstop \]
  
  Since \( x_{j+1}\neq 0 \), then not all of the coefficients \( a_{j+1,i} \) are zero. In fact, we must have that \( a_{j+1,i}\neq 0 \) for some \( i\geq j+1 \) (if this were not the case, then it follows that \( x_{j+1} \) is a linear combination of the vectors \( x_1,\dots, x_j \), which contradicts the linear independence of the \( x_i \)'s). Without loss of generality, we'll take \( a_{j+1,j+1}\neq 0 \). Hence, we can write
  \[ v_{j+1}=\frac{1}{a_{j+1,j+1}}\qty(x_{j+1}-a_{j+1,1}x_1-\dots -a_{j+1,j}x_j-a_{j+1,j+2}v_{j+2}-\dots a_{j+1,n}v_n)\fstop \]
  
  We'll now demonstrate that \( \qty{x_1,\dots,x_{j+1},v_{j+2},\dots,v_n} \) is a basis for \( V \). Let \( y\in V \). Writing \( y \) in terms of the basis \( \qty{x_1,\dots, x_j, v_{j+1},\dots v_n} \) yields
  \[ y=c_1x_1+\dots+c_jx_j+c_{j+1}v_{j+1}+\dots+c_nv_n \]
  for some constants \( c_1,\dots,c_n\in\R \). Substituting in our expression for \( v_{j+1} \) gives
  \[ y=\qty(c_1-\frac{a_{j+1,1}}{a_{j+1,j+1}})x_1+\dots\qty(c_j-\frac{a_{j+1,j}}{a_{j+1,j+1}})x_j+\frac{c_{j+1}}{a_{j+1,j+1}}x_{j+1}+\qty(c_{j+2}-\frac{a_{j+1,j+2}}{a_{j+1,j+1}})v_{j+2}+\dots+\qty(c_n-\frac{a_{j+1,n}}{a_{j+1,j+1}})v_n\fstop \]
  
  Hence \( y\in\text{span}\qty{x_1,\dots,x_{j+1},v_{j+2},\dots,v_n} \). Since the choice of \( y\in V \) was arbitrary, it follows that \( V=\text{span}\qty{x_1,\dots,x_{j+1},v_{j+2},\dots,v_n} \).

  \vspace{3mm}

  Now we'll demonstrate linear independence. Consider the following vector equation:
  \[ k_1x_1+\dots+k_{j+1}x_{j+1}+k_{j+2}v_{j+2}+\dots+k_nv_n=0 \]
  for constants \( k_1,\dots,k_n\in\R \). Expanding \( x_{j+1} \) in terms of the basis \( \qty{x_1,\dots, x_j, v_{j+1},\dots v_n} \) yields
  \begin{align*}
    0&= k_1x_1+\dots+k_{j+1}\qty(a_{j+1,1}x_1+\dots a_{j+1,j}x_j+a_{j+1,j+1}v_{j+1}+\dots+a_{j+1,n}v_n)+k_{j+2}v_{j+2}+\dots+k_nv_n\\
    &= \qty(k_1+k_{j+1}a_{j+1,1})x_1+\dots+\qty(k_j+k_{j+1}a_{j+1,j})x_j+k_{j+1}a_{j+1,j+1}v_{j+1}+\qty(k_{j+2}+k_{j+1}a_{j+1,j+2})v_{j+2}+\\
    &\hspace{6mm}\dots+\qty(k_n+k_{j+1}a_{j+1,n})v_n\fstop
  \end{align*}

  By linear independence of \( \qty{x_1,\dots, x_j, v_{j+1},\dots v_n} \), all of the above coefficients must vanish. In particular, consider the \( j+1 \) coefficient: \( k_{j+1}a_{j+1,j+1}=0 \). From this, we conclude that \( k_{j+1}=0 \) since \( a_{j+1,j+1}\neq 0 \), and hence looking at the remaining coefficients we must have \( k_i=0 \) for all \( i=1,\dots, n \), thus demonstrating the linear independence of \( \qty{x_1,\dots,x_{j+1},v_{j+2},\dots,v_n} \), and hence proving that it is a basis for \( V \).

  \vspace{3mm}

  By induction on the finite set \( \qty{1,\dots,n} \), we can carry out our replacement procedure and end up with the fact that \( \qty{x_1,\dots,x_n} \) forms a basis for \( V \). Now, since \( x_{n+1}\in V \), we can write it as a nonzero linear combination of the basis vectors \( x_1,\dots,x_n \). However, this contradicts the linear independence of the set \( \qty{x_1,\dots,x_{n+1}} \). We conclude that no linearly independent set can have any more vectors than any basis of \( V \), if there exists a finite basis for \( V \). Since bases are linearly independent sets, then no finite basis can have any more vectors than any other basis. So if there exists a finite basis for \( V \), then every basis for \( V \) must also be finite and have the same number of elements. The only other possibility is that every basis of \( V \) contains infinitely many elements.
\end{proof}

This justifies the following definition:
\begin{definition}
  Let \( V \) be a vector space. The \textbf{dimension} of \( V \), denoted \( \dim V \), is the number of elements in any basis of \( V \) if they are finite, or \( \infty \) otherwise.
\end{definition}

We will only consider finite dimensional vector spaces, that is, vector spaces in which there exists a basis with finitely many elements. Doing calculus on infinite dimensional vector spaces crosses into the realm of functional analysis, but we won't stray in that direction in the scope of these notes.

\vspace{3mm}

One key advantage of working in finite dimensional spaces is that things all turn out to be significantly simpler. One easy consequence of the previous theorem is that in an \( n \)-dimensional vector space, if we have a linearly independent set consisting of \( n \) elements, it must actually span the entire space and is thus automatically a basis.
\begin{corollary}
  Let \( V \) be an \( n \)-dimensional vector space and let \( \qty{v_1,\dots,v_n} \) be a linearly independent set. Then \( \qty{v_1,\dots,v_n} \) is a basis for \( V \).
\end{corollary}
\begin{proof}
  Suppose for a contradiction that \( \text{span}\qty{v_1,\dots,v_n}\neq V \). So there exists a \( x\in V \) such that \( x \) is not a linear combination of the \( v_i \)'s. Hence, \( \qty{v_1,\dots,v_n,x} \) is a linearly independent set. So we have a linearly independent set consisting of \( n+1 \) elements in an \( n \)-dimensional space. This is impossible, since every basis of \( V \) must contain \( n \) elements and no linearly independent set can have more elements than any basis of \( V \). We must conclude that \( V=\text{span}\qty{v_1,\dots,v_n} \), and hence \( \qty{v_1,\dots,v_n} \) is a basis for \( V \).
\end{proof}

Another simplification in the finite dimensional setting is that there actually aren't `that many' different types of finite dimensional vector spaces for each dimension \( n\in\N \) to study; we can easily completely classify every real finite dimensional vector space up to isomorphism - even better: they are all structurally identical to \( \R^n \)!

\begin{theorem}
  \label{thm:iso}
  Let \( V \) be an \( n \)-dimensional vector space. Then \( V \) is isomorphic to \( \R^n \).
\end{theorem}
\begin{proof}
  Let \( \qty{v_1,\dots,v_n} \) be a basis for \( V \). Define the mapping \( \phi:V\to\R^n \) to act on \( v=a_1v_1+\dots+a_nv_n \) by
  \begin{align*}
    \phi(v)&=\phi(a_1v_1+\dots+a_nv_n)\\
    &= (a_1,\dots,a_n)^T\fstop
  \end{align*}

  So \( \phi \) is the canonical map that maps a vector \( v\in V \) to an \( n \)-tuple of real numbers, which are simply the coordinates of \( v \) with respect to a certain basis of \( V \). We will show that \( \phi \) is an isomorphism.

  \vspace{3mm}

  Note that the elements mapped by \( \phi \) to \( 0\in\R^n \) must have zero coefficients when expanded in terms of the basis \( \qty{v_1,\dots, v_n} \), and thus can only be the zero vector. This shows that \( \ker\phi=\qty{0} \) and hence that \( \phi \) is injective.

  \vspace{3mm}

  Let \( x\in\R^n \). Then \( x=(a_1,\dots,a_n)^T \) for some \( a_1,\dots,a_n\in\R \). Thus, it is easy to see that the element \( v=a_1v_1+\dots+a_nv_n\in V \) is mapped to \( x \) by \( \phi \). Since the choice of \( x\in\R^n \) was arbitrary, it follows that \( \phi \) is surjective.

  \vspace{3mm}

  Let \( v=a_1v_1+\dots+a_nv_n\in V \) and \( w=b_1v_1+\dots+b_nv_n\in V \). Then
  \begin{align*}
    \phi(v+w)&= (a_1+b_1,\dots,a_n+b_n)\\
    &= (a_1,\dots,a_n)+(b_1,\dots,b_n)\\
    &= \phi(v)+\phi(w)\fstop
  \end{align*}

  Let \( v\in V \) be as above and let \( k\in\R \). Then
  \begin{align*}
    \phi(kv)&= (ka_1,\dots,ka_n)\\
    &= k(a_1,\dots,a_n)\\
    &= k\phi(v)\fstop
  \end{align*}

  Thus, \( \phi \) is a vector space homomorphism. Together with the fact that \( \phi \) is bijective, we have hence shown that \( \phi \) is an isomorphism.
\end{proof}

So any two \( n \)-dimensional vector spaces are isomorphic. This is what we meant when we said that the only \( n \)-dimensional vector space we will care about is \( \R^n \) - all other instances of an \( n \)-dimensional vector space are algebraically equivalent, so we may as well just consider this simple example as being \emph{the} \( n \)-dimensional vector space.

\vspace{3mm}

One more tidbit that we should address is to show that two vector spaces of different dimensions cannot be isomorphic, so that finite dimensional vector spaces are isomorphic if and only if their dimensions are equal (i.e.\ dimension is a vector space property that is preserved by isomorphism). This fact would imply that we have completely classified every single finite dimensional vector space: there is only one unique vector space for each dimension \( n\in\N \) - namely \( \R^n \), and \( \R^n\simeq\R^m \) if and only if \( m=n \).

\begin{proposition}
  Let \( V \) and \( W \) be finite dimensional vector spaces. If \( V \) and \( W \) are isomorphic, then \( \dim V=\dim W \).
\end{proposition}
\begin{proof}
  Let \( n=\dim V \) and let \( \qty{v_1,\dots,v_n} \) be a basis for \( V \). Let \( \phi:V\to W \) be a vector space isomorphism. We will show that \( \qty{\phi(v_1),\dots,\phi(v_n)} \) is a basis for \( W \).

  \vspace{3mm}

  First, we prove linear independence. Consider the following vector equation:
  \[ a_1\phi(v_1)+\dots+a_n\phi(v_n)=0 \]
  where \( a_i\in\R \) for \( i=1,\dots, n \). By linearity of the mapping \( \phi \), this is equivalent to:
  \[ \phi(a_1v_1+\dots+a_nv_n)=0\fstop \]
  
  So \( a_1v_1+\dots+a_nv_n\in\ker\phi \). However, since \( \phi \) is injective, \( \ker\phi=\qty{0} \). It follows that:
  \[ a_1v_1+\dots+a_nv_n=0\fstop \]
  
  By linear independence of \( \qty{v_1,\dots,v_n} \), it follows that \( a_i=0 \) for all \( i=1,\dots,n \). Hence, \( \qty{\phi(v_1),\dots,\phi(v_n)} \) is linearly independent.

  \vspace{3mm}

  Let \( w\in W \). Since \( \phi \) is surjective, there exists a \( v\in V \) such that \( w=\phi(v) \). Express \( v \) in terms of the basis \( \qty{v_1,\dots,v_n} \):
  \[ v=c_1v_1+\dots+c_nv_n\fstop \]
  
  Then:
  \begin{align*}
    w&= \phi(v)\\
    &= \phi(c_1v_1+\dots+c_nv_n)\\
    &= c_1\phi(v_1)+\dots+c_n\phi(v_n) &(\text{by linearity of }\phi)\fstop
  \end{align*}
  
  Hence, \( w\in\text{span}\qty{\phi(v_1),\dots,\phi(v_n)} \). Since the choice of \( w\in W \) is arbitrary, it follows that \( W=\text{span}\qty{\phi(v_1),\dots,\phi(v_n)} \). This shows that \( \qty{\phi(v_1),\dots,\phi(v_n)} \) is a basis for \( W \), which has \( n \) elements. By definition, this implies that \( \dim W=n=\dim V \).
\end{proof}

In summary, we have proven the following very important result about finite dimensional vector spaces:
\begin{theorem}
  \label{thm:iso-samedim}
  Let \( V \) and \( W \) be finite dimensional vector spaces. Then \( V \) and \( W \) are isomorphic if and only if \( \dim V=\dim W \).
\end{theorem}

\section{Euclidean Space}
\label{sec:EuclidSpace}
Now that we have formulated all of the algebraic structure of the multidimensional spaces we will be working in, we will need to endow some further structure onto these spaces in order to do calculus. Of great importance is the ability to measure distances between points/vectors in our space. At the heart of analysis/calculus is the notion of \emph{limits} and \emph{convergence}. We will also frequently want to find bounds and estimates for quantities that may very well be vector-valued. So what we ultimately want is to prescribe some notion of `length' for vectors, which will in turn naturally define a distance between two vectors (simply take the length of the difference of the two vectors). This is captured by the concept of a vector \emph{norm}.

\begin{definition}
  Let \( V \) be a vector space. A norm on \( V \) is a function \( \norm{-}:V\to\R \) satisfying:
  \begin{enumerate}[label=(\alph*)]
  \item Positivity: For all \( x\in V \) \( \norm{x}\geq 0 \). Furthermore, \( \norm{x}=0 \) if and only if \( x=0 \).
  \item Homogeneity of degree 1: For all \( x\in V \) and \( a\in\R \), \( \norm{ax}=\abs{a}\norm{x} \).
  \item Triangle inequality: for all \( x,y\in V \), \( \norm{x+y}\leq \norm{x}+\norm{y} \).
  \end{enumerate}
  A vector space together with a norm is called a \textbf{normed vector space}.
\end{definition}

Another useful concept from coordinate geometry is the notion of angles. This will lend us the ability to define properties such as orthogonality, and perform operations such as projecting a vector onto a subspace. This structure is given rise to by an \emph{inner product}, which we define below:
\begin{definition}
  Let \( V \) be a vector space. A (real) \textbf{inner product} on \( V \) is a function \( \ev{-,-}:V\cross V\to\R \) satisfying:
  \begin{enumerate}[label=(\alph*)]
  \item Symmetry: for all \( x,y\in V \), \( \ev{x,y}=\ev{y,x} \).
  \item Distributivity: for all \( x,y,z\in V \), \( \ev{x+y,z}=\ev{x,z}+\ev{y,z} \).
  \item Homogeneity of degree 1: For all \( x,y\in V \) and \( a\in\R \), \( \ev{ax,y}=a\ev{x,y} \).
  \item Positivity: For all \( x\in V \), \( \ev{x,x}\geq 0 \). Moreover, \( \ev{x,x}=0 \) if and only if \( x=0 \).
  \end{enumerate}
  A vector space together with an inner product is called an \textbf{inner product space}.
\end{definition}

By symmetry of the inner product, we can deduce from the definition that \( \ev{x,ay}=a\ev{x,y} \) and \( \ev{x,y+z}=\ev{x,z}+\ev{y,z} \). Hence, an inner product on a vector space is simply a bilinear function (i.e.\ it becomes a linear function of one variable if we fix the value of the other input variable).

\vspace{3mm}

You are likely already quite familiar with an inner product on \( \R^n \) from coordinate geometry, more commonly known as the dot product or scalar product. Given elements \( x=(x_1,\dots,x_n)^T\in\R^n \) and \( y=(y_1,\dots,y_n)^T\in\R^n \), we define the dot product as:
\[ \ev{x,y}=\sum_{i=1}^nx_iy_i\fstop \]

The notation \( x\cdot y \) is more commonly used instead of \( \ev{x,y} \) in this context. It is not too hard to see that the dot product is indeed an inner product on \( \R^n \).

\vspace{3mm}

One powerful fact about inner products is that they can be used to define a norm in a very natural way. Let \( V \) be an inner product space. Then for any \( x\in V \), we set the norm of \( x \) to be given by \( \norm{x}=\ev{x,x}^{\frac{1}{2}} \). We say that the inner product `induces' a norm on \( V \). So in some sense, once we had defined the notion of angles on our vector space, we have already implicitly fixed some compatible notion of distance as well for free. The positivity and homogeneity properties of the norm follow readily from the properties of the inner product and from the definition of the induced norm. Proving the triangle inequality will take a little more work, and will rely on an identity known as the \emph{Schwarz inequality}:

\begin{proposition}
  (Schwarz Inequality) Let \( V \) be an inner product space. For every \( x,y\in V \), we have
  \[ \ev{x,y}\leq \norm{x}\norm{y}\fstop \]
\end{proposition}
\begin{proof}
  Let \( x,y\in V \) and let \( t\in\R \). By positivity of the inner product, we have that \( \ev{x-ty,x-ty}\geq 0 \). This can be rewritten as
  \begin{align*}
    0&\leq \ev{x-ty,x-ty}\\
    &= \ev{x,x}-t\ev{x,y}-t\ev{y,x}+t^2\ev{y,y}\\
    &= \norm{x}^2-2t\ev{x,y}+t^2\norm{y}^2\fstop
  \end{align*}
  
  So we have a real quadratic equation in the variable \( t \). The quadratic can have at most a single zero, since \( \ev{x-ty,x-ty}=0 \) if and only if \( x-ty=0 \), i.e.\ \( x=ty \). Hence, the discriminant of the quadratic must be either 0 (which corresponds to the single zero) or negative (no real zeros). The discriminant of the quadratic is
  \[ 4\ev{x,y}^2-4\norm{x}^2\norm{y}^2\fstop \]
  
  Putting this all together, we conclude that
  \begin{align*}
    4\ev{x,y}^2-4\norm{x}^2\norm{y}^2&\leq 0\\
    \ev{x,y}&\leq\norm{x}\norm{y}\fstop
  \end{align*}
\end{proof}

From the above proof, we note that equality occurs in the Schwarz inequality if and only if \( x=ty \) (i.e.\ the vectors are scalar multiples of each other).

\vspace{3mm}

We are now ready to tackle the proof that the inner product gives rise to a vector norm:

\begin{theorem}
  Let \( V \) be an inner product space. The function \( \norm{-}:V\to\R \) defined by \( \norm{x}=\ev{x,x}^{\frac{1}{2}} \) is a norm on \( V \).
\end{theorem}
\begin{proof}
  \emph{Positivity:} Let \( x\in V \). We have that \( \norm{x}^2=\ev{x,x}\geq 0 \), and hence \( \norm{x}\geq 0 \). Now, from \( \norm{x}^2=\ev{x,x} \), we can easily see that \( \norm{x}=0 \) if and only if \( x=0 \), by positivity of the inner product.

  \vspace{3mm}

  \emph{Homoegenity of degree 1:} Let \( x\in V \) and \( a\in\R \). Then \( \norm{ax}^2=\ev{ax,ax}=a^2\ev{x,x}=a^2\norm{x}^2 \). Taking square roots gives \( \norm{ax}=\abs{a}\norm{x} \).

  \vspace{3mm}

  \emph{Triangle inequality:} Let \( x,y\in V \). Then:
  \begin{align*}
    \norm{x+y}^2&= \ev{x+y,x+y}\\
    &= \ev{x,x}+2\ev{x,y}+\ev{y,y}\\
    &= \norm{x}^2+2\ev{x,y}+\norm{y}^2\\
    &\leq \norm{x}^2+2\norm{x}\norm{y}+\norm{y}^2 &(\text{by the Schwarz inequality})\\
    &= \qty(\norm{x}+\norm{y})^2\fstop
  \end{align*}
  
  Taking square roots gives the desired result: \( \norm{x+y}\leq\norm{x}+\norm{y} \).
\end{proof}

Finally, we need to formalise the notion of measuring distance in our vector space. The idea of distance is captured by a `metric', which intuitively speaking is a distance-measuring function.

\begin{definition}
  Let \( X \) be a set. A \textbf{metric} on \( X \) is a function \( d:X\cross X\to\R \) satisfying:
  \begin{enumerate}[label=(\alph*)]
  \item Positivity: for all \( x,y\in X \), \( d(x,y)\geq 0 \). Moreover, \( d(x,y)=0 \) if and only if \( x=y \).
  \item Symmetry: for all \( x,y\in X\), \( d(x,y)=d(y,x) \).
  \item Triangle inequality: for all \( x,y,z\in X \), \( d(x,z)\leq d(x,y)+d(y,z) \).
  \end{enumerate}
  A set together with a metric is called a \textbf{metric space}.
\end{definition}

We mentioned earlier that once we had a norm on a vector space, this was sufficient to define a notion of distance. Specifically, we claimed that to measure the distance between two vectors \( x \) and \( y \), we simply need to compute the length of the vector \( x-y \) using the vector norm. Thus, we claim that the norm naturally induces a metric on the vector space.

\begin{theorem}
  Let \( V \) be a normed vector space. The function \( d:V\cross V\to\R \) defined by \( d(x,y)=\norm{x-y} \) is a metric on \( V \).
\end{theorem}
\begin{proof}
  \emph{Positivity:} Let \( x,y\in V \). Then \( d(x,y)=\norm{x-y}\geq 0 \) by positivity of the norm. Furthermore, \( d(x,y)=0 \) if and only if \( x-y=0 \), i.e.\ \( x=y \).

  \vspace{3mm}

  \emph{Symmetry} Let \( x,y\in V \). Then \( d(x,y)=\norm{x-y}=\norm{-(y-x)}=\abs{-1}\norm{y-x}=d(y,x) \), which follows from the homogeneity of the norm.

  \vspace{3mm}

  \emph{Triangle inequality:} Let \( x,y,z\in V \). Then:
  \begin{align*}
    d(x,z)&= \norm{x-z}\\
    &= \norm{(x-y)+(y-z)}\\
    &\leq \norm{x-y}+\norm{y-z} &(\text{since the norm obeys the triangle inequality})\\
    &= d(x,y)+d(y,z)\fstop
  \end{align*}
\end{proof}

In summary, by defining an inner product on our vector space, the inner product induces a norm, which in turn induces a metric. So we get the geometric concepts of angle, length and distance all at once via an inner product. Hence, we call an \( n \)-dimensional vector space with an inner product \textbf{Euclidean \( n \)-space}; where `Euclidean' refers to the fact that we have a means to measure distances in our space.

\vspace{3mm}

Returning to the example of \( \R^n \), we see that \( \R^n \) together with the dot product defines a Euclidean \( n \)-space. In fact, it is interesting to note that the metric induced by the dot product is simply the function that returns the Euclidean distance between two points in \( \R^n \) (i.e.\ the Euclidean metric \( d_2 \)):
\[ d_2(x,y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}\fstop \]

In the previous section, we showed that all \( n \)-dimensional vector spaces are isomorphic to each other, and in particular, \( \R^n \), so we can just talk about \( \R^n \) without any loss of generality. Even better, it is the case that all \( n \)-dimensional inner product spaces are isomorphic, and moreover the inner product is preserved by the isomorphism. Thus, there is essentially only one Euclidean \( n \)-space up to isomorphism: namely, \( \R^n \) with the dot product. The proof of this will be the subject of the following section.

\section{Orthonormal Basis}
Most of the time when we are working with vector spaces, it is very useful to pick a basis and represent all vectors in terms of that basis. Some bases are more convenient to work with than others. Indeed, in a Euclidean space, now that we have some notion of angles provided by the inner product, we can define what it means for two vectors to be orthogonal (the generalisation of perpendicularity from coordinate geometry). Hence, we could try to find bases that are \emph{orthonormal}; the basis vectors are orthogonal and normalised. We'll define these concepts below:
\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. Two elements \( x,y\in E \) are \textbf{orthogonal} if \( \ev{x,y}=0 \).

  \vspace{3mm}
  
  A subset \( A\subset E \) is said to be orthogonal if every pair of distinct elements from \( A \) are orthogonal.
\end{definition}

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. A set \( A\subset E \) is \textbf{orthonormal} if \( A \) is orthogonal and \( \norm{x}=1 \) for all \( x\in A \).
\end{definition}

Orthonormality is a powerful condition. One immediate consequence of the orthonormality of a set is linear independence:
\begin{proposition}
  Every orthonormal set in a Euclidean \( n \)-space is linearly independent.
\end{proposition}
\begin{proof}
  Suppose \( \qty{e_1,\dots,e_m} \) is an orthonormal set in a Euclidean \( n \)-space \( E \) (\( m\leq n \)). Consider the vector equation \( a_1e_1+\dots+a_me_m=0 \) for constants \( a_1,\dots,a_n\in\R \). Taking the inner product of \( e_i \), \( i=1,\dots,m \) with the above equation yields:
  \begin{align*}
    0&= \ev{e_i,\sum_{j=1}^ma_je_j}\\
    &=\sum_{j=1}^ma_j\ev{e_i,e_j}\\
    &= \sum_{j=1}^ma_j\delta_{ij}\\
    &= a_i
  \end{align*}
  where we have made use of the Kronecker delta symbol \( \delta_{ij} \), which returns \( 1 \) if \( i=j \) and 0 otherwise.

  \vspace{3mm}

  
  So \( a_i=0 \) for all \( i=1,\dots, m \). This proves the linear independence of the finite set \( \qty{e_1,\dots,e_m} \).

  \vspace{3mm}

  Now suppose for a contradiction there exists an orthonormal set \( A\subset E \) with more than \( n \) elements. Pick \( n \) elements from \( A \) to form the orthonormal set \( \qty{e_1,\dots,e_n} \), which is linearly independent from what we have just proven. Hence, it forms a basis for \( E \). Pick another \emph{distinct} element \( e_{n+1}\in A \), and write it as a linear combination of the basis elements:
  \[ e_{n+1}=k_1e_1+\dots+k_ne_n\fstop \]
  
  Now take the inner product of the above equation with \( e_{n+1} \):
  \begin{align*}
    \ev{e_{n+1},e_{n+1}}&= \ev{e_{n+1},\sum_{j=1}^nk_je_j}\\
    &= \sum_{j=1}^nk_j\ev{e_{n+1},e_j}\\
    &= 0\fstop
  \end{align*}
  
  However, \( \ev{e_{n+1},e_{n+1}}=\norm{e_{n+1}}^2=1 \), so we have that \( 1=0 \); a contradiction. It follows that there cannot exist an orthonormal set in \( E \) with more than \( n \) elements. Since we have proven that every orthonormal set with \( m\leq n \) elements is linearly independent, then we thus conclude that every orthonormal set in \( E \) is linearly independent.
\end{proof}

One interesting consequence of the above proof is the following. Suppose we could write a vector \( x \) from a Euclidean space \( E \) as a linear combination of orthonormal vectors \( x=a_1e_1+\dots+a_ne_n \). Then we can define a mapping \( \pi_i:E\to\R \) for \( i=1,\dots, n \) by \( \pi_i(x)=\ev{x,e_i}=a_i \). This illustrates another advantage of using an orthonormal basis to represent vectors: finding the coefficients becomes a straightforward task - one simply needs to calculate \( \pi_i(x) \) to find the \( i^{\text{th}} \) coefficient. We call the mapping \( \pi_i \) a \emph{projection operator}; essentially \( \pi_i \) is projecting the vector \( x \) onto the subspace spanned by the \( i^{\text{th}} \) basis vector and picking out the amplitude of the vector projection.

\vspace{3mm}

Now that we are sufficiently hyped up about orthonormal bases, the next natural question to ask is whether it is always possible to find an orthonormal basis for any Euclidean \( n \)-space. We know from the previous result that every orthonormal set is linearly independent. Combined with the fact that in an \( n \)-dimensional space, any linearly independent set consisting of \( n \) elements is automatically a basis, all we need to do is prove the existence of an orthonormal set consisting of \( n \) elements. Spoiler alert: yes, this is certainly possible. Better yet, the proof of this result will be constructive, it illustrates a standard algorithm called the `Gram-Schmidt' process which transforms an arbitrary basis into an orthonormal basis.

\begin{proposition}
  Let \( E \) be a Euclidean \( n \)-space. Then there exists an orthonormal subset \( A\subset E \) consisting of \( n \) elements.
\end{proposition}
\begin{proof}
  Let \( \qty{v_1,\dots,v_n} \) be a linearly independent set in \( E \). It is worth pointing out that none of the vectors are 0 due to linear independence. Thus, we can create an orthonormal set of one element by taking the first element from the set and normalising it: define \( e_1=\flatfrac{v_1}{\norm{v_1}} \). Then \( \qty{e_1} \) is an orthonormal set, and e.g.\ \( \qty{e_1,v_2} \) is a linearly independent set (since \( e_1 \) is simply a scalar multiple of \( v_1 \)). This sets us up to prove the inductive step.

  \vspace{3mm}

  Suppose that we had that \( \qty{e_1,\dots,e_k} \) is an orthonormal set (\( k<n \)) and that \( \qty{e_1,\dots,e_k,v_{k+1}} \) is a linearly independent set. Define:
  \[ y_{k+1}=v_{k+1}-\sum_{j=1}^k\ev{v_{k+1},e_j}e_j\fstop \]
  
  It follows that \( y_{k+1}\neq 0 \) since \( y_{k+1} \) is a linear combination of \( e_1,\dots,e_k,v_{k+1} \), which is a linearly independent set - and so the only way for such a linear combination to yield the zero vector is if all of the coefficients are 0, which is not the case here. Hence, \( \norm{y_{k+1}}\neq 0 \), and so we can safely define:
  \[ e_{k+1}=\frac{y_{k+1}}{\norm{y_{k+1}}}\fstop \]

  So \( \norm{e_{k+1}}=1 \), and for all \( i=1,\dots,k \):
  \begin{align*}
    \ev{e_{k+1},e_i}&=\ev{\frac{1}{\norm{y_{k+1}}}\qty(v_{k+1}-\sum_{j=1}^k\ev{v_{k+1},e_j}e_j),e_i}\\
    &= \frac{1}{\norm{y_{k+1}}}\qty(\ev{v_{k+1},e_i}-\sum_{j=1}^k\ev{v_{k+1},e_i}\ev{e_j,e_i})\\
    &= \frac{1}{\norm{y_{k+1}}}\qty(\ev{v_{k+1},e_i}-\sum_{j=1}^k\ev{v_{k+1},e_j}\delta_{ij})\\
    &= \frac{1}{\norm{y_{k+1}}}\qty(\ev{v_{k+1},e_i}-\ev{v_{k+1},e_i})\\
    &= 0\fstop
  \end{align*}

  Hence, \( \qty{e_1,\dots,e_{k+1}} \) is an orthonormal set. If \( k+1<n \), then there is still an element \( v_{k+2} \) that cannot be written as a linear combination of the orthonormal set \( \qty{e_1,\dots,e_{k+1}} \), since the \( e_i \)'s are simply linear combinations of \( v_1,\dots,v_{k+1} \), but \( \qty{v_1,\dots,v_{k+2}} \) is a linearly independent set. It follows that \( \qty{e_1,\dots,e_{k+1},v_{k+2}} \) is a linearly independent set. We can then proceed via induction to deduce that we can produce an orthonormal set \( \qty{e_1,\dots, e_n} \).
\end{proof}

The previous two results together imply the following:
\begin{theorem}
  Every Euclidean \( n \)-space has an orthonormal basis.
\end{theorem}

Let us return to the original problem that we wished to address in this section: that there is only one Euclidean \( n \)-space up to isomorphism. Since we have already proven that the vector space structure of two Euclidean \( n \)-spaces are equivalent, we simply need to prove that the `extra structure' of the Euclidean space is also preserved under a vector space isomorphism. Let's first make explicit what `extra stuff' needs to be preserved:

\begin{definition}
  Let \( E \) and \( F \) be Euclidean spaces. We say that \( E \) and \( F \) are \textbf{isomorphic} if there exists a mapping \( \phi:E\to F \) such that \( \phi \) is a vector space isomorphism that also preserves the inner product, i.e.\ for all \( x,y\in E \), \( \ev{x,y}=\ev{\phi(x),\phi(y)} \).
\end{definition}

So we require that just the inner product needs to be preserved by the isomorphism. But what about the corresponding norm and metric? Well, if the inner product between Euclidean spaces \( E \) and \( F \) is preserved by an isomorphism \( \phi:E\to F \), it follows that \( \phi \) also preserves the induced norms and metrics on these spaces. To see this, let \( x\in E \) and notice that \( \norm{x}^2=\ev{x,x}=\ev{\phi(x),\phi(x)}=\norm{\phi(x)}^2 \). Now that we have shown that \( \phi \) is norm-preserving, the fact that distances are preserved follows easily. For any \( x,y\in E \) we have that \( d(x,y)=\norm{x-y}=\norm{\phi(x-y)}=\norm{\phi(x)-\phi(y)}=d(\phi(x),\phi(y)) \). Hence, we see that \( \phi \) is an isometry (a distance preserving function).

\vspace{3mm}

Recall from our proof that every \( n \)-dimensional vector space is isomorphic (to \( \R^n \)) (Theorem \ref{thm:iso}), we constructed a function that essentially mapped one basis onto another. In order to preserve the inner product of the Euclidean spaces, the isomorphisms that we will construct in the following proof will be a subset of the isomorphisms that did the trick in the more general vector space case. Specifically, the isomorphisms between Euclidean spaces will, loosely speaking, send \emph{orthonormal bases to orthonormal bases}.

\begin{theorem}
  Let \( E \) and \( F \) be Euclidean \( n \)-spaces. Then \( E \) and \( F \) are isomorphic.
\end{theorem}
\begin{proof}
  Let \( \qty{e_1,\dots,e_n} \) be an orthonormal basis in \( E \), and \( \qty{\overline{e_1},\dots,\overline{e_n}} \) an orthonormal basis in \( F \) (we know that orthonormal bases for Euclidean spaces exist by the previous theorem which we have just proven). We can express any \( x\in E \) as a linear combination of the basis vectors: \( x=a_1e_1+\dots+a_ne_n \). We can hence define the map \( \phi:E\to F \) by
  \[ \phi(x)=a_1\overline{e_1}+\dots+a_n\overline{e_n}\fstop \]
  
  The form of \( \phi \) is exactly that of the isomorphism which we have constructed in the proof of Theorem \ref{thm:iso}, i.e.\ a map that sends a vector represented in a given basis to a vector in the other space with the same coordinates/coefficients but with respect to a basis in the target space\footnote{Specifically, we mapped a basis from an \( n \)-dimensional vector space to the standard basis in \( \R^n \) in that proof.}. Thus, we already have that \( \phi \) is a vector space isomorphism. We thus simply need to demonstrate that \( \phi \) preserves the inner product. Let \( x\in E \) as above and let \( y=b_1e_1+\dots+b_ne_n\in E \). Then
  \begin{align*}
    \ev{x,y}&= \ev{\sum_{i=1}^na_ie_i,\sum_{j=1}^nb_je_j}\\
    &= \sum_{i=1}^na_i\ev{e_i,\sum_{j=1}^nb_je_j}\\
    &= \sum_{i=1}^n\sum_{j=1}^na_ib_j\ev{e_i,e_j}\\
    &= \sum_{i=1}^n\sum_{j=1}^na_ib_j\delta_{ij}&(\text{by orthonormality of the basis vectors})\\
    &= \sum_{i=1}^na_ib_i\fstop
  \end{align*}

  But we also have that
  \begin{align*}
    \ev{\phi(x),\phi(y)}&= \ev{\sum_{i=1}^na_i\overline{e_i},\sum_{j=1}^nb_j\overline{e_j}}\\
    &= \sum_{i=1}^n\sum_{j=1}^na_ib_j\ev{\overline{e_i},\overline{e_j}}\\
    &= \sum_{i=1}^n\sum_{j=1}^na_ib_j\delta_{ij}&(\text{by orthonormality of the basis vectors})\\
    &= \sum_{i=1}^na_ib_i\fstop
  \end{align*}

  So \( \ev{x,y}=\ev{\phi(x),\phi(y)} \), and so the inner product is preserved. Hence, \( \phi \) is an isomorphism of Euclidean spaces.
\end{proof}

Hence, every Euclidean \( n \)-space is `the same' from a structural point of view, so we might as well work with the easiest example of it we can think of: i.e.\ \( \R^n \) with the dot product.

\vspace{3mm}

While on the topic of orthonormal bases, we'll conclude this section with a fun digression. Let \( E \) be a Euclidean \( n \)-space, and suppose we had two orthonormal bases in \( E \): \( \qty{e_1,\dots,e_n} \) and \( \qty{\overline{e_1},\dots,\overline{e_n}} \). We can write the barred basis vectors \( \overline{e_i} \) in terms of the basis vectors \( e_j \):
\[ \overline{e_i}=\sum_{j=1}^na_{ij}e_j \quad i=1,\dots,n\fstop \]

The \( n^2 \) constants \( a_{ij}\in\R \) form a \( n\times n \) matrix \( O \), which represents the linear map \( \phi:E\to E \) defined by \( \phi(a_1e_1+\dots+a_ne_n)=a_1\overline{e_1}+\dots+a_n\overline{e_n} \) (more on this in \Cref{sec:linear-maps}), which we know from above to be an automorphism of \( E \). So if we wrote the components of a vector \( x\in E \) with respect to the orthonormal basis \( \qty{e_1,\dots,e_n} \) as a column vector, then \( \phi(x)=Ox \), where the components of \( \phi(x) \) are also expressed in a column vector. What properties does the matrix \( O \) have? Well, since \( \ev{\overline{e_i},\overline{e_j}}=\delta_{ij} \) by orthonormality, we can substitute our expression for the barred basis vectors in terms of the unbarred basis vectors to obtain:
\begin{align*}
  \delta_{ij}&= \ev{\sum_{k=1}^na_{ik}e_k,\sum_{l=1}^na_{jl}e_l}\\
  &= \sum_{k=1}^n\sum_{l=1}^na_{ik}a_{jl}\ev{e_k,e_l}\\
  &= \sum_{k=1}^n\sum_{l=1}^na_{ik}a_{jl}\delta_{kl}\\
  &= \sum_{k=1}^na_{ik}a_{jk}\fstop
\end{align*}

Suppose that \( b_{ij} \) are the components of the matrix \( O^T \) (the matrix transpose of \( O \)). Then \( b_{kj}=a_{jk} \), so that we have that \( \delta_{ij}=\sum_{k=1}^n a_{ik}b_{kj} \). Since \( \delta_{ij} \) are the components of the \( n\times n \) identity matrix \( I_n \), we can write the above equation in the following matrix form
\[ I_n=OO^T\fstop \]

Since \( O \) is the matrix representation of a vector space automorphism, it is invertible, and hence by premultiplying both sides of the above equation by \( O^{-1} \), we arrive at
\[ O^{-1}=O^T \]
i.e.\ the matrix \( O \) is an orthogonal matrix. This reveals that orthogonal transformations on Euclidean spaces are simply the ones that map orthonormal bases to orthonormal bases.

\section{Dual Space}
\label{sec:dual}
In this section, we study a very special type of vector space mapping. Namely, the linear maps that take a vector as an input and returns a scalar. Such maps are called \textbf{linear functionals}.
\begin{definition}
  Let \( V \) be an \( n \)-dimensional vector space. A \textbf{linear functional} on \( V \) is a linear mapping \( f:V\to\R \).
\end{definition}

\begin{definition}
  Let \( V \) be an \( n \)-dimensional vector space. The \textbf{dual space} \( V^* \) is the set of all linear functionals on \( V \).
\end{definition}

The dual space of a vector space is itself a vector space under the operations of addition and scalar multiplication of real-valued functions. Elements of the dual space are sometimes also referred to as \emph{covectors}.

\vspace{3mm}

There are many important linear functionals that are central to analysis. To name a few, the trace operator, the inner product with one input fixed, and integral operators. It is therefore quite valuable to study them in a general setting.

\vspace{3mm}

In the finite dimensional setting, the study of linear functionals and the dual space is quite simple. In the following proposition, we show that every linear functional on a finite dimensional vector space can be completely characterised simply by knowing what values it takes at finitely many points (specifically, at the basis vectors), and that we can essentially cook up a linear functional to behave in `any way we want' (i.e.\ take specific values at certain desired points).
  \begin{proposition}
    \label{thm:dualprops}
  Let \( V \) be an \( n \)-dimensional vector space and let \( \qty{v_1,\dots,v_n} \) be a basis for \( V \). Then: 
  \begin{enumerate}[label=(\alph*)]
  \item Every linear functional \( f\in V^* \) is completely determined by its values at \( v_1,\dots,v_n \).
  \item For any real numbers \( a_1,\dots, a_n\in\R \) there exists an \( f\in V^* \) such that \( f(v_i)=a_i \) for all \( i=1,\dots,n \).
  \end{enumerate}
\end{proposition}
\begin{proof}
  (a) Let \( f\in V^* \) and let \( x=k_1v_1+\dots+k_nv_n\in V \). Then
  \begin{align*}
    f(x)&= f(k_1v_1+\dots+k_nv_n)\\
    &= k_1f(v_1)+\dots+k_nf(v_n) &(\text{by linearity of }f)\fstop
  \end{align*}
  
  One sees from the above expression that \( f(x) \) can be computed using only the knowledge of the values \( f(v_1),\dots,f(v_n) \). Moreover, this \emph{uniquely} identifies the linear functional. Suppose \( g\in V^* \) agrees with \( f \) at each of the basis vectors, i.e.\ \( f(v_i)=g(v_i) \) for all \( i=1,\dots,n \). Then for any \( x\in V \) with representation as above, we have that
  \begin{align*}
    g(x)&= g(k_1v_1+\dots+k_nv_n)\\
    &= k_1g(v_1)+\dots+k_ng(v_n)\\
    &= k_1f(v_1)+\dots+k_nf(v_n)\\
    &= f(x)\fstop
  \end{align*}

  \vspace{3mm}

  (b) Let \( a_1,\dots, a_n\in\R \). For \( x=k_1v_1+\dots+k_nv_n\in V \), define the mapping \( f:V\to\R \) by
  \[ f(x)=k_1a_1+\dots+k_na_n\fstop \]
  
  Then \( f(v_i)=a_i \) for all \( i=1,\dots,n \), as required. We'll now set about to show that \( f \) is linear.

  \vspace{3mm}

  Let \( x\in V \) as above and let \( y=l_1v_1+\dots l_nv_n\in V \). Then
  \begin{align*}
    f(x+y)&= (k_1+l_1)a_1+\dots+(k_n+l_n)a_n\\
    &= k_1a_1+\dots+k_na_n+l_1a_1+\dots+l_na_n\\
    &= f(x)+f(y)\fstop
  \end{align*}

  Let \( c\in\R \). Then
  \begin{align*}
    f(cx)&= ck_1v_1+\dots+ck_nv_n\\
    &= c(k_1v_1+\dots+k_nv_n)\\
    &= cf(x)\fstop
  \end{align*}
  This proves that \( f \) is a linear map, so that \( f\in V^* \).
\end{proof}

Given a vector space, how big is its dual space? In the infinite dimensional case, it can be shown via the axiom of choice that the dual space is always `bigger' than the original space. In the finite dimensional case however, again we have it easy: the dimensional of the dual space is the same as that of original space, so that they are isomorphic.

\begin{theorem}
  \label{thm:ndual}
  Let \( V \) be an \( n \)-dimensional vector space. Then its dual space \( V^* \) is also an \( n \)-dimensional vector space.
\end{theorem}
\begin{proof}
  We will prove that \( V^* \) is isomorphic to \( \R^n \) as vector spaces, which will imply that \( V^* \) has the same dimension as \( \R^n \), which is \( n \).

  \vspace{3mm}

  We first need to construct the function between \( V^* \) and \( \R^n \) which will serve as the candidate for what we hope will be the isomorphism. Fix a basis \( \qty{v_1,\dots,v_n} \) for \( V \). Let \( f\in V^* \). Define \( \psi:V^*\to\R^n \) by
  \[ \psi(f)=(f(v_1),\dots,f(v_n))^T\fstop \]

  Let \( f,g\in V^* \). Then
  \begin{align*}
    \psi(f+g)&= (f(v_1)+g(v_1),\dots,f(v_n)+g(v_n))^T\\
    &= (f(v_1),\dots,f(v_n))^T+(g(v_1),\dots,g(v_n))^T\\
    &= \psi(f)+\psi(g)\fstop
  \end{align*}

  Let \( a\in\R \). Then
  \begin{align*}
    \psi(af)&=(af(v_1),\dots,af(v_n))^T\\
    &= a(f(v_1),\dots,f(v_n))^T\\
    &= a\psi(f)\fstop
  \end{align*}

  Thus, \( \psi \) is a linear map. We now proceed to show that \( \psi \) is a bijection.

  \vspace{3mm}

  Suppose \( \psi(f)=\psi(g) \). Then \( (f(v_1),\dots,f(v_n))^T=(g(v_1),\dots,g(v_n))^T \), i.e.\ \( f \) and \( g \) agree at each of the basis vectors. By part (a) Proposition \ref{thm:dualprops}, this implies that \( f=g \), thus proving the injectivity of \( \psi \).

  \vspace{3mm}

  Now let \( x=(a_1,\dots,a_n)^T\in\R^n \). By part (b) of Proposition \ref{thm:dualprops}, there exists an \( f\in V^* \) such that \( f(v_i)=a_i \) for all \( i=1,\dots, n \). It follows that \( \psi(f)=x \). Since the choice of \( x\in\R^n \) was arbitrary, it follows that \( \mathcal{R}(\psi)=\R^n \), thus proving surjectivity of \( \psi \). Putting all of this together shows that \( \psi \) is a bijective linear function, i.e.\ a vector space isomorphism.
\end{proof}

Let \( V \) be an \( n \)-dimensional vector space. Since the dual space \( V^* \) is \( n \)-dimensional, it has a basis consisting of \( n \) elements. Given a basis \( \qty{v_1,\dots,v_n} \) for \( V \), there is a particular nice choice for a basis on \( V^* \). For each \( i=1,\dots,n \), define the linear functional \( v^*_i:V\to\R \) by
\[ v^*_i(v_j)=\delta_{ij}, \quad\text{for }j=1,\dots,n\fstop \]

So the linear functionals \( v^*_i \) satisfy a `bi-orthogonality property'. The fact that such linear functionals exist again comes from Proposition \ref{thm:dualprops}. The set \( \qty{v^*_1,\dots,v^*_n} \) is a linearly independent set in \( V^* \). To see this, let \( x=a_1v_1+\dots+a_nv_n\in V \) be arbitrary and consider the following vector equation
\begin{align*}
  0&= \sum_{i=1}^nc_iv^*_i(x)\\
  &= \sum_{i=1}^nc_iv^*_i\qty(\sum_{j=1}^na_jv_j)\\
  &= \sum_{i=1}^n\sum_{j=1}^nc_ia_jv^*_i(v_j)\\
  &= \sum_{i=1}^n\sum_{j=1}^nc_ia_j\delta_{ij}\\
  &= \sum_{i=1}^n c_ia_i\fstop
\end{align*}

Since the above equation must hold for all \( x\in V \), choosing \( x=v_k \) for each \( k=1,\dots,n \) means that \( a_i=\delta_{ik} \) in the above expression, which gives \( c_k=0 \). This demonstrates the linear independence of the \( v^*_i \)'s. Since we have shown that \( V^* \) is an \( n \)-dimensional vector space, and we have that \( \qty{v^*_1,\dots,v^*_n} \) is a linearly independent set in \( V^* \) consisting of \( n \) elements, we know automatically that this must be a basis for \( V^* \). We call this special basis the \textbf{dual basis}.

\vspace{3mm}

Now, since the dual space \( V^* \) is itself a vector space, we can consider its dual space \( V^{**} \), called the \textbf{second dual}. We know that \( V^{**} \) must also be a vector space with the same dimension as \( V \), so they are isomorphic. Better yet, there is a natural identification of elements from \( V \) with elements from \( V^{**} \), so that the isomorphism is canonical.

\begin{theorem}
  Let \( V \) be an \( n \)-dimensional vector space. Then \( V \) is canonically isomorphic to its second dual \( V^{**} \).
\end{theorem}
\begin{proof}
  For each \( x\in V \), define the mapping \( \phi_x:V^*\to\R \) by
  \[ \phi_x(f)=f(x) \]
  i.e.\ \( \phi_x \) is an evaluation map. Given a linear functional on \( V^* \), it returns a scalar by evaluating it at the fixed point \( x \). We will show that \( \phi_x\) is a linear map.

  \vspace{3mm}

  Let \( f,g\in V^* \). Then \( \phi_x(f+g)=(f+g)(x)=f(x)+g(x)=\phi_x(f)+\phi_x(g) \).

  Let \( a\in\R \). Then \( \phi_x(af)=af(x)=a\phi_x(f) \).

  \vspace{3mm}

  Thus, \( \phi_x \) is a linear functional on \( V^* \), so \( \phi_x\in V^{**} \). We can hence define the mapping \( \psi:V\to V^{**} \) by \( \psi(x)=\phi_x \). We will show that \( \psi \) is a vector space homomorphism. Let \( x,y\in V \) and let \( f\in V^*\). Then
  \begin{align*}
    (\psi(x+y))(f)&= \phi_{x+y}(f)\\
    &= f(x+y)\\
    &= f(x)+f(y)\\
    &= \phi_x(f)+\phi_y(f)\\
    &= (\psi(x))(f)+(\psi(y))(f)\fstop
  \end{align*}
  
  Since the choice of \( f\in V^* \) was arbitrary, this shows that \( \psi(x+y)=\psi(x)+\psi(y) \).

  \vspace{3mm}

  Let \( a\in\R \). Then
  \begin{align*}
    (\psi(ax))(f)&= \phi_{ax}(f)\\
    &= f(ax)\\
    &= af(x)\\
    &= a\phi_x(f)\\
    &= a(\psi(x))(f)\fstop
  \end{align*}
  
  Thus, \( \psi(ax)=a\psi(x) \). This proves that \( \psi \) is a vector space homomorphism.

  \vspace{3mm}

  To show the injectivity of \( \psi \), we will prove that its kernel is trivial. Suppose \( x\in\ker\psi \), so \( \psi(x)=0 \). This means that for all \( f\in V^* \), \( f(x)=0 \). If \( x\neq 0 \), then with respect to some basis \( \qty{v_1,\dots,v_n} \) of \( V \), \( x \) can be written as a nontrivial linear combination of the basis vectors: \( x=a_1v_1+\dots+a_nv_n \), where at least one of the \( a_i \)'s are nonzero. Pick a nonzero coefficient and call it \( a_k \). Then by part (b) of Proposition \ref{thm:dualprops}, there exists a \( g\in V^* \) such that \( g(v_k)=1 \) and \( g(v_i)=0 \) for all \( i\neq k \). Then
  \begin{align*}
    g(x)&=g(a_1v_1+\dots+a_nv_n)\\
        &= a_1g(v_1)+\dots+a_kg(v_k)+\dots+a_ng(v_n)\\
        &= a_k\\
        &\neq 0\fstop
  \end{align*}
  
  This is a contradiction. Hence, \( x=0 \), which shows that \( \ker\psi=\qty{0} \), and hence \( \psi \) is injective.

  \vspace{3mm}

  Finally, we show that \( \psi \) is surjective. Let \( \chi\in V^{**} \). Let \( \qty{v^*_1,\dots,v^*_n} \) be the dual basis for \( \qty{v_1,\dots,v_n} \). Take \( a_i=\chi(V^*_i) \) for \( i=1,\dots,n \), and thus construct the element \( x=a_1v_1+\dots+a_nv_n\in V \). We claim that \( \chi=\phi_x \). Let \( f\in V^* \), and expand it in terms of the dual basis
  \[ f=\sum_{i=1}^nc_iv^*_i\fstop \]

  Then we have
  \begin{align*}
    \phi_x(f)&= f(x)\\
    &= \sum_{i=1}^nc_iv^*_i(x)\\
    &= \sum_{i=1}^nc_iv^*_i\qty(\sum_{j=1}^na_jv_j)\\
    &= \sum_{i=1}^n\sum_{j=1}^nc_ia_jv^*_i(v_j)\\
    &= \sum_{i=1}^n\sum_{j=1}^nc_ia_j\delta_{ij}\\
    &= \sum_{i=1}^nc_ia_i\\
    &= \sum_{i=1}^n c_i\chi(v^*_i)\\
    &= \chi\qty(\sum_{i=1}^nc_iv^*_i) &(\text{by linearity of }\chi)\\
    &= \chi(f)\fstop
  \end{align*}

  The above argument holds for all \( f\in V^* \), and thus we conclude that \( \chi=\phi_x=\psi(x) \). This proves that \( \psi \) is surjective, and hence a bijection, and hence an isomorphism.
\end{proof}

The property that a vector space is naturally isomorphic to its second dual is referred to as \emph{algebraic reflexivity}. This is another property that is guaranteed to hold in the finite dimensional setting, but not in the infinite dimensional case. There is always a natural monomorphism (injective homomorphism) from a vector space to its second dual, however it is an isomorphism if and only if the vector space is finite dimensional.

\vspace{3mm}

Naturally, we can keep playing the game of successively finding the dual space of a dual space, but the results in this section show that this is not necessary. It turns out in the finite dimensional setting that a vector space and its dual are of equal dimension, and are isomorphic. So the study of the dual space of a finite dimensional vector space reduces to the study of the vector space itself.

\section{Topology of Euclidean Space}
So far, we have mostly concerned ourselves with the algebraic properties of Euclidean space. We will now consider the structure of Euclidean space that makes it amiable to do calculus on. Recall that the Euclidean inner product induces a vector norm, which in turn induces a metric. To add to this already lengthy chain of implications, the metric induces a \emph{topology} on our Euclidean space. A topological space is the most general setting in which one can define the notion of continuity, which is certainly something we would like to have in a calculus setting!

\vspace{3mm}

Since the topology of Euclidean space is the metric topology (where the metric is the Euclidean metric \( d_2 \)), we will define the open sets via the notion of \emph{open balls}. We have already described the Euclidean metric for \( \R^n \). To generalise it to Euclidean \( n \)-space \( E \), take an orthonormal basis \( \qty{e_1,\dots,e_n} \) of \( E \). Then for any \( x=x_1e_1+\dots+x_ne_n\in E \), the Euclidean norm is given by
\[ \norm{x}=\sqrt{\sum_{i=1}^nx_i^2} \]
with corresponding Euclidean metric
\[ d_2(x,y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2} \]
where \( y=y_1e_1+\dots+y_ne_n \).

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. An \textbf{open ball} of centre \( x\in E \) and radius \( r>0 \) is the set \( B(x,r)=\qty{y\in E\mid d_2(x,y)<r}=\qty{y\in E\mid \norm{x-y}<r} \).

  \vspace{3mm}

  The \textbf{closed ball} of centre \( x \) and radius \( r \) is the set \( \overline{B}(x,r)=\qty{y\in E\mid d_2(x,y)\leq r}=\qty{y\in E\mid \norm{x-y}\leq r} \).
\end{definition}

An open ball centered at a point \( x\in E \) with radius \( r \) simply consists of all of the points that are within a distance \( r \) of \( x \). The closed ball also includes those points that are exactly a distance of \( r \) away from \( x \) (as measured using the Euclidean metric).

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. A set \( U\subset E \) is said to be \textbf{open} if for every \( x\in U \), there exists an \( r>0 \) such that \( B(x,r)\subset U \).
\end{definition}

Some trivial examples of open sets include the empty set, and the whole space \( E \). Open balls themselves are also open sets. An open rectangle, which is simply the Cartesian product of open intervals in \( \R \), is an open set in \( \R^n \).

\vspace{3mm}

Now we address the age-old question of: `once we have a bunch of open sets, how do we make new open sets from old?' This is answered by:

\begin{proposition}
  \label{thm:newopenfromold}
  Let \( E \) be a Euclidean \( n \)-space. Then:
  \begin{enumerate}[label=(\alph*)]
  \item If \( U_\alpha\subset E \) is open for each \( \alpha\in A \), then the union
    \[ U=\bigcup_{\alpha\in A}U_\alpha \]
    is open in \( E \).
  \item If \( U_1,\dots,U_k \) are open subsets of \( E \), then the intersection
    \[ U=U_1\cap \dots \cap U_k \]
    is open in \( E \).
  \end{enumerate}
\end{proposition}
\begin{proof}
  (a) Let \( U_\alpha\subset E \) be open for each \( \alpha\in A \), and consider the set \( U=\bigcup_{\alpha\in A}U_\alpha \). Let \( x\in U \). Then there exists a \( \beta\in A \) such that \( x\in U_\beta \). Since \( U_\beta \) is open, there exists an \( r>0 \) such that \( B(x,r)\subset U_\beta\subset U \). Thus, \( U \) is open.

  \vspace{3mm}

  (b) Let \( U_1,\dots,U_k \) be open, and consider the set \( U=U_1\cap\dots\cap U_k \). Let \( x\in U \). For each \( i=1,\dots,k \), \( x\in U_i \), so there exists an \( r_i>0 \) such that \( B(x,r_i)\subset U_i \). Choose \( r=\min\qty{r_1,\dots,r_k}>0 \). Then \( B(x,r)\subset U_i \) for all \( i=1,\dots, k \). Thus, \( B(x,r)\subset U \), and hence \( U \) is open.
\end{proof}

So arbitrary unions of open sets are open, and \emph{finite} intersections of open sets are open. Infinite intersections of open sets are not necessarily open. For example, consider \( I_n=\qty(-\frac{1}{n},\frac{1}{n}) \), which is an open set in \( \R \) for each \( n\in\N \). But:
\[ \bigcap_{n=1}^\infty I_n=\qty{0} \]
which is \emph{not} open in \( \R \).

\vspace{3mm}

Proposition \ref{thm:newopenfromold} above, in combination with the fact that \( \emptyset \) and \( E \) are open, shows that the collection of open sets in a Euclidean space \( E \) forms a \emph{topology} on \( E \). This fact did not appeal to anything particular about Euclidean space other than its underlying metric space structure.

\vspace{3mm}

We will later want to define the notions of limits and continuity, which both involve the idea of points being forced to being `really close to each other'. Frequently in analysis we will also allow for our variables to vary in a `sufficiently small amount'. To make formal these ideas, we use the topologcal concept of `closeness' in terms of \emph{neighbourhoods} of points:

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space and let \( x\in E \). A \textbf{neighbourhood} of \( x \) is a subset \( U\subset E \) such that there exists an open subset \( V\subset U \) that contains \( x \).
\end{definition}

Note that in the metric topology, an open set containing a point \( x\in E \) must contain an open ball \( B(x,r) \) for some \( r>0 \), so that we can also state our definition of a neighbourhood of a point \( x \) as simply `a set which contains an open ball centred at \( x \)'. Hence, the topological notion of `wiggling around \( x \) in an arbitrarily small amount' (constraining variation to smaller and smaller neighbourhoods) equates to the metric notion of small movement (constraining variation to smaller and smaller open balls).

\vspace{3mm}

Thus, an alternative way to characterise open sets is the statement that `a set is open if and only if it is a neighbourhood of all of its points', which follows immediately from the definition. We can also rephrase this in terms of \emph{interior points}, which we define below:

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. An \textbf{interior point} of a set \( S\subset E \) is an \( x\in S \) such that there exists an \( r>0 \) such that \( B(x,r)\subset S \).
\end{definition}

One sees immediately from the definition above and from the definition of an open set that a set is open if and only if it all of its points are interior points. So if a set \( S \) has any points that are not interior points, it cannot be open. However, if we consider the subset of \( S \) comprising of only the interior points of \( S \), then we get an open subset of \( S \). In fact, this is the largest open subset contained in \( S \), which we call the \emph{interior} of \( S \).

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space and let \( S\subset E \). The \textbf{interior} of \( S \), denoted \( S^\circ \), is the union of all open subsets of \( S \).
\end{definition}

Our definition of the interior above formulates it as the largest open subset contained in \( S \), but it can be readily shown that the interior is simply the set of interior points of \( S \). This means that a set is open if and only if it is equal to its interior.

\vspace{3mm}

Also equally important in a topological space is the notion of a \emph{closed} set:

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. A set \( C\subset E \) is said to be \textbf{closed} if it is the complement of an open set, i.e.\ \( C=E\backslash U \) for some open set \( U\subset E \).
\end{definition}

One should be careful that a set being closed is \emph{not} the negation of a set being open. There are sets that are neither open nor closed (e.g.\ the half-open interval in \( \R \): \( [a,b) \)), and sets that are \emph{both} open and closed (the empty set and the entire space).

\vspace{3mm}

Closed balls are a simple example of closed sets. Cartesian products of closed intervals in \( \R \) are closed sets in \( \R^n \). An important example of closed sets in Euclidean space are vector subspaces. Complementary to Proposition \ref{thm:newopenfromold}, we now list the ways one can combine closed sets to make new closed sets:

\begin{proposition}
  Let \( E \) be a Euclidean \( n \)-space. Then:
  \begin{enumerate}[label=(\alph*)]
  \item If \( C_\alpha\subset E \) is closed for each \( \alpha\in A \), then the intersection
    \[ C=\bigcap_{\alpha\in A}C_\alpha \]
    is closed in \( E \).
  \item If \( C_1,\dots,C_k \) are closed subsets of \( E \), then the union
    \[ C=C_1\cup\dots\cup C_k \]
    is closed in \( E \).
  \end{enumerate}
\end{proposition}
\begin{proof}
  (a) Let \( C_\alpha\subset E \) be closed for each \( \alpha\in A \). Then \( E\backslash C_\alpha \) is open for all \( \alpha\in A \). Hence, \( \bigcup_{\alpha\in A}(E\backslash C_\alpha) \) is an open set, since arbitrary unions of open sets are open. But by de Morgan's Laws, we have that
  \[ \bigcup_{\alpha\in A}(E\backslash C_\alpha)=E\,\backslash\qty(\bigcap_{\alpha\in A}C_\alpha)\fstop \]
  
  We thus see that \( C=\bigcap_{\alpha\in A}C_\alpha \) is closed.

  \vspace{3mm}

  (b) Let \( C_1,\dots, C_k \) be closed. Then \( E\backslash C_i \) is open for all \( i=1,\dots,k \). Thus, \( (E\backslash C_1)\cap\dots\cap(E\backslash C_k) \) is open since finite intersections of open sets are open. But by de Morgan's Laws, we have that
  \[ (E\backslash C_1)\cap\dots\cap(E\backslash C_k)=E\backslash\qty(C_1\cup\dots\cup C_k)\fstop \]
  
  It immediately follows that \( C_1\cup\dots\cup C_k \) is closed.
\end{proof}

One way to study closed sets, and the degree at which a set fails to be closed is via the notion of \emph{limit points}:

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space and let \( S\subset E \). A \textbf{limit point} of \( S \) is a point \( x\in E \) such that for every \( r>0 \), \( S\cap (B(x,r)\backslash\qty{x})\neq\emptyset \).
\end{definition}

The motivation for the name `limit point' arises from the fact you can approach a limit point of a set and get arbitrarily close to it even whilst remaining completely within the set. That is, it is the limit of a nonconstant sequence consisting entirely of points from the set.

\vspace{3mm}

One important fact about closed sets is that the limit of a sequence consisting of points from the set must stay inside the set. From the above discussion, it is then clear that a closed set must contain all of its limit points. This turns out to be both a necessary and sufficient criteria for a set to be closed.

\begin{proposition}
  Let \( E \) be a Euclidean \( n \)-space. A set is closed in \( E \) if and only if it contains all of its limit points.
\end{proposition}
\begin{proof}
  \( (\implies) \) Suppose \( S\subset E \) is closed, but assume for a contradiction that \( S \) does not contain all of its limit points. Let \( x\in E \) be a limit point of \( S \) but \( x\notin S \). Then \( x\in E\backslash S \). However, we know that \( E\backslash S \) is an open set (since \( S \) is closed), so there exists an \( r>0 \) such that \( B(x,r)\subset E\backslash S \). But since \( x \) is a limit point of \( S \), then there exists a \( y\in  S\cap (B(x,r)\backslash\qty{x}) \), so that \( y\in S\cap E\backslash S \), which is impossible. Thus, \( S \) must contain all of its limit points.

  \vspace{3mm}

  \( (\impliedby) \) Suppose \( S \) contains all of its limit points. Let \( x\in E\backslash S \), so that \( x \) is not a limit point of \( S \). Therefore, there exists an \( r>0 \) such that \( S\cap (B(x,r)\backslash\qty{x})=\emptyset \). Hence, \( B(x,r)\subset E\backslash S \). This proves that \( E\backslash S \) is open, and hence that \( S \) is closed.
\end{proof}

Given a set \( S \) that is not necessarily closed, we can `add' points to it until we obtain a closed set. The smallest set that results after we add the minimum amount of points required to make the set closed is called the closure of the set:

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space and let \( S\subset E \). The \textbf{closure} of \( S \), denoted \( \overline{S} \), is the intersection of all closed sets containing \( S \).
\end{definition}

It is clear from the above definition that the closure of \( S\subset E \) is the smallest closed set containing \( S \), consistent with our above discussion. In fact, the additional points that we need to add to \( S \) to make it closed are simply the limit points of \( S \) which it does not already contain. Hence, the closure of \( S \) is simply the union of \( S \) with its set of limit points.

\section{Completeness}
In calculus, we will be working with limits \emph{quite a bit}. In particular, we want to be taking limits of functions. First, as is typical, we will define the limit of a sequence, since limits of functions can be studied by considering limits of sequences:
\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. A sequence \( (x_k)_{k=1}^\infty \) in \( E \) is said to \textbf{converge} to \( L\in E \) if for all \( \epsilon>0 \), there exists an \( N\in\N \) such that for all \( k\geq N \), \( \norm{x_k-L}<\epsilon \).
\end{definition}

The usual notation to denote the limit of a convergent sequence is \( x_k\to L \) as \( k\to\infty \), or \( \lim_{k\to\infty}x_k=L \). We will use both interchangably.

\vspace{3mm}

The metric structure on Euclidean space gives us all of the expected desirable properties about limits and convergence that we know and love from \( \R \). If a sequence is convergent, the limit is unique. Closed sets in Euclidean space are exactly the \emph{sequentially closed sets} (i.e.\ every convergent sequence whose points are all contained within a sequentially closed set have a limit inside the set). Since we have the algebraic structure of a vector space as well, we have some algebraic limit laws: the limit of a sum of sequences is equal to the sum of the limits, and we can pull scalars through limits.

\vspace{3mm}

One might notice from the definition of convergence of a sequence that in order to prove directly that a sequence is convergent one must already secretly know what the limit ought to be, so that one can show that the distance between terms in the sequence and the candidate for the limit become arbitrarily small. Guessing what the limit ought to be isn't always a straightforward task, and so fortunately there is another way to show the convergence of a sequence, which relies upon the notion of \emph{completeness}. In a complete metric space, a necessary and sufficient condition for a sequence to be convergent is for the sequence to be \emph{Cauchy}; that is, the terms the sequence progressively become arbitrarily close to \emph{each other}.

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. A sequence \( (x_k)_{k=1}^\infty \) in \( E \) is said to be \textbf{Cauchy} if for all \( \epsilon>0 \), there exists an \( N\in\N \) such that for all \( j,k\geq N \), \( \norm{x_j-x_k}<\epsilon \).
\end{definition}

Intuitively, if the terms in the sequence are gradually varying in distance to a lesser and lesser extent as time goes on, then they must eventually be getting closer to some fixed point, provided they are not clumping around a `gap'. A complete metric space is one in which there are `no gaps', much like you would have heard that \( \R \) has no gaps.

\begin{definition}
  Let \( (X,d) \) be a metric space. We say that \( X \) is \textbf{complete} with respect to the metric \( d \) if every Cauchy sequence in \( X \) converges.
\end{definition}

So the big advantage of completeness is that instead of having to find a candidate limit in order to prove a sequence that you suspect converges actually does so, you can just show that the sequence is Cauchy, i.e.\ the terms just eventually clump up together. Completeness guarantees that the sequence then has a limit. We will now demonstrate the completeness of Euclidean space with respect to the Euclidean metric. The proof is primarily based on the fact that \( \R \) is complete, and that Euclidean space is finite-dimensional.

\begin{theorem}
  Let \( E \) be a Euclidean \( n \)-space. Then \( E \) is complete with respect to the Euclidean metric \( d_2 \).
\end{theorem}
\begin{proof}
  Let \( (x_k)_{k=1}^\infty \) be a Cauchy sequence in \( E \). Let \( \qty{e_1,\dots,e_n} \) be an orthonormal basis for \( E \). We can hence express each term of the sequence in terms of the orthonormal basis:
  \[ x_k=a_{1k}e_1+\dots+a_{nk}e_n\fstop \]
  
  We claim that, for each \( i=1,\dots,n \), the coefficients sequence \( (a_{ik})_{k=1}^\infty \) is Cauchy in \( \R \). To see this, note that
  \begin{align*}
    \abs{a_{ij}-a_{ik}}&=\sqrt{\qty(a_{ij}-a_{ik})^2}\\
    &\leq \sqrt{\sum_{l=1}^n (a_{lj}-a_{lk})^2} &(\text{since the square root function is a real increasing function})\\
    &= \norm{x_j-x_k}\fstop
  \end{align*}

  Let \( \epsilon>0 \). Choose \( N\in\N \) such that for all \( j,k\geq N \), \( \norm{x_j-x_k}<\epsilon \). Such a choice of \( N \) is possible since \( (x_k)_{k=1}^\infty \) is a Cauchy sequence. Then for all \( j,k\geq N \), \( \abs{a_{ij}-a_{ik}}<\epsilon \). This proves our claim.

  \vspace{3mm}

  Now, by completeness of \( \R \), the sequence \( (a_{ik})_{k=1}^\infty \) converges to, say, \( a_i\in\R \). Define \( x=a_1e_1+\dots+a_ne_n \). We will show that \( x_k\to x \) as \(k\to\infty  \). First, observe that
  \begin{align*}
    \norm{x_k-x}&= \sqrt{\sum_{i=1}^n(a_{ik}-a_i)^2}\\
    &\leq \sqrt{n\max_{i=1,\dots,n}(a_{ik}-a_i)^2}\\
    &= \sqrt{n}\max_{i=1,\dots,n}\abs{a_{ik}-a_i}\\
    &<\sqrt{n}\qty(\sum_{i=1}^n\abs{a_{ik}-a_i})\fstop
  \end{align*}

  Let \( \epsilon>0 \). For each \( i=1,\dots,n \), choose \( N_i\in\N \) such that for all \( k\geq N_i \), \( \abs{a_{ik}-a_i}<\frac{\epsilon}{n\sqrt{n}} \). Such a choice of \( N_i \) is possible since \( a_{ik}\to a_i \) as \( k\to\infty \). Choose \( N=\max\qty{N_1,\dots,N_n} \). Then for all \( k\geq N \), we have
  \[ \norm{x_k-x}<\sqrt{n}\sum_{i=1}^n\frac{\epsilon}{n\sqrt{n}}=\epsilon\fstop \]
  
  Hence, \( x_k\to x \) as \( k\to\infty \). This shows that \( E \) is complete with respect to the Euclidean metric.
\end{proof}

From the above proof, we can take away a few useful facts. Namely, a sequence in Euclidean space is Cauchy if and only if all of its (real) component sequences are Cauchy, and a sequence in Euclidean space is convergent if and only if all of its (real) component sequences are convergent. Moreover, the components of the limit of the sequence in Euclidean space will simply be the limits of the individual component sequences.

\vspace{3mm}

Now we'll consider a few consequences of the completeness of Euclidean space. In fact, most of these properties that we'll look at have corresponding analogues in \( \R \). Firstly, we have the Nested Interval Property for Euclidean space, which talks about a nested sequence of closed intervals. By a \emph{closed interval} in Euclidean space, we mean sets of the form \( \qty{c_1e_1+\dots+c_ne_n\mid c_i\in\qty[a_i,b_i] \;\forall\,i=1,\dots,n}  \), where \( \qty{e_1,\dots,e_n} \) is an orthonormal basis.

\begin{theorem}
  \label{thm:NIP}
  (Nested Interval Property) Let \( E \) be a Euclidean \( n \)-space. Let \( I_1\supset I_2\supset\dots\supset I_k\supset\dots \) be a decreasing sequence of nonempty closed intervals in \( E \). Then
  \[ \bigcap_{k=1}^\infty I_k\neq\emptyset\fstop \]
\end{theorem}
\begin{proof}
  Let \( \qty{e_1,\dots,e_n} \) be an orthonormal basis for \( E \), and thus denote the interval \( I_k \) by
  \[ I_k=\qty{c_1e_1+\dots +c_ne_n\in E\mid c_i\in \qty[a_{ik},b_{ik}]\;\forall\,i=1,\dots n}\fstop \]
  
  Since \( I_1\supset I_2\supset\dots I_k\supset\dots \) is a decreasing sequence of closed intervals, we have that for all \( i=1,\dots,n \), \( (a_{ik})_{k=1}^\infty \) is an increasing sequence of real numbers. Furthermore, the sequence is bounded above by \( b_{i1}\in\R \). Hence, we have a bounded, monotonic sequence of real numbers. By the Monotone Convergence Theorem in \( \R \), the sequence converges: so \( a_{ik}\to a_i \) as \( k\to\infty \). We will hence show that \( x=a_1e_1+\dots+a_ne_n\in\bigcap_{k=1}^\infty I_k \).

  \vspace{3mm}

  Let \( m\in\N \). Fix \( i\in\qty{1,\dots,n} \). Since \( (a_{ik})_{k=1}^\infty \) is an increasing sequence, then \( a_{im}\leq a_i \). Additionally, since \( a_{ik}\leq b_{im} \) for all \( k\in\N \), then \( a_i\leq b_{im} \), since limits preserve inequalities. Hence, we have that \( a_i\in\qty[a_{im},b_{im}] \). Since this holds for each \( i \), this shows that \( x\in I_m \). Since the choice of \( m\in\N \) is arbitrary, then \( x \) is in every closed interval \( I_m \), and thus in the intersection of all of them.
\end{proof}

The Nested Interval Property is an immediate consequence of the completeness of Euclidean space. However, one may glance at our above proof and wonder where we used the completeness of Euclidean space in the argument? Secretly, we kind of cheated: rather than appealing to completeness of Euclidean space directly in the proof, we made use of the completeness of \( \R \) (in the form of the Monotone Convergence Theorem) and the finite-dimensionality of Euclidean space, which were the two key facts exploited in the completeness proof. One could certainly prove the Nested Interval Property directly from the completeness of Euclidean space, but we thought the above proof was quite simple and a further demonstration of how easy things are in a finite dimensional setting.

\vspace{3mm}

Next up, we'll look at another key consequence of completeness: the Bolzano-Weierstrass Theorem. This theorem states that every bounded sequence has a convergent subsequence. First, let's define what it means for a set to be bounded in Euclidean space:

\begin{definition}
  Let \( E \) be a Euclidean \( n \)-space. A subset \( A\subset E \) is \textbf{bounded} if there exists an \( M>0 \) such that for all \( x\in A \), \( \norm{x}\leq M \).
\end{definition}

A bounded sequence in \( E \) would thus be a sequence \((x_k)_{k=1}^\infty  \) in which the set of its terms \( \qty{x_k\in E\mid k\in\N} \) is bounded. First, let's prove a slightly rephrased version of the Bolzano-Weierstrass Theorem, which makes use of the Nested Interval Property.

\begin{proposition}
  \label{thm:boundedlim}
  Every bounded infinite set in Euclidean \( n \)-space has at least one limit point.
\end{proposition}
\begin{proof}
  Let \( E \) be a Euclidean \( n \)-space and let \( A\subset E \) be a bounded infinite set. Then there exists an \( M>0 \) such that for all \( x\in A \), \( \norm{x}\leq M \). Let \( \qty{e_1,\dots,e_n} \) be an orthonormal basis in \( E \). Define the closed interval \( I_1=\qty{a_1e_1+\dots+a_ne_n\in E\mid a_i\in[-M,M]\; \forall\, i=1,\dots n} \). Clearly, \( A\subset I_1 \). Then partition \( I_1 \) into \( 2^n \) closed intervals of side length \( \frac{M}{2} \). Choose \( I_2 \) to be one of the subintervals that contains infinitely many points of \( A \).

  \vspace{3mm}

  Continue in this fashion: given the closed interval \( I_k \), partition it into \( 2^n \) subintervals of side length \( \frac{M}{2^{k}} \) and choose \( I_{k+1} \) to be a subinterval which contains infinitely many points of \( A \). This process constructs a decreasing sequence \( I_1\supset I_2\supset\dots\supset I_k\supset\dots \) of nonempty closed intervals. Hence, by the Nested Interval Property, \( \bigcap_{k=1}^\infty I_k\neq\emptyset \). Let \( x\in\bigcap_{k=1}^\infty I_k \). We will show that \( x \) is a limit point of \( A \).

  \vspace{3mm}

  Let \( \epsilon>0 \). Choose \( k\in\N \) such that \( \frac{1}{k}<\frac{\epsilon}{M\sqrt{n}} \). Such a choice of \( k \) is possible since \( \R \) satisfies the Archimedean property. Choose \( a\in A \) such that \( a\in I_k \), which is possible since \( I_k \) contains infinitely many points of \( A \). Since we also have that \( x\in I_k \), then the (Euclidean) distance between \( x \) and \( a \) must be less than or equal to the largest possible distance between two points in the closed interval. For a closed cubical interval of equal side lengths \( r>0 \), the greatest distance is \( \sqrt{nr^2}=r\sqrt{n} \). Hence, we have that
  \[ \norm{x-a}\leq\frac{M}{2^{k-1}}\sqrt{n}\leq\frac{M}{k}\sqrt{n}<\epsilon\fstop \]
  
  Hence, from the above expression we have that \( a\in A\cap\qty(B(x,\epsilon)\backslash\qty{x}) \). Since this is true for all \( \epsilon>0 \), it follows that \( x \) is a limit point of \( A \).
\end{proof}

Now the \emph{actual} Bolzano-Weierstrass Theorem follows as an easy consequence of the above result:

\begin{theorem}
  (Bolzano-Weierstrass Theorem) Every bounded sequence in Euclidean \( n \)-space has a convergent subsequence.
\end{theorem}
\begin{proof}
  Let \( E \) be a Euclidean \( n \)-space and let \( (x_k)_{k=1}^\infty \) be a bounded sequence in \( E \). Then there exists an \( M>0 \) such that \( \norm{x_k}\leq M \) for all \( k\in\N \). Thus, the set \( \qty{x_k\in E\mid k\in\N} \) is a bounded subset of \( E \). Thus, by Proposition \ref{thm:boundedlim}, it has a limit point, say \( x\in E \).

  \vspace{3mm}

  Since \( x \) is a limit point of the set \( \qty{x_k\in E\mid k\in\N} \), then for every \( m\in\N \), pick an \( x_{k_m}\in \qty{x_k\in E\mid k\in\N}\cap\qty(B\qty(x,\frac{1}{m})\backslash\qty{x}) \) such that \( k_m>k_{m'} \) for all \( m'<m \). This constructs the subsequence \( (x_{k_m})_{m=1}^\infty \), which converges to \( x \) as we will now show.

  \vspace{3mm}

  Let \( \epsilon>0 \). Choose \( N\in\N \) such that \( \frac{1}{N}<\epsilon \). Such a choice of \( N \) is possible since \( \R \) satisfies the Archimedean property. Then for all \( m\geq N \), we have
  \[ \norm{x_{k_m}-x}<\frac{1}{m}<\frac{1}{N}<\epsilon\fstop \]
  
  Thus, \( x_{k_m}\to x \) as \( m\to\infty \).
\end{proof}

\section{Continuity}
We will now finally define the notion of limits of functions between Euclidean spaces, which will also give us the necessary framework to talk about continuous functions.
\begin{definition}
  Let \( E, F \) be Euclidean spaces, \( U\subset E \) a subset of \( E \), \( x_0\in E \) a limit point of \( U \) and \( f:U\to F \) a function. The limit of \( f \) as \( x \) approaches \( x_0 \) is the point \( y_0\in F \) such that for all \( \epsilon>0 \), there exists a \( \delta>0 \) such that for all \( x\in U\backslash\qty{x_o} \) with \( \norm{x-x_0}<\delta \) we have \( \norm{f(x)-y_0}<\epsilon \), if such a point \( y_0 \) exists. Else, we say that the limit does not exist.
\end{definition}

The notations \( f(x)\to y_0 \) as \( x\to x_0 \) and \( \lim_{x\to x_0}f(x)=y_0 \) are common - we will employ both throughout this text.

\vspace{3mm}

The astute reader might notice that we are being somewhat sloppy, as we are using \( \norm{-} \) to denote both the Euclidean norms in \( E \) and \( F \). However, it should be fairly clear from the context which norm is being used.

\vspace{3mm}

A very useful fact is that limits of functions can be characterised in terms of limits of sequences, the latter of which are usually much simpler to deal with (we can use all of our facts about limits of sequences and apply them to limits of functions).

\begin{proposition}
  \label{thm:limseq}
   Let \( E, F \) be Euclidean spaces, \( U\subset E \) a subset of \( E \), \( x_0\in E \) a limit point of \( U \) and \( f:U\to F \) a function. Then \( \lim_{x\to x_0}f(x)=y_0 \) if and only if for every sequence \( (x_k)_{k=1}^\infty \) in \( U\backslash\qty{x_0} \) with \( x_k\to x_0 \) as \( k\to\infty \), we have that \( f(x_k)\to y_0 \) as \( k\to\infty \).
 \end{proposition}
 \begin{proof}
   \( (\implies) \) Suppose \( \lim_{x\to x_0}f(x)=y_0 \). Let \( (x_k)_{k=1}^\infty \) be a sequence in \( U\backslash\qty{x_0} \) with \( x_k\to x_0 \) as \( k\to\infty \). Consider the sequence \( (f(x_k))_{k=1}^\infty \) in \( F \). We will show that \( f(x_k)\to y_0 \) as \( k\to\infty \).

   \vspace{3mm}

   Let \( \epsilon>0 \). Choose \( \delta>0 \) such that for all \( x\in U\backslash\qty{x_0} \) with \( \norm{x-x_0}<\delta \) we have \( \norm{f(x)-y_0}<\epsilon \). Such a choice of \( \delta \) is possible since \( \lim_{x\to x_0}f(x)=y_0 \).  Choose \( N\in\N \) such that for all \( k\geq N \), \( \norm{x_k-x_0}<\delta \). Such a choice of \( N \) is possible since \( (x_k)_{k=1}^\infty \) converges to \( x_0 \). Then for all \( k\geq N \), we have that \( \norm{f(x_k)-y_0}<\epsilon \). This proves that \( f(x_k)\to y_0 \) as \( k\to\infty \).

   \vspace{3mm}

   \( (\impliedby) \) Suppose that for every sequence \( (x_k)_{k=1}^\infty \) in \( U\backslash\qty{x_0} \) with \( x_k\to x_0 \) as \( k\to\infty \), we have that \( f(x_k)\to y_0 \) as \( k\to\infty \). Assume for a contradiction that \( \lim_{x\to x_0}f(x)\neq y_0 \). Then there exists an \( \epsilon>0 \) such that for all \( k\in\N \), there exists an \( x_k\in U\backslash\qty{x_0} \) with \( \norm{x_k-x_0}<\frac{1}{k} \) but \( \norm{f(x_k)-y_0}\geq \epsilon \). This constructs a sequence \( (x_k)_{k=1}^\infty \) in \( U\backslash\qty{x_0} \).

   \vspace{3mm}

   Clearly, \( x_k\to x_0 \) as \( k\to\infty \). Hence, \( f(x_k)\to y_0 \) as \( k\to\infty \). This means that there exists an \( N\in\N \) such that for all \( k\geq N \), we have that \( \epsilon\leq \norm{f(x_k)-y_0}<\epsilon \). This is impossible, so we are forced to conclude that \( \lim_{x\to x_0}f(x)= y_0 \).
 \end{proof}

 We can thus easily derive the algebraic limit laws of functions from the above result and the corresponding facts for sequences: the limit of a sum of functions is the sum of the limits, and you can pull scalars through limits. Moreover, limits behave `nicely' with composition of functions: given \( f:E\to F \) and \( g:F\to G \) and \( a\in E \), if \( f(x)\to L\in F \) as \( x\to a \) and \( g(y)\to M \) as \( y\to L \), then \( g\circ f(x)\to M \) as \( x\to a \).

 \vspace{3mm}

 We will now introduce the notion of a continuous function between Euclidean spaces. Intuitively, the idea is that sufficiently small variations in the domain result in arbitrarily small variations in the image of the function.

 \begin{definition}
   Let \( E, F \) be Euclidean spaces, \( U\subset E \) a subset of \( E \) and \( f:U\to F \) a function. We say that \( f \) is \textbf{continuous} at \( x_0\in E \) if for all \( \epsilon>0 \), there exists a \( \delta>0 \) such that for all \( x\in U \) with \( \norm{x-x_0}<\delta \), we have \( \norm{f(x)-f(x_0)}<\epsilon \).

   \vspace{3mm}

   If \( f \) is continuous at all \( x\in U \), then we say that \( f \) is continuous on \( U \).
 \end{definition}

 One should notice that the above definition closely resembles that of the limit of a function, instead that the limit of the function as you approach a point is equal to the function evaluated at that particular point, if it is continuous there. This is why people often like to say that a function \( f:U\to F \) is continuous at \( x_0\in U \) if \( \lim_{x\to x_0}f(x)=f(x_0) \). As continuity can be formulated in terms of a limit, we can obtain an analogous result to Proposition \ref{thm:limseq}, where we can formulate continuity via limits of sequences. The proof follows the same argument as Proposition \ref{thm:limseq}.

 \begin{proposition}
   \label{thm:seqcont}
   Let \( E, F \) be Euclidean spaces, \( U\subset E \) a subset of \( E \) and \( f:U\to F \) a function. Then \( f \) is continuous at \( x_0\in U \) if and only if for all sequences \( (x_k)_{k=1}^\infty \) in \( U \) with \( x_k\to x_0 \) as \( k\to\infty \), we have that \( f(x_k)\to f(x_0) \) as \( k\to\infty \).
 \end{proposition}

 Thus, by the algebraic limit laws, the sum of continuous functions is continuous, and scalar multiples of continuous functions are continuous. Additionally, the composition of continuous functions is continuous.

 \vspace{3mm}

 Let \( E \) be a Euclidean \( n \)-space and \( F \) a Euclidean \( m \)-space. Let \( \overline{e}_1,\dots,\overline{e}_m \) be a basis for \( F \). A multivariable function \( f:E\to F \) can be studied in terms of its real-valued component functions with respect to the basis in \( F \): \( f(x)=f_1(x)\overline{e}_1+\dots+f_m(x)\overline{e}_m \). One nice fact is that \( f \) is continuous if and only if each of the component functions \( f_i:E\to\R \), \( i=1,\dots, m \), are continuous, so that we can simply deal with real-valued functions.
 \begin{proposition}
   A function \( f:E\to F \) is continuous at \( x_0\in E \) if and only if all of its component functions \( f_i:E\to\R \) are continuous at \( x_0 \).
 \end{proposition}
 \begin{proof}
   (\( \implies \)) Suppose \( f \) is continuous at \( x_0 \). Let \( i\in\qty{1,\dots,m} \) and consider the component map \( f_i:E\to\R \). Then for all \( x\in E \), we have
   \[ \abs{f_i(x)-f_i(x_0)}\leq\qty(\sum_{k=1}^m(f_k(x)-f_k(x_0))^2)^{\frac{1}{2}}=\norm{f(x)-f(x_0)}\fstop \]

   Taking the limit as \( x\to x_0 \) yields
   \[ 0\leq\lim_{x\to x_0}\abs{f_i(x)-f_i(x_0)}\leq 0 \]
   since \( \lim_{x\to x_0}\norm{f(x)-f(x_0)} \) by continuity of \( f \) at \( x_0 \). It follows that \( f_i \) is continuous at \( x_0 \).

   \vspace{3mm}

   (\( \impliedby \)) Suppose \( f_i:E\to\R \) is continuous for each \( i\in\qty{1,\dots\,m} \). Then for all \( x\in E \), we have
   \begin{align*}
     \norm{f(x)-f(x_0)}^2&=\sum_{k=1}^m(f_k(x)-f_k(x_0))^2\\
     &\leq m\times\max_{i\in\qty{1,\dots,m}}(f_i(x)-f_i(x_0))^2
   \end{align*}

   This, in turn, gives
   \[ \norm{f(x)-f(x_0)}\leq \sqrt{m}\times\max_{i\in\qty{1,\dots,m}}\abs{f_i(x)-f_i(x_0)}\leq\sqrt{m}\sum_{k=1}^m\abs{f_k(x)-f_k(x_0)}\fstop \]

   Applying the limit as \( x\to x_0 \) yields
   \[ 0\leq\lim_{x\to x_0}\norm{f(x)-f(x_0)}\leq\sqrt{m}\sum_{k=1}^m\lim_{x\to x_0}\abs{f_k(x)-f_k(x_0)}=0 \]
   which follows from the continuity of each of the \( f_k \) at \( x_0 \). Hence, we have that \( \lim_{x\to x_0}\norm{f(x)-f(x_0)}=0 \), which implies that \( f \) is continuous at \( x_0 \).
 \end{proof}

 We conclude this section with a fun fact: linear maps between Euclidean spaces are automatically continuous. So linear maps are really, really nice to work with! This is another consequence of the finite dimensionality of the spaces we are working in.

 \begin{theorem}
   \label{thm:lin-cont}
   Let \( f:E\to F \) be a linear mapping between Euclidean spaces. Then \( f \) is continuous.
 \end{theorem}
 \begin{proof}
   Let \( \qty{e_1,\dots, e_n} \) be an orthonormal basis for \( E \). Let \( x_0\in E \), and let \( (x_k)_{k=1}^\infty \) be a sequence in \( E \) such that \( x_k\to x_0 \) as \( k\to\infty \). We can write each term of the sequence as a linear combination of the basis vectors
   \[ x_k=a_{1k}e_1+\dots+a_{nk}e_n\fstop \]

   Let's also expand \( x_0 \) in terms of the basis vectors
   \[ x_0=a_1e_1+\dots+a_ne_n\fstop \]

   Now, we know that \( x_k\to x_0 \) as \( k\to\infty \) if and only if all of the real component sequences \( (a_{ik})_{k=1}^\infty \) converge to the corresponding component \( a_i \) of \( x_0 \), i.e.\ we have that \( a_{ik}\to a_i \) as \( k\to\infty \) for each \( i=1,\dots,n \).

   \vspace{3mm}

   Finally, by the linearity of \( f \), we have that
   \begin{align*}
     \lim_{k\to\infty}f(x_k)&= \lim_{k\to\infty}\sum_{i=1}^n a_{ik}f(e_i)\\
     &= \sum_{i=1}^n f(e_i)\lim_{k\to\infty} a_{ik} &(\text{by linearity of limits})\\
     &= \sum_{i=1}^n a_i f(e_i)\\
     &= f\qty(\sum_{i=1}^n a_ie_i) &(\text{by linearity of }f)\\
     &= f(x_0)\fstop
   \end{align*}

   By Proposition \ref{thm:seqcont}, this shows that \( f \) is continuous at \( x_0 \). Since the choice of \( x_0\in E \) was arbitrary, it follows that \( f \) is continuous on all of \( E \).
 \end{proof}

 \section{Compactness}
 Here we introduce a property of subsets of Euclidean space called \emph{compactness}. In some sense, it is an extension of the notion of finiteness of sets. While a compact set does not necessarily have finitely many elements, in a way it can always be described with a `finite amount of information'. The most general setting in which one can define compactness is in a topological space, in which the notion of a set being compact is basically that the set is `topologically finite':
 \begin{definition}
   Let \( X \) be a topological space. A subset \( A\subset X \) is \textbf{compact} if for every collection of open sets \( \mathcal{U}=\qty{U_\alpha\mid \alpha\in I} \) satisfying \( A\subset\bigcup_{\alpha\in I}U_\alpha \), there exists a finite subset \( \qty{U_1,\dots,U_r}\subset\mathcal{U} \) such that \( A\subset U_1\cup\dots\cup U_r \).
 \end{definition}

 A collection of open sets \( \mathcal{U} \) in the definition above is called an \textbf{open cover} of the set \( A \). Thus, we can more concisely recite the above definition by stating that a set is compact if every open cover has a finite subcover. So if we try to write down a compact set as a union of infinitely many open sets, we will always have `redundant information', and so we can afford to throw away most of the sets from the union until finitely many remain.

 \vspace{3mm}

 Any finite subset of a topological space is compact. This is in alignment with our introductory spiel that compactness generalises the notion of finiteness for sets.

 \vspace{3mm}

 Since Euclidean space is also a metric space, there exists a different notion of compactness that is phrased in terms of convergence of sequences, called \emph{sequential compactness}.

 \begin{definition}
   Let \( X \) be a metric space. A subset \( A\subset X \) is \textbf{sequentially compact} if every sequence in \( A \) has a convergent subsequence whose limit is contained in \( A \).
 \end{definition}

 We also have that every finite subset of a metric space is sequentially compact. Any sequence whose terms are taken from a finite set must take on at least one of the values from the set infinitely many times, and so we can construct from it a constant subsequence, which is obviously convergent within the set. So sequential compactness is another possibility for extending the notion of finiteness in a metric space setting.

 \vspace{3mm}

 Since every metric space can also be considered as a topological space using the metric topology, one can define both the notions of compactness and sequential compactness on a metric space. The remarkable fact that we will proceed to show is that these notions actually coincide.

 \vspace{3mm}

 Before we tackle the proof, we will require a useful auxillary result that we will use in order to construct a finite subcover of a given open cover of a sequentially compact set.

 \begin{lemma}
   (Lebesgue's number lemma) Let \( A\subset X \) be a sequentially compact subset of a metric space \( X \) and let \( \mathcal{U} \) be an open cover of \( A \). Then there exists a \( \delta>0 \) such that for all \( x\in A \), there exists a \( U\in\mathcal{U} \) such that \( B(x,\delta)\subset U \).
 \end{lemma}
 \begin{proof}
   Assume for a contradiction that for all \( \delta>0 \), there exists an \( x\in A \) such that \( B(x,\delta)\cap (X\backslash U)\neq\emptyset \) for all \( U\in\mathcal{U} \). Then for each \( n\in\N \), pick an \( x_n\in A \) such that \( B\qty(x_n,\frac{1}{n})\cap (X\backslash U)\neq\emptyset \) for all \( U\in\mathcal{U} \).

   \vspace{3mm}

   This defines a sequence \( (x_n)_{n=1}^\infty \) in \( A \), which has a convergent subsequence \( (x_{n_k})_{k=1}^\infty \) since \( A \) is sequentially compact. Suppose that \( x_{n_k}\to a \) as \( k\to\infty \). Since \( a\in A \), then \( a\in V \) for some open set \( V\in\mathcal{U} \), since \( \mathcal{U} \) is an open cover of \( A \). This means that there exists an \( \epsilon>0 \) such that \( B(a,\epsilon)\subset V \). But by convergence of the subsequence \( (x_{n_k})_{k=1}^\infty \), there exists an \( N_1\in\N \) such that for all \( k\geq N_1 \) \( d(x_{n_k},a)<\frac{\epsilon}{2} \). Additionally, by the Archimedean property, there exists an \( N_2\in\N \) such that for all \( k\geq N_2 \), \( \frac{1}{k}<\frac{\epsilon}{2} \). Choose \( N=\max\qty{N_1,N_2} \). Then for all \( k\geq N \), we have that for any \( y\in B\qty(x_{n_k},\frac{1}{k}) \):
   \[ d(y,a)\leq d(y,x_{n_k})+d(x_{n_k},a)<\frac{1}{k}+\frac{\epsilon}{2}<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon\fstop \]

   So \( y\in B(a,\epsilon) \), and hence \( B\qty(x_{n_k},\frac{1}{k})\subset B(a,\epsilon)\subset V \), which contradicts the choice of \( x_{n_k} \).
 \end{proof}
 The positive constant \( \delta>0 \) in the lemma above is called a \emph{Lebesgue number} for the open cover \( \mathcal{U} \).

 \begin{theorem}
   Let \( X \) be a metric space and let \( A\subset X \). Then \( A \) is compact if and only if it is sequentially compact.
 \end{theorem}
 \begin{proof}
   (\( \implies \)) We prove the contrapositive. Let \((x_n)_{n=1}^\infty  \) be a sequence in \( A \) with no convergent subsequence, and denote by \( S=\qty{x_n\in A\mid n\in\N} \) to be the set of terms in the sequence. Then any \( a\in A \) cannot be a limit point of \( S \), else one could construct a subsequence of \( (x_n)_{n=1}^\infty \) that converges to \( a \). Thus, for all \( a\in A \), there exists an \( r_a>0 \) such that \( \qty(B(a,r_a)\backslash\qty{a})\cap S=\emptyset \), that is, the ball \( B(a,r_a) \) contains no points in the sequence \( (x_n)_{n=1}^\infty \), except possibly \( a \) itself. Clearly, we have that \( \mathcal{U}=\bigcup_{a\in A}B(a,r_a) \) is an open cover of \( A \). The open cover \( \mathcal{U} \) cannot have a finite subcover, since any finite subset of \( \mathcal{U} \) can only contain finitely many terms of the sequence \( (x_n)_{n=1}^\infty \), all of which are points belonging to \( A \). We have thus proven that if \( A \) is \emph{not} sequentially compact, then it is \emph{not} compact.

   \vspace{3mm}

   (\( \impliedby \)) Suppose \( A \) is sequentially compact. Let \( \mathcal{U} \) be an open cover of \( A \). By Lebesgue's number lemma, there exists a Lebesgue number \( \delta>0 \) such that for all \( x\in A \), there exists an open set \( U\in\mathcal{U} \) such that \( B(x,\delta)\subset U \). We will use this property to construct a finite subcover of \( \mathcal{U} \).

   \vspace{3mm}

   Pick any \( U_1\in\mathcal{U} \). If \( A\subset U_1 \), then we are done (\( U_1 \) would be a finite subcover of \( \mathcal{U} \)). Else, pick any \( x_1\in A\backslash U_1 \). By Lebesgue's number lemma, we can pick \( U_2\in\mathcal{U} \) such that \( B(x_1,\delta)\subset U_2 \). If \( A\subset U_1\cup U_2 \), we are done. Else, pick an \( x_2\in A\backslash\qty(U_1\cup U_2) \) and \( U_3\in\mathcal{U} \) satisfying \( B(x_2,\delta)\subset U_3 \). Notice that since \( x_2\notin U_1\cup U_2 \), then \( x_2\notin B(x_1,\delta) \), so that \( d(x_1,x_2)\geq\delta \).

   \vspace{3mm}

   Continue the process in the following fashion. Suppose we had a finite collection of open sets \( U_1,\dots U_n\in\mathcal{U} \) that was \emph{not} a subcover of \( \mathcal{U} \) and a finite subset \( \qty{x_1,\dots,x_{n-1}}\subset A \) such that \( d(x_i,x_j)\geq\delta \) for each \( i,j=1,\dots,n-1 \) with \( i\neq j \). Pick \( x_n\in A\backslash\qty(U_1\cup\dots\cup U_n) \) and \( U_{n+1}\in\mathcal{U} \) satisfying \( B(x_n,\delta)\subset U_{n+1} \). Since \( x_n\notin (U_1\cup\dots\cup U_n) \), then \( x_n\notin U_i \) for each \( i=1,\dots,n-1 \), and hence \( x_n\notin B(x_i,\delta) \) so that \( d(x_n,x_i)\geq\delta \).

   \vspace{3mm}

   Suppose that this process did \emph{not} terminate. That is, there does not exist an \( n\in\N \) such that \( A\subset U_1\cup\dots\cup U_n \). Then by induction on \( \N \), we obtain a sequence \( (x_n)_{n=1}^\infty \) in \( A \) with \( d(x_i,x_j)\geq\delta \) for all \( i,j\in\N \) with \( i\neq j \). By sequential compactness of \( A \), this sequence has a convergent subsequence \( (x_{n_k})_{k=1}^\infty \) with limit \( x\in A \). Hence, there exists an \( N\in\N \) such that for all \( k\geq N \), \( d(x_{n_k},x)<\frac{\delta}{2} \). But by construction, we have that \( x_{n_l}\notin B(x_k,\delta) \) for all \( l\geq k \), so \( d(x_{n_k},x_{n_l})\geq\delta \). Taking \( l\to\infty \), by continuity of the metric we have that \( d(x_{n_k},x)\geq\delta \). This is a contradiction, and so we must have that there exists an \( n\in\N \) such that \( A\subset U_1\cup\dots\cup U_n \), so that \( \mathcal{U} \) has a finite subcover.
 \end{proof}

 In light of the above result, we will simply use the term `compactness' to refer to both topological compactness and sequential compactness in the context of subsets of Euclidean space. In practice, most of the time when we invoke the property of compactness for a subset of Euclidean space, we will use sequential compactness because sequences are easier to work with, owing to the rich theory of sequences and convergence in a metric space.

 \vspace{3mm}

 Even better, the conditions for compactness become significantly simpler in a finite dimensional setting. This is the celebrated \emph{Heine-Borel Theorem}, which states that the compact subsets of Euclidean space are exactly the closed and bounded subsets. In general, for a metric space, every compact set is closed and bounded, but in Euclidean space, this becomes a sufficient condition for compactness.

 \begin{theorem}
   (Heine-Borel Theorem) Let \( E \) be a Euclidean space and let \( A\subset E \). Then \( A \) is compact if and only if it is closed and bounded.
 \end{theorem}
 \begin{proof}
   (\( \implies \)) Suppose \( A\subset E \) is compact. To see that \( A \) is closed, let \( (a_n)_{n=1}^\infty \) be a sequence in \( A \) with limit \( a\in E \). Since \( A \) is (sequentially) compact, there exists a convergent subsequence \( (a_{n_k})_{k=1}^\infty \) with limit \( x\in A \). However, since the original sequence \( (a_n)_{n=1}^\infty \) is convergent, then the limits of any of its subsequences must be equal to \( a \). Hence, we see that \( a=x \), so that \( a\in A \). This shows that \( A \) is \emph{sequentially closed}, and hence closed.

   \vspace{3mm}

   Now, suppose for a contradiction that \( A \) was unbounded. Then for each \( n\in\N \), we can choose an \( x_n\in A \) such that \( \norm{x_n}>n \). This constructs a sequence \( (x_n)_{n=1}^\infty \) in \( A \). By (sequential) compactness of \( A \), this sequence has a convergent subsequence \( (x_{n_k})_{k=1}^\infty \) with limit \( x\in A \). However, taking \( N=\lceil\norm{x}+1\rceil \), we have for all \( k\geq N \) that \( \norm{x_{n_k}-x}\geq\abs{\norm{x_{n_k}}-\norm{x}}\geq 1 \) by the reverse triangle inequality, which contradicts the fact that \( x_{n_k}\to x \) as \( k\to\infty \). Hence, we must have that \( A \) is bounded.

   \vspace{3mm}

   (\( \impliedby \)) Suppose \( A\subset E \) is closed and bounded. Let \( (a_n)_{n=1}^\infty \) be a sequence in \( A \). Thus, \( (a_n)_{n=1}^\infty \) is a bounded sequence. By the Bolzano-Weierstrass Theorem, there exists a convergent subsequence \( (a_{n_k})_{k=1}^\infty \) with limit \( a\in E \). However, since \( A \) is closed, we must have that the limit is contained in \( A \), i.e.\ \( a\in A \). This proves the (sequential) compactness of \( A \).
 \end{proof}

 Now that we have seen the various ways we can characterise a compact set in Euclidean space, we will now spend some time looking at why compact sets are nice to work with. In particular, compact sets interact nicely with continuous functions. The first fact we will look at is that if a continuous real-valued function defined on a compact set is always positive, then it does not get arbitrarily close to 0, that is, the function is bounded below by a positive constant.

 \begin{proposition}
   \label{thm:posfunc}
   Let \( E \) be a Euclidean space, \( A\subset E \) a compact set, and \( f:A\to\R \) a continuous function with \( f(x)>0 \) for all \( x\in A \). Then there exists an \( \alpha>0 \) such that \( f(x)>\alpha \) for all \( x\in A \).
 \end{proposition}
 \begin{proof}
   Suppose for a contradiction that for all \( \epsilon>0 \), there exists an \( x\in A \) such that \( f(x)\leq\epsilon \). Then for each \( n\in\N \), choose an \( x_n\in A \) such that \( f(x_n)\leq \frac{1}{n} \). This defines a sequence \( (x_n)_{n=1}^\infty \) in \( A \). By compactness of \( A \), there exists a subsequence \( (x_{n_k})_{k=1}^\infty \) that converges to \( a\in A \). By continuity of \( f \), we thus have that \( f(x_{n_k})\to f(a) \) as \( k\to\infty \). But
   \begin{align*}
     \lim_{k\to\infty}f(x_{n_k})&\leq \lim_{k\to\infty}\frac{1}{n_k}\\
     \therefore f(a)&\leq 0\fstop
   \end{align*}

   This contradicts the fact that \( f(x)>0 \) for all \( x\in A \).
 \end{proof}

 We will make use of the above result in the following section.

 \vspace{3mm}

 Compact sets are preserved by continuous functions. That is, the image of a compact set under a continuous function is itself a compact set:

 \begin{proposition}
   \label{thm:cptimage}
   Let \( f:E\to F \) be a continuous mapping between Euclidean spaces and let \( A\subset E \) be a compact set. Then \( f(A) \) is a compact subset of \( F \).
 \end{proposition}
 \begin{proof}
   Let \( (y_n)_{n=1}^\infty \) be a sequence in \( f(A) \). Then for each \( n\in\N \), there exists an \( x_n\in A \) such that \( y_n=f(x_n) \). Consider the sequence \( (x_n)_{n=1}^\infty \) in \( A \). By compactness of \( A \), this sequence has a convergent subsequence \( (x_{n_k})_{k=1}^\infty \) with limit \( x\in A \). By continuity of \( f \), we have that \( f(x_{n_k})\to f(x) \) as \( k\to\infty \), i.e.\ \( y_{n_k}\to f(x)\in f(A) \) as \( k\to\infty \). So \( (y_{n_k})_{k=1}^\infty \) is a convergent subsequence of \( (y_n)_{n=1}^\infty \) whose limit is contained in \( f(A) \). Hence, \( f(A) \) is compact.
 \end{proof}

 An immediate application of the above fact is the \emph{Extreme Value Theorem}. This is an existence result that guarantees that a continuous real-valued function defined on a compact set achieves global extrema on its domain. Since one of the main goals of differential calculus is optimisation, it is a good first step to know that global extrema exist somewhere so it makes sense to look for them in the first place!
 \begin{theorem}
   (Extreme Value Theorem) Let \( E \) be a Euclidean space, \( A\subset E \) a compact set and \( f:A\to\R \) a continuous function. Then \( f \) attains a maximum and a minimum on \( A \).
 \end{theorem}
 \begin{proof}
   By Proposition \ref{thm:cptimage}, we have that \( f(A) \) is a compact set. By the Heine-Borel Theorem, this implies that \( f(A) \) is a closed and bounded subset of \( \R \). The boundedness of \( f(A) \) implies that \( \sup f(A)<\infty \) and \( \inf f(A)>-\infty \). For each \( n\in\N \), choose \( x_n,y_n\in A \) such that \( \sup f(A)-\frac{1}{n}<f(x_n)\leq\sup f(A) \) and \( \inf f(A)\leq f(y_n)<\inf f(A)+\frac{1}{n} \). By taking limits as \( n\to\infty \) and applying the Squeeze Theorem, we see that \( f(x_n)\to\sup f(A) \) and \( f(y_n)\to\inf f(A) \) as \( n\to\infty \). By closedness of \( f(A) \), we have that \( \sup f(A)\in f(A) \) and \( \inf f(A)\in f(A) \), so there exists \( x_0,x_1\in A \) satisfying \( f(x_0)=\sup f(A) \) and \( f(x_1)=\inf f(A) \), i.e.\ \( f \) attains a maximum at \( x_0 \) and a minimum at \( x_1 \).
 \end{proof}
 
 \section{Equivalence of Norms}
 In \Cref{sec:EuclidSpace}, we saw that the Euclidean inner product \emph{uniquely} induces the Euclidean norm \( \norm{-}_2 \) on \( \R^n \), which gave us sufficient machinery to define distances, convergence, continuity, etc. If we (temporarily) forget about the inner product structure of Euclidean space, a very natural thing to think about is what would happen if we equipped \( \R^n \) with a \emph{different} norm. Would the resulting topological structure differ from that of Euclidean space? Perhaps surprisingly, the answer is no. While different norms may prescribe different lengths to the same vector, they actually all induce the exact same topology in a finite dimensional setting!

 \vspace{3mm}

 To discuss this idea further on a more solid ground, we will define what it means in general for two norms to be `the same' under a topological lens. We will call topologically compatible pairs of norms to be \emph{strongly equivalent}:
 
 \begin{definition}
   Let \( V \) be a vector space and let \( \norm{-}_1 \) and \( \norm{-}_2 \) be norms on \( V \). We say that the norms \( \norm{-}_1 \) and \( \norm{-}_2 \) are \textbf{strongly equivalent} if there exists constants \( C,D>0 \) such that \( C\norm{x}_1\leq \norm{x}_2\leq D\norm{x}_1 \) for all \( x\in V \).
 \end{definition}

 Strong equivalence of norms on a vector space defines an equivalence relation on all of the possible norms that can be defined on the vector space.

 \vspace{3mm}

 The condition for two norms being strongly equivalent to one another may seem somewhat opaque at a first glance. Admittedly, it is more of an \emph{operational definition} in that the definition itself is not very intuitive or particularly illuminating, but it makes proving the shared properties of strongly equivalent norms a relatively simple task thanks to the nice inequality involving the two different norms that it establishes. We will say more on the implications of having strongly equivalent norms a bit later in this section. First, we will establish the very important fact that any norm that one can define on \( \R^n \) is strongly equivalent to any other.

 \begin{theorem}
   \label{thm:norm-equiv}
   Let \( E \) be a Euclidean \( n \)-space. Then any two norms on \( E \) are strongly equivalent.
 \end{theorem}
 \begin{proof}
   We will show that every norm on \( E \) is strongly equivalent to the Euclidean norm \( \norm{-} \). Let \( \norm{-}_0 \) be any norm on \( E \). Define the function \( f:E\backslash\qty{0}\to\R \) by
   \[ f(x)=\frac{\norm{x}}{\norm{x}_0}\fstop \]

   Note that \( f(x)>0 \) for all \( x\in E\backslash\qty{0} \).

   \vspace{3mm}
   
   Consider the unit sphere in the Euclidean norm, i.e.\ the set \( S=\qty{x\in E\mid\norm{x}=1} \). The set \( S\subset E \) is compact since it is a closed, bounded set (Heine-Borel Theorem). Hence, if we restrict the domain of \( f \) to \( S \), we can apply Proposition \ref{thm:posfunc} and obtain a constant \( C>0 \) such that \( f(x)>C \) for all \( x\in S \). Hence, we have that \( \frac{\norm{x}}{\norm{x}_0}>C \) for all \( x\in E \) with \( \norm{x}=1 \).

   \vspace{3mm}

   Now let \( y\in E \) with \( y\neq 0 \). Define \( \vu{y}=\frac{y}{\norm{y}} \); the vector \( y \) normalised to unit length in the Euclidean norm. Clearly, \( \norm{\vu{y}}=1 \), so that we have
   \[ \frac{\norm{\vu{y}}}{\norm{\vu{y}}_0}>C\fstop \]
   
   But by homogeneity of the norm, we can pull a factor of \( \frac{1}{\norm{y}} \) outside the norms in both the numerator and denominator of the left-hand-side of the above expression, which cancel. We hence arrive at
   \[ \frac{\norm{y}}{\norm{y}}_0>C\fstop \]

   This rearranges to \( C\norm{y}_0<\norm{y} \), which holds for all \( y\neq 0 \). If \( y=0 \), then we have a trivial equality (\( 0=C\norm{y}_0=\norm{y}=0 \)). Hence, we have that \( C\norm{x}_0\leq\norm{x} \) for all \( x\in E \).

   \vspace{3mm}

   Similarly, define the function \( g:E\backslash\qty{0}\to\R \) by
   \[ g(x)=\frac{\norm{x}_0}{\norm{x}}\fstop \]

   We have that \( g(x)>0 \) for all \( x\in E\backslash\qty{0} \). Once again, we restrict the domain of \( g \) to the unit sphere \( S \) and apply Proposition \ref{thm:posfunc} to obtain a constant \( D'>0 \) such that \( g(x)>D' \) for all \( x\in S \). Hence, we have that \( \frac{\norm{x}_0}{\norm{x}}>D' \) for all \( x\in E \) with \( \norm{x}=1 \).

   \vspace{3mm}

   Let \( y\in E \) with \( y\neq 0 \). Then
   \begin{align*}
     \frac{\norm{\vu{y}}_0}{\norm{\vu{y}}}&>D'\\
     \frac{\norm{\frac{y}{\norm{y}}}_0}{\norm{\frac{y}{\norm{y}}}}&>D'\\
     \frac{\norm{y}}{\norm{y}}\,\frac{\norm{y}_0}{\norm{y}}&>D'\\
     \frac{\norm{y}_0}{\norm{y}}&>D'\fstop
   \end{align*}

   Hence, we have that \( \norm{y}<D\norm{y}_0 \) for all \( y\neq 0 \), where we have defined \( D=\frac{1}{D'}>0 \). Once again, if \( y=0 \), then we trivially have that \( \norm{y}=D\norm{y}_0 \). Hence, for any \( x\in E \), we have the inequality \( \norm{x}\leq D\norm{x}_0 \).

   \vspace{3mm}

   Putting this all together, we have that \( C\norm{x}_0\leq \norm{x}\leq D\norm{x}_0 \) for all \( x\in E \). This proves that the norm \( \norm{-}_0 \) is strongly equivalent to the Euclidean norm. Since strong equivalence is an equivalence relation on the norms on \( E \), it follows that every norm on \( E \) is strongly equivalent to one another.
 \end{proof}

 We are now ready to explore the properties shared by strongly equivalent norms. First, we formally prove the statement that strongly equivalent norms induce the same topology by showing that the open sets in the metric topology induced by \( \norm{-}_1 \) exactly coincide with the open sets in the metric topology induced by \( \norm{-}_2 \).

 \begin{proposition}
   Let \( V \) be a vector space and let \( \norm{-}_1 \) and \( \norm{-}_2 \) be strongly equivalent norms on \( V \). Then a subset \( U\subset V \) is open with respect to the norm \( \norm{-}_1 \) if and only if \( U \) is open with respect to the norm \( \norm{-}_2 \).
 \end{proposition}
 \begin{proof}
   (\( \implies \)) Suppose \( U \) is open with respect to \( \norm{-}_1 \), i.e.\ for all \( x\in U \), there exists an \( r_x>0 \) such that \( B_1(x,r_x)=\qty{y\in V\mid \norm{x-y}_1<r_x}\subset U \).

   \vspace{3mm}
   
   Since \( \norm{-}_1 \) and \( \norm{-}_2 \) are strongly equivalent, there exists \( C>0 \) such that \( C\norm{v}_1\leq\norm{v}_2 \) for all \( v\in V \). Let \( x\in U \) and let \( y\in B_2\qty(x,Cr_x)=\qty{z\in V\mid \norm{x-z}_2<Cr_x} \). Then
   \[ C\norm{x-y}_1\leq\norm{x-y}_2<Cr_x\fstop \]

   We hence obtain \( \norm{x-y}_1<r_x \), which demonstrates that \( y\in B_1(x,r_x) \). We thus have that \( B_2(x,Cr_x)\subset B_1(x,r_x)\subset U \), which proves that \( U \) is open with respect to \( \norm{-}_2 \).

   \vspace{3mm}

   (\( \impliedby \)) Suppose \( U \) is open with respect to \( \norm{-}_2 \), i.e.\ for all \( x\in U \), there exists an \( r_x>0 \) such that \( B_2(x,r_x)=\qty{y\in V\mid\norm{x-y}_2<r_x}\subset U \).

   \vspace{3mm}

   Since \( \norm{-}_1 \) and \( \norm{-}_2 \) are strongly equivalent, there exists \( D>0 \) such that \( \norm{v}_2\leq D\norm{v}_1 \) for all \( v\in V \). Let \( x\in U \) and let \( y\in B_1\qty(x,\frac{1}{D}r_x)=\qty{z\in V\mid \norm{x-z}_1<\frac{1}{D}r_x} \). Then
   \[ \frac{1}{D}\norm{x-y}_2\leq\norm{x-y}_1<\frac{1}{D}r_x\fstop \]

   We hence have that \( \norm{x-y}_2<r_x \), so that \( y\in B_2(x,r_x) \). Hence, \( B_1\qty(x,\frac{1}{D}r_x)\subset B_2(x,r_x)\subset U \), which proves that \( U \) is open with respect to \( \norm{-}_1 \).
 \end{proof}

 Naturally, since a normed space enjoys the same set of topological properties regardless of which norm is used (as long as they are all strongly equivalent), one can expect that sequences would behave similarly in any norm. In particular, if a sequence is either Cauchy or convergent in any one norm, then it will be Cauchy or convergent (with the same limit) in any other strongly equivalent norm.
 
 \begin{proposition}
   \label{thm:norm-equiv-conv}
   Let \( V \) be a vector space and let \( \norm{-}_1 \) and \( \norm{-}_2 \) be strongly equivalent norms on \( V \). Let \( (x_n)_{n=1}^\infty \) be a sequence in \( V \). Then:
   \begin{enumerate}[label=(\alph*)]
   \item \( (x_n)_{n=1}^\infty \) is Cauchy with respect to \( \norm{-}_1 \) if and only if \( (x_n)_{n=1}^\infty \) is Cauchy with respect to \( \norm{-}_2 \).
   \item \( \norm{x_n-x}_1\to 0 \) if and only if \( \norm{x_n-x}_2\to 0 \).
   \end{enumerate}
 \end{proposition}
 \begin{proof}
   \begin{enumerate}[label=(\alph*)]
   \item Firstly, since \( \norm{-}_1 \) and \( \norm{-}_2 \) are strongly equivalent norms, there exist \( C,D>0 \) such that the following inequality holds for all \( m,n\in\N \)
     \[ 0\leq C\norm{x_m-x_n}_1\leq\norm{x_m-x_n}_2\leq D\norm{x_m-x_n}_1\fstop \]
     (\( \implies \)) Suppose \( (x_n)_{n=1}^\infty \) is Cauchy with respect to \( \norm{-}_1 \). Let \( \epsilon>0 \). Choose \( N\in\N \) such that for all \( m,n\geq N \), \( \norm{x_m-x_n}_1<\flatfrac{\epsilon}{D} \). Then for all \( m,n\geq N \), we have
     \[ \norm{x_m-x_n}_2\leq D\norm{x_m-x_n}_1<D\times\frac{\epsilon}{D}=\epsilon\fstop \]

     (\( \impliedby \)) Suppose \( (x_n)_{n=1}^\infty \) is Cauchy with respect to \( \norm{-}_2 \). Let \( \epsilon>0 \). Choose \( N\in\N \) such that for all \( m,n\geq N \), \( \norm{x_m-x_n}_2<C\epsilon \). Then for all \( m,n\geq N \), we have
     \[ \norm{x_m-x_n}_1\leq \frac{1}{C}\norm{x_m-x_n}_2<\frac{C\epsilon}{C}=\epsilon\fstop \]
   \item For all \( n\in\N \), we have that
     \[ 0\leq C\norm{x_n-x}_1\leq\norm{x_n-x}_2\leq D\norm{x_n-x}_1 \]
     for some \( C,D>0 \), since \( \norm{-}_1 \) and \( \norm{-}_2 \) are strongly equivalent norms.

     \vspace{3mm}

     Suppose \( \lim_{n\to\infty}\norm{x_n-x}_1=0 \). Then
     \begin{align*}
       0&\leq\lim_{n\to\infty}\norm{x_n-x}_2\leq D\lim_{n\to\infty}\norm{x_n-x}_1\\
       0&\leq\lim_{n\to\infty}\norm{x_n-x}_2\leq 0\fstop
     \end{align*}

     This implies that \( \norm{x_n-x}_2\to 0 \) as \( n\to\infty \) by the Squeeze Theorem. Similarly, if \( \lim_{n\to\infty}\norm{x_n-x}_2=0 \), then
     \begin{align*}
       0&\leq\lim_{n\to\infty}\norm{x_n-x}_1\leq \frac{1}{C}\lim_{n\to\infty}\norm{x_n-x}_2\\
       0&\leq\lim_{n\to\infty}\norm{x_n-x}_1\leq 0
     \end{align*}
     which implies that \( \norm{x_n-x}_1\to 0 \) as \( n\to\infty \).
   \end{enumerate}
 \end{proof}

 This directly gives us the following result:
 \begin{proposition}
   Let \( V \) be a vector space and let \( \norm{-}_1 \) and \( \norm{-}_2 \) be strongly equivalent norms on \( V \). Then \( V \) is complete with respect to \( \norm{-}_1 \) if and only if \( V \) is complete with respect to \( \norm{-}_2 \).
 \end{proposition}
 \begin{proof}
   Suppose \( V \) is complete with respect to \( \norm{-}_1 \). Let \( (x_n)_{n=1}^\infty \) be a Cauchy sequence with respect to \( \norm{-}_2 \). By \Cref{thm:norm-equiv-conv} part (a), \( (x_n)_{n=1}^\infty \) is Cauchy with respect to \( \norm{-}_1 \). By completeness of \( V \) in the 1-norm, we have that \( \norm{x_n-x}_1\to 0 \) for some \( x\in V \). But by \Cref{thm:norm-equiv-conv} part (b), this implies that \( \norm{x_n-x}_2\to 0 \) as \( n\to\infty \). This shows that every Cauchy sequence in \( (V,\norm{-}_2) \) has a limit, thus proving completeness. The same argument works in reverse for proving completeness of \( (V,\norm{-}_1) \) from that of \( (V,\norm{-}_2) \).
 \end{proof}

 Finally, given a function \( f:V \to W \) between finite dimensional normed spaces, the property of \( f \) being continuous is, in effect, independent on the particular norms used for \( V \) and \( W \).
 \begin{proposition}
   Let \( V,W \) be finite dimensional normed vector spaces. Then the continuity of a mapping \( f:V\to W \) does not depend on the choice of norm on \( V \) and \( W \).
 \end{proposition}
 \begin{proof}
   Suppose the map \( f:V\to W \) is continuous with respect to the norm \( \norm{-}_1 \) on \( V \) and \( \norm{-}_2 \) on \( W \). Let \( \norm{-}_{1'} \) be an arbitrary on \( V \) and \( \norm{-}_{2'} \) be an arbitrary norm on \( W \). By \Cref{thm:norm-equiv}, \( \norm{-}_1,\norm{-}_{1'} \) are strongly equivalent norms on \( V \) and \( \norm{-}_2,\norm{-}_{2'} \) are strongly equivalent norms on \( W \). We will show that \( f \) is continuous with respect to the norms \( \norm{-}_{1'} \) and \( \norm{-}_{2'} \) on \( V \) and \( W \), respectively.

   \vspace{3mm}

   Let \( (x_n)_{n=1}^\infty \) be a sequence in \( V \) which converges to \( x\in V \) in the norm \( \norm{-}_{1'} \). By \Cref{thm:norm-equiv-conv}, \( x_n\to x \) as \( n\to\infty \) with respect to \( \norm{-}_1 \). By continuity of \( f \) (with respect to the unprimed norms) we have that \( f(x_n)\to f(x) \) as \( n\to\infty \) with respect to \( \norm{-}_2 \). Another application of \Cref{thm:norm-equiv-conv} shows that \( f(x_n)\to f(x) \) as \( n\to\infty \) with respect to \( \norm{-}_{2'} \). This shows the sequential continuity of \( f \) with respect to the primed norms, and hence demonstrates continuity.
 \end{proof}

 In summary, we have shown that when we are working in Euclidean space \( E \), we can equip \( E \) with any norm we want, and any topological result we deduce (including convergence and continuity) will be universal (i.e.\ norm independent). Thus, we do not necessarily need to work in the Euclidean norm \( \norm{-}_2 \) all of the time, but rather we can choose convenient norms that are better suited for the particular problems we are working on at the time.

 \vspace{3mm}

 In particular, for each \( p\in\N \), we have the following family of norms on \( E \) \( \norm{-}_p:E\to\R \)
 \[ \norm{x}_p=\qty(\sum_{i=1}^n\abs{x_i}^p)^{\frac{1}{p}} \]
 where the coefficients \( x_i \) are the coordinates of the vector \( x\in E \) with respect to some basis of \( E \). The norm \( \norm{-}_p \) is called the \( p \)-norm on \( E \). If \( p=2 \), the resulting \( p \)-norm is exactly the Euclidean norm \( \norm{-}_2 \), which justifies the notation.

 \vspace{3mm}

 Another important norm we can define on \( E \) is the \emph{maximum norm}, denoted \( \norm{-}_\infty \), which is defined by
 \[ \norm{x}_\infty=\max_{i\in\qty{1,\dots, n}}\qty{\abs{x_i}}\fstop \]

 This is a very convenient norm to work with which we will frequently use, especially once we begin to do calculus on Euclidean space.
 
 \section{The Space \( L(E,F) \)}
 \label{sec:linear-maps}
 The importance of linear maps in multivariable calculus cannot be understated (indeed, we will later see that our definition of the derivative of a multivariable function will be an appropriate linear map). We will therefore carry out a study on the collection of all linear maps between two Euclidean spaces, and consider the structures that can be naturally prescribed to this space of maps to see what existing theories can be used to describe these objects and their relationships with one another.

 \begin{definition}
   Let \( E \), \( F \) be vector spaces. Define \( L(E,F) \) as the set of all linear maps \( T:E\to F \).
 \end{definition}

 Here, we will study the space \( L(E,F) \) for the case where \( E \) and \( F \) are both Euclidean spaces (of possibly different dimension).

 \vspace{3mm}

 First, we formally introduce the \emph{matrix representation} of an element of \( L(E,F) \), which is a convenient way to describe and work with linear maps. In this representation, all of the information about a given linear map is captured by a (finite) array of real numbers. The details are as follows:

 \vspace{3mm}

 Let \( E \) be Euclidean \( n \)-space, \( F \) be Euclidean \( m \)-space and let \( A\in L(E,F) \). Given a basis \( \qty{e_1,\dots, e_n} \) for \( E \), and a basis \( \qty{\overline{e}_1,\dots,\overline{e}_m} \) for \( F \), we can represent the linear map \( A \) as an \( m\times n \) \emph{matrix} with respect to these bases, i.e.\ a collection of \( mn \) real numbers arranged in an \( m\times n \) array (\( m \) rows and \( n \) columns). To see this, given any \( x\in E \), we can express \( x \) in terms of the basis vectors
 \[ x=\sum_{j=1}^nx_je_j\fstop \]

 We act on \( x \) with the linear map \( A \), and by linearity we obtain
 \[ Ax=A\qty(\sum_{j=1}^nx_je_j)=\sum_{j=1}^nx_jAe_j\fstop \]

 We can expand each \( Ae_j\in F \) in terms of the basis in \( F \)
 \[ Ae_j=\sum_{i=1}^mA_{ij}\overline{e}_i\fstop \]

 Substituting this into our expression for \( Ax \) yields
 \[ Ax=\sum_{i=1}^m\sum_{j=1}^nA_{ij}x_j\overline{e}_i\fstop\]

 The constants \( A_{ij} \) are referred to as the entries of the matrix representing the linear map \( A \). Notice that the entries depend on the choice of basis on both \( E \) and \( F \).

 \vspace{3mm}

 The composition \( B\circ A \) of two linear maps \( A\in L(E,F) \) and \( B\in L(F,G) \) is again a linear map. What will the corresponding matrix representation of \( B\circ A \) look like in terms of the matrices for \( A \) and \( B \)? We'll use the same bases for \( E \) and \( F \) and consider the basis \( \qty{\hat{e}_1,\dots,\hat{e}_l} \) for \( G \) (\( l \)-dimensional Euclidean space). Then for any \( x\in E \), we can write
 \begin{align*}
   B\circ A x&= B\qty(\sum_{j=1}^m\sum_{k=1}^n A_{jk}x_k\overline{e}_j)\\
   &= \sum_{j=1}^m\sum_{k=1}^n A_{jk}x_kB\overline{e}_j
 \end{align*}
 where the final equality follows from the linearity of \( B \). We expand \( B\overline{e}_j \) in terms of the given basis in \( G \), noting that the corresponding coefficients are simply the matrix elements \( B_{ij} \) of \( B \)
 \[ B\circ Ax=\sum_{i=1}^l\sum_{j=1}^m\sum_{k=1}^nB_{ij}A_{jk}x_k\hat{e}_i=\sum_{i=1}^l\sum_{k=1}^nC_{ik}x_k\hat{e}_i\fstop \]

 Hence, the linear map \( B\circ A:E\to G \) is represented by an \( l\times n \) matrix, and its corresponding matrix elements are \( C_{ik} \), which are given by
 \[ C_{ik}=\sum_{j=1}^mB_{ij}A_{jk}\fstop \]

 One may recognise that the the matrix for the composite map \( B\circ A \) is obtained by performing matrix multiplication with the matrices for \( B \) and \( A \).

 \vspace{3mm}

 We now consider the relevant structures which we can endow upon \( L(E,F) \). Firstly, from the algebraic properties of addition and scalar multiplication of vector-valued functions, we can consider \( L(E,F) \) to be a vector space.
 
 \begin{theorem}
   Let \( E,F \) be vector spaces with \( \dim E=n \) and \( \dim F=m \). Then \( L(E,F) \) is an \( mn \)-dimensional vector space.
 \end{theorem}
 \begin{proof}
   Since sums of linear maps are linear, and scalar multiples of linear maps are linear, it follows that \( L(E,F) \) is a vector space (a subspace of the vector space of all functions \( f:E\to F \)). We compute the dimension of \( L(E,F) \) by explicitly finding a basis. The construction that will be employed below is a generalisation of the construction of the dual basis for \( E^* \) in \Cref{sec:dual}. Fix a basis \( \qty{e_1,\dots,e_n} \) for \( E \) and a basis \( \qty{\overline{e}_1,\dots,\overline{e}_m} \) for \( F \), and define linear maps \( \phi_{ij}:E\to F \) for each \( i=1,\dots,n \) and \( j=1,\dots,m \) by
   \[ \phi_{ij}(e_k)=\delta_{jk}\overline{e}_i\fstop \]

   Note that defining the map \( \phi_{ij} \) on the basis elements of \( E \) completely defines the map on all of \( E \) by linearity. We will now show that:
   \begin{enumerate}[label=(\roman*)]
   \item The set \( \qty{\phi_{ij}:E\to F\mid i\in\qty{1,\dots,m}, j\in\qty{1,\dots,n}} \) is linearly independent.
   \item The span of \( \qty{\phi_{ij}:E\to F\mid i\in\qty{1,\dots,m}, j\in\qty{1,\dots,n}} \) is equal to \( L(E,F) \).
   \end{enumerate}

   This will demonstrate that \( \qty{\phi_{ij}:E\to F\mid i\in\qty{1,\dots,m}, j\in\qty{1,\dots,n}} \) is a basis for \( L(E,F) \).

   \begin{enumerate}[label=(\roman*)]
   \item Consider the following operator equation
     \[ \sum_{i=1}^m\sum_{j=1}^n c_{ij}\phi_{ij}=0 \]
     where the \( c_{ij} \) are some real constants, and the right-hand side is understood to be the zero operator in \( L(E,F) \). Consider the action of the above operators on the basis vector \( e_k \) for an arbitrary \( k\in\qty{1,\dots,n} \)
     \[ \sum_{i=1}^m\sum_{j=1}^nc_{ij}\phi_{ij}(e_k)=\sum_{i=1}^m\sum_{j=1}^nc_{ij}\delta_{jk}\overline{e}_i=\sum_{j=1}^nc_{ik}\overline{e}_i=0\fstop \]

     However, by linear independence of \( \qty{\overline{e}_1,\dots,\overline{e}_m} \), it follows that \( c_{ik}=0 \) for all \( i\in\qty{1,\dots, m} \). This holds for any \( k\in\qty{1,\dots,n} \), which implies that the only solution to the initial operator equation is the trivial solution. This proves linear independence.

   \item Let \( A\in L(E,F) \), and let \( x\in E \). From a previous calculation, we have that
     \begin{align*}
       Ax&=\sum_{i=1}^m\sum_{j=1}^n A_{ij}x_j\overline{e}_i\\
       &=\sum_{i=1}^m\sum_{j=1}^nA_{ij}\sum_{k=1}^nx_k\phi_{ij}(e_k)\\
       &=\sum_{i=1}^m\sum_{j=1}^nA_{ij}\phi_{ij}\qty(\sum_{k=1}^nx_ke_k)\\
       &=\sum_{i=1}^m\sum_{j=1}^nA_{ij}\phi_{ij}(x)\fstop 
     \end{align*}

     Since the above holds for all \( x\in E \), we must have equality of the operators
     \[ A=\sum_{i=1}^m\sum_{j=1}^nA_{ij}\phi_{ij}\fstop \]

     So \( A \) can be written as a linear combination of the \( \phi_{ij} \). Moreover, the coefficients are none other than the entries \( A_{ij} \) of the matrix corresponding to \( A \). In fact, one sees that the matrix representation of the linear map \( \phi_{ij} \) simply contains a `1' in the \( i^{\text{th}} \) row and \( j^{\text{th}} \) column, and zeros elsewhere.
   \end{enumerate}

   Now, since the set \( \qty{\phi_{ij}:E\to F\mid i\in\qty{1,\dots,m},j\in\qty{1,\dots,n}} \) is a basis for \( L(E,F) \), the dimension of \( L(E,F) \) is simply the number of basis elements. There are \( m\times n=mn \) such linear maps in this basis, and so it follows that \( \dim L(E,F)=mn \).
 \end{proof}

 The dual space \( E^* \) of Euclidean \( n \)-space \( E \) can be alternatively expressed as \( L(E,\R) \). We know that the dimension of \( E^* \) is \( n \) - the same as \( E \). This is consistent with the above theorem, which says that \( \dim E^*=\dim L(E,\R)=n\times 1=n \).

 \vspace{3mm}

 Next, we will see that \( L(E,F) \) inherits a normed structure from \( E \) and \( F \), and so can be considered to be a normed vector space. Thus, all of our work on studying finite dimensional normed vector spaces can be applied to \( L(E,F) \).

 \begin{theorem}
   Let \( E,F \) be finite dimensional normed vector spaces. Then \( L(E,F) \) is a normed vector space, with norm given by
   \[ \norm{T}=\sup_{\norm{x}_E=1}\qty{\norm{Tx}_F\mid x\in E} \]
   where \( \norm{-}_E \) is the norm on \( E \) and \( \norm{-}_F\) is the norm on \( F \).
 \end{theorem}
 \begin{proof}
   Let \( T\in L(E,F) \). Denote by \( S \) the unit sphere in \( E \) with respect to the norm on \( E \), i.e.\
   \[ S=\qty{x\in E\mid \norm{x}_E=1}\fstop \]

   The set \( S \) is a compact subset of \( E \). Since \( T \) is linear, it is continuous by \Cref{thm:lin-cont}. Thus, the composition of the norm \( \norm{-}_F \) with \( T \) is a continuous function. Hence, by the Extreme Value Theorem, the restriction of \( \norm{-}_F\circ T \) to \( S \) attains a maximum value, which is defined to be \( \norm{T} \). This shows that the function \( \norm{-}:L(E,F)\to\R \) described above has well-defined values on all of \( L(E,F) \). We will proceed to show that this function is indeed a norm on \( L(E,F) \).
   \begin{enumerate}[label=(\roman*)]
   \item (Positivity) Let \( T\in L(E,F) \). Then \( \norm{Tx}_F\geq 0 \) for all \( x\in S \) by positivity of the norm \( \norm{-}_F \). Taking the supremum over \( S \) on both sides gives \( \norm{T}=\sup_{x\in S}\norm{Tx}_F\geq 0 \).

     \vspace{3mm}

     If \( T=0 \) (i.e.\ the zero function), then \( \norm{Tx}_F=0 \) for all \( x\in S \), and so \( \norm{T}=\sup_{x\in S}\norm{Tx}_F=0 \). Conversely, if \( \norm{T}=0 \), then for all \( x\in S \), we have
     \[ 0\leq\norm{Tx}_F\leq\sup_{x\in S}\norm{Tx}_F=\norm{T}=0\fstop \]

     So \( \norm{Tx}_F=0 \) for all \( x\in S \). Let \( y\in E\backslash\qty{0} \). Then \( \flatfrac{y}{\norm{y}_E}\in S \), so that
     \[ 0=\norm{T\frac{y}{\norm{y}_E}}_F=\frac{1}{\norm{y}_E}\norm{Ty}_F\fstop \]

     Thus, \( \norm{Ty}_F=0 \), which implies that \( Ty=0 \). Thus, \( T \) maps every element of \( E \) to \( 0\in F \), and so we must have that \( T=0 \).
   \item (Homogeneity of degree 1) Let \( a\in\R \). Then
     \[ \norm{aT}=\sup_{x\in S}\norm{aTx}_F=\abs{a}\sup_{x\in S}\norm{Tx}_F=\abs{a}\norm{T}\fstop \]
   \item (Triangle Inequality) Let \( A,B\in L(E,F) \). Then
     \[ \norm{A+B}=\sup_{x\in S}\norm{(A+B)x}_F\leq\sup_{x\in S}\qty(\norm{Ax}_F+\norm{Bx}_F)=\norm{A}+\norm{B}\fstop \]
   \end{enumerate}
 \end{proof}

 The norm defined on \( L(E,F) \) is often referred to as the `operator norm'. It is a natural norm induced by the norms on \( E \) and \( F \). A geometric interpretation for the norm of a linear map \( T \) is the maximum extent/scale factor that a unit vector is scaled by \( T \). So the value of \( \norm{Tx} \) never `blows up' as we approach any \( x_0\in E \) with finite length. This is a feature of the finite dimensional nature of the spaces we are working in.

 \vspace{3mm}

 Linear operators can be seen to satisfy the following boundedness property. Given any \( x\in E\backslash\qty{0} \), we have
 \begin{align*}
   \norm{Tx}&= \norm{x}\norm{T\hat{x}}\\
   &\leq \norm{x}\sup_{\norm{y}=1}\norm{Ty}\\
   &= \norm{T}\norm{x}
 \end{align*}
 where \( \hat{x}=\flatfrac{x}{\norm{x}} \) is the vector \( x \) normalised to have unit length. This inequality is also valid for \( x=0 \), in which case we have the trivial equality \( 0=0 \), and so the inequality holds for all \( x\in E \). In general, an operator satisfying this inequality is called a \emph{bounded operator}. The name arises from the fact that a bounded operator maps bounded sets to bounded sets. In the finite dimensional case, every linear operator is bounded.

 \vspace{3mm}
 
 The value of the operator norm will depend on which norms we use for \( E \) and \( F \), but since \( L(E,F) \) is a finite dimensional vector space, every possible choice will yield strongly equivalent norms, and so it doesn't really matter too much which norms on \( E \) and \( F \) we pick for the purposes of doing analysis.

 \vspace{3mm}

 Finally, we will present some further results on linear maps. In particular, we will present a version of the First Isomorphism Theorem for vector spaces, and from it derive the famous `Rank Theorem'. We then show a situation in which the injectivity of a linear map becomes a sufficient property for bijectivity.

 \vspace{3mm}
 
 In the results that follow, we will make use of the notion of a \emph{quotient space}. A somewhat geometrical picture is the following: given a vector space \( V \) and a subspace \( W \) of \( V \), we wish to consider the space \( W \) within \( V \) (which contains the origin), and all possible translations of \( W \) by vectors in \( V \). The set consisting of \( W \) as well as all of its different translates is what we will call the quotient space \( V/W \). So elements of \( V/W \) are subsets of \( V \), called cosets. When are two vectors \( v_1,v_2\in V \) contained in the same coset? Well, if they were, then we could write each vector as the sum of some vector in \( W \) and the constant offset/translation vector \( t\in V \), i.e.\
 \begin{align*}
   v_1&= w_1+t\\
   v_2&= w_2+t
 \end{align*}
 for some \( w_1,w_2\in W \). Hence, the difference between \( v_1 \) and \( v_2 \) is
 \[ v_1-v_2=(w_1+t)-(w_2+t)=w_1-w_2\in W \]
 where the statement \( w_1-w_2\in W \) follows from the fact that \( W \) is a subspace. This motivates us to define the relation \( \sim \) on \( V \) by
 \[ v_1\sim v_2\quad\text{if }v_1-v_2\in W\fstop \]

 This is an equivalence relation on \( V \), which follows from the fact that \( W \) is a subspace. The equivalence classes \( [v] \) of \( V \) under the relation \( \sim \) are the vector cosets \( v+W \) (translation of \( W \) by \( v \)). To show this, let \( v'\in [v] \). Then \( v'-v\coloneqq w\in W \), and so \( v'=v+(v'-v)=v+w \), which shows that \( v'\in v+W \), implying that \( [v]\subseteq v+W \). Conversely, if we suppose that \( v'\in v+W \), then \( v'=v+w \) for some \( w\in W \). This implies that \( v'-v=w\in W \), which shows that \( v'\sim v \), i.e.\ \( v'\in [v] \) and hence \( v+W\subseteq [v] \). Together, we have that \( [v]=v+W \). This leads us to the following definition:
 
 \begin{definition}
   Let \( V \) be a vector space, and let \( W \) be a subspace of \( V \). The \textbf{quotient space} \( V/W \) is the set of vector cosets
   \[ V/W=\qty{v+W\mid v\in V}\fstop \]
 \end{definition}

 We can define addition and scalar multiplication on \( V/W \) by \( (v_1+W)+(v_2+W)=(v_1+v_2)+W \) and \( a(v_1+W)=av_1+W \). These definitions are appropriate, since we have:
  \begin{proposition}
   Let \( V \) be a vector space, and let \( W \) be a subspace of \( V \). The operations of addition and scalar multiplication of vector cosets are well-defined operations on the quotient space \( V/W \).
 \end{proposition}
 \begin{proof}
   Let \( v_1,v_2,v_1',v_2'\in V \) such that
   \begin{align*}
     v_1+W&= v_1'+W\\
     v_2+W&= v_2'+W\fstop
   \end{align*}

   This implies that \( v_1\sim v_1' \) and \( v_2\sim v_2' \), and hence that \( v_1'-v_1\in W \) and \( v_2'-v_2\in W \). We therefore have that
   \begin{align*}
     (v_1'+W)+(v_2'+W)&= (v_1'-v_1+v_1+W)+(v_2'-v_2+v_2+W)\\
     &= (v_1+W)+(v_2+W)\fstop
   \end{align*}

   Now let \( a\in\R \). Then
   \[ av_1'+W=a(v_1'+W)=a(v_1'-v_1+v_1+W)=a(v_1+W)=av_1+W\fstop \]
 \end{proof}

 Thus, when equipped with these operations, \( V/W \) is a vector space. The fact that \( V/W \) satisfies the defining properties of a vector space follows quite readily from simple and not very illuminating calculations involving the analogous properties of \( V \). The additive identity on \( V/W \) is \( 0+W=W \).

 \vspace{3mm}

 We now have the machinery required to present the (coincidentally) first major theorem for this section: the First Isomorphism Theorem (for vector spaces), which states that given a linear map \( T:V\to W \) between vector spaces, the quotient space \( V/\ker T \) is isomorphic to the range \( \mathcal{R}(T) \) of \( T \). This, at first glance, may seem like magic, but actually once you break it down it really is just common sense! Let's explain:

 \vspace{3mm}

 A desired isomorphism \( \widetilde{T}:V/\ker T\to \mathcal{R}(T) \) will be constructed from \( T \). How does \( T \) itself fail to be an isomorphism? Since it is a vector space homomorphism (i.e.\ a linear map), the only way it can fall short of being an isomorphism is if it fails to be bijective. We form the map \( \widetilde{T} \) by `patching in' the deficiencies in \( T \) by `fixing' injectivity and surjectivity. It turns out that the process of fixing these failures shrinks the codomain to the image of \( T \) and shrinks the domain to the quotient space \( V/\ker T \).

  \begin{theorem}
   (First Isomorphism Theorem) Let \( V \), \( W \) be vector spaces and let \( T:V\to W \) be a linear map. Then \( V/\ker T\simeq\mathcal{R}(T) \).
 \end{theorem}
 \begin{proof}
   The map \( T \) can be made surjective by restricting its codomain to its range \( \mathcal{R}(T) \), so that \( T:V\to\mathcal{R}(T) \) is a surjective linear map.

   \vspace{3mm}

   A linear map \( T \) is injective if and only if \( \ker T=\qty{0} \). So if \( T \) is not injective, then its kernel is a subspace of \( V \) whose dimension is nonzero. In order to `shrink' the kernel, we form the quotient space \( V/\ker T \), which in essence collapses all vectors in the kernel down to a single element; a vector coset. We hence define a map \( \widetilde{T}:V/\ker T\to\mathcal{R}(T) \) that acts on the cosets by
   \[ \widetilde{T}(v+\ker T)=T(v)\fstop \]

   We will need to show that
   \begin{enumerate}[label=(\roman*)]
   \item \( \widetilde{T} \) is well-defined.
   \item \( \widetilde{T} \) is linear.
   \item \( \widetilde{T} \) is injective.
   \item \( \widetilde{T} \) is surjective.
   \end{enumerate}

   Together, these would show that \( \widetilde{T} \) is an isomorphism between \( V/\ker T \) and \( \mathcal{R}(T) \).
   \begin{enumerate}[label=(\roman*)]
   \item Let \( v'\in V \) be such that \( v'+\ker T=v+\ker T \), i.e.\ \( v'\sim v \). Thus, \( v'-v\in\ker T \). We therefore have that
     \[ \widetilde{T}(v'+\ker T)=T(v')=T(v'-v+v)=T(v'-v)+T(v)=0+T(v)=T(v)=\widetilde{T}(v+\ker T)\fstop \]
   \item Let \( v_1+\ker T, v_2+\ker T\in V/\ker T \). Then
     \begin{align*}
       \widetilde{T}(v_1+\ker T+v_2+\ker T)&= \widetilde{T}(v_1+v_2+\ker T)\\
       &= T(v_1+v_2)\\
       &= T(v_1)+T(v_2)\\
       &= \widetilde{T}(v_1+\ker T)+\widetilde{T}(v_2+\ker T)\fstop
     \end{align*}

     Let \( a\in\R \) and let \( v+\ker T\in V/\ker T \). Then
     \begin{align*}
       \widetilde{T}(a(v+\ker T))&= \widetilde{T}(av+\ker T)\\
       &= T(av)\\
       &= aT(v)\\
       &= a\widetilde{T}(v+\ker T)\fstop
     \end{align*}
   \item Let \( (v+\ker T)\in\ker\widetilde{T} \). So \( \widetilde{T}(v+\ker T)=0 \). But
     \[ 0=\widetilde{T}(v+\ker T)=T(v)\fstop \]

     This implies that \( v\in\ker T \), and hence \( v+\ker T=\ker T \). Thus, \( \ker\widetilde{T}=\qty{\ker T} \), i.e.\ the kernel of \( \widetilde{T} \) contains only the zero coset. Since \( \widetilde{T} \) is linear, this proves the injectivity of \( \widetilde{T} \).
   \item Let \( w\in\mathcal{R}(T) \). Since \( T:V\to\mathcal{R}(T) \) is surjective, there exists a \( v\in V \) such that \( T(v)=w \). But then we have that \( v+\ker T\in V/\ker T \) satisfies
     \[ \widetilde{T}(v+\ker T)=T(v)=w\fstop \]
   \end{enumerate}
 \end{proof}

 This line of thinking can be used to prove other versions of the First Isomorphism Theorem between different structures, e.g.\ groups, rings, fields. The central idea remains the same: given a map that is \emph{not} bijective, how can we `fix' it and make it bijective? We fix surjectivity by restricting the codomain to the image of the map, and fix injectivity by grouping together elements that map to the same object. Of course, this means that the modified map now acts on \emph{equivalence classes} rather than the original elements. Finally, if the map preserves some structure (e.g.\ is a group homomorphism), then the modified map will also preserve the same structure (e.g.\ group multiplication).

 \vspace{3mm}

 Back to quotient spaces: what is the dimension of the quotient space? Given that in the construction of the quotient space, we group together entire subsets of vectors to form a single vector coset, we expect that the quotient space is `smaller' than the original vector space. Exactly how much smaller is dictated by the size of the subspace that has been quotiented out.
 \begin{proposition}
   \label{thm:codim}
   Let \( V \) be a finite dimensional vector space, and let \( W \) be a subspace of \( V \). Then \( \dim V/W=\dim V-\dim W \).
 \end{proposition}
 \begin{proof}
   Suppose \( \dim V=n \), \( \dim W=k\leq n \), \( \qty{w_1,\dots, w_k} \) is a basis for \( W \) and \( \qty{v_1,\dots, v_n} \) be a basis for \( V \). By an analogous argument as was used in the proof of \Cref{thm:dim}, we can replace \( k \) of the basis vectors for \( V \) with the basis vectors for \( W \) and obtain (after some relabelling) the following basis\footnote{By the way, this result is often called the `Replacement Lemma'.} for \( V \):
   \[ \qty{w_1,\dots,w_k,v_{k+1},\dots,v_n}\fstop \]

   We claim that \( \qty{v_{k+1}+W,\dots,v_n+W} \) is a basis for \( V/W \).

   \vspace{3mm}

   Let \( v+W\in V/W \). We can expand \( v\in V \) as a linear combination of our basis for \( V \)
   \[ v=c_1w_1+\dots+c_kw_k+c_{k+1}v_{k+1}+\dots+c_nv_n\fstop \]

   This implies that
   \[ v+W=c_1(w_1+W)+\dots+c_k(w_k+W)+c_{k+1}(v_{k+1}+W)+\dots+c_n(v_n+W)\fstop \]

   But \( w_i+W=W \) for each \( i\in\qty{1,\dots,k} \), so that
   \[ v+W=c_{k+1}(v_{k+1}+W)+\dots+c_n(v_n+W)\fstop \]

   Thus, any element of \( V/W \) can be expressed as a linear combination of the vectors in
   \[ \qty{v_{k+1}+W,\dots,v_n+W} \fstop \]

   This shows that
   \[ V/W=\text{span}\qty{v_{k+1}+W,\dots,v_n+W}\fstop \]

   Next, consider the following equation
   \[ a_1(v_{k+1}+W)+\dots+a_{n-k}(v_n+W)=W \]
   where \( a_1,\dots,a_{n-k}\in \R \). Performing the addition of the vector cosets allows us to write
   \[ \qty(a_1v_{k+1}+\dots+a_{n-k}v_n)+W=W\cma \]
   which is satisfied if and only if
   \[ a_1v_{k+1}+\dots+a_{n-k}v_n\in W\fstop \]

   Thus, the above vector can be written as a linear combination of the basis vectors for \( W \)
   \[ a_1v_{k+1}+\dots+a_{n-k}v_n=b_1w_1+\dots+b_kw_k\fstop \]

   We can rearrange this as follows
   \[ a_1v_{k+1}+\dots+a_{n-k}v_n-b_1w_1-\dots-b_kw_k=0\fstop \]

   But since \( \qty{w_1,\dots,w_k,v_{k+1},\dots,v_n} \) is a basis for \( V \), it is a linearly independent set. All of the above coefficients must vanish, although in particular, we have that \( a_1=\dots=a_{n-k}=0 \), which implies the linear independence of \( \qty{v_{k+1}+W,\dots,v_n+W} \), and thus demonstrates that it is a basis for \( V/W \). The number of elements in this basis is equal to \( n-k \), so we have that
   \[ \dim(V/W)=n-k=\dim V-\dim W\fstop \]
 \end{proof}

 The above result, together with the First Isomorphism Theorem, leads to an easy proof of the Rank Theorem, which provides a relationship between the kernel and range of a linear map.
 
 \begin{theorem}
   (Rank Theorem) Let \( V \), \( W \) be finite dimensional vector spaces and let \( T:V\to W \) be a linear map. Then
   \[ \dim\ker T+\dim\mathcal{R}(T)=\dim V\fstop \]
 \end{theorem}
 \begin{proof}
   By the First Isomorphism Theorem, \( V/\ker T \) is isomorphic to \( \mathcal{R}(T) \). By \Cref{thm:iso-samedim}, this implies that \( \dim (V/\ker T)=\dim\mathcal{R}(T) \). But by \Cref{thm:codim}, we have that
   \[ \dim\mathcal{R}(T)=\dim (V/\ker T)=\dim V-\dim\ker T\fstop \]

   Rearranging the above expression gives the desired relationship.
 \end{proof}

 The name `Rank Theorem' is due to the fact that the dimension of the range of \( T \) is often referred to as the \emph{rank} of the linear operator \( T \).

 \vspace{3mm}
 
 Finally, we provide a quick application of the Rank Theorem:
 \begin{proposition}
   Let \( E \) and \( F \) be \( n \)-dimensional vector spaces, and let \( T\in L(E,F) \). Then \( T \) is injective if and only if \( T \) is bijective (and hence an isomorphism).
 \end{proposition}
 \begin{proof}
   Suppose \( T \) is injective. Since \( T \) is linear, this implies that \( \ker T=\qty{0} \), so that \( \dim\ker T=0 \). Then by the Rank Theorem, we have that
   \[ \dim\mathcal{R}(T)=\dim E=n=\dim F\fstop \]

   However, since \( \mathcal{R}(T)\subseteq F \), we must have that \( \mathcal{R}(T)=F \), as the only \( n \)-dimensional subspace of an \( n \)-dimensional vector space \( F \) is \( F \) itself. This shows that \( T \) is surjective, and hence a bijection.
 \end{proof}

 This result will be useful to establish when a linear map is \emph{invertible} (recall that a function is invertible if and only if it is bijective). For instance, this will pop up when we come to discuss the \emph{Inverse Function Theorem}, which is concerned with the inverse of a differentiable function.

 \vspace{3mm}

 There are many more purely algebraic and topological things in Euclidean space which we can discuss further, but for now, we'll call it a day and start doing some calculus!

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "multivar"
%%% End: